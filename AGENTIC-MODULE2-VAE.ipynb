{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "a40ccbca-eb02-42a7-eb10-effd436feb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUNNING SMART PIPELINE (RECOMMENDED)\n",
            "============================================================\n",
            "============================================================\n",
            "STARTING SMART VAE PIPELINE\n",
            "============================================================\n",
            "Loading VAE models with custom layers...\n",
            "VAE models loaded successfully!\n",
            "✅ VAE models loaded successfully!\n",
            "✅ Synthetic data already exists! Loading...\n",
            "   Loaded 350000 synthetic samples\n",
            "✅ VAR windows already computed! Loading...\n",
            "   Loaded 350000 VAR windows\n",
            "   Loaded 350000 final synthetic samples\n",
            "============================================================\n",
            "✅ SMART PIPELINE COMPLETE!\n",
            "   Synthetic samples: 350000\n",
            "   VAR windows: 350000\n",
            "   🚀 All work completed - no computation needed!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUATING VAE PERFORMANCE\n",
            "============================================================\n",
            "Running quick VAE performance check...\n",
            "✅ VAE Performance Check:\n",
            "   MSE: 0.014578\n",
            "   MAE: 0.058756\n",
            "   Input shape: (10, 50, 13)\n",
            "   Output shape: (10, 50, 13)\n",
            "✅ Good reconstruction quality!\n",
            "\n",
            "🎉 PIPELINE COMPLETE!\n",
            "   Synthetic data shape: (350000, 650)\n",
            "   VAR windows shape: (350000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Just copy and paste this for immediate use:\\n\\ngenerator = VAEDataGenerator()\\nresults = generator.run_smart_pipeline(\\n    r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy',\\n    target_samples=350000\\n)\\n\\n# Check performance\\ngenerator.quick_vae_check(\\n    r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy'\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# VAE Data Generator Module - COMPLETE CODE with Smart Resumability\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "from statsmodels.tsa.api import VAR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class VAEDataGenerator:\n",
        "    \"\"\"\n",
        "    One-time VAE training and synthetic data generation with smart resumability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.vae = None\n",
        "\n",
        "        # Create checkpoints directory\n",
        "        self.checkpoint_dir = f\"{output_dir}checkpoints/\"\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    def register_custom_layers(self):\n",
        "        \"\"\"Register custom layers for model loading\"\"\"\n",
        "        # Clear and register custom layers\n",
        "        saving.get_custom_objects().clear()\n",
        "\n",
        "        @saving.register_keras_serializable(package=\"MyLayers\")\n",
        "        class Sampling(layers.Layer):\n",
        "            \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "            def __init__(self, factor):\n",
        "                super().__init__()\n",
        "                self.factor = factor\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "            def get_config(self):\n",
        "                return {\"factor\": self.factor}\n",
        "\n",
        "        return Sampling\n",
        "\n",
        "    def load_vae_models(self):\n",
        "        \"\"\"Load VAE models with proper custom layer registration\"\"\"\n",
        "        # Register custom layers first\n",
        "        Sampling = self.register_custom_layers()\n",
        "\n",
        "        encoder_path = f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras'\n",
        "        decoder_path = f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "        if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "            print(\"Loading VAE models with custom layers...\")\n",
        "            self.encoder = keras.models.load_model(encoder_path)\n",
        "            self.decoder = keras.models.load_model(decoder_path)\n",
        "            print(\"VAE models loaded successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"VAE models not found!\")\n",
        "            return False\n",
        "\n",
        "    def train_vae_pipeline(self, train_data_path):\n",
        "        \"\"\"\n",
        "        Complete VAE training pipeline\n",
        "        \"\"\"\n",
        "        print(\"Starting VAE training pipeline...\")\n",
        "\n",
        "        # Load data\n",
        "        train_data = np.load(train_data_path)\n",
        "        n_seq = train_data.shape[0]\n",
        "        window_size = train_data.shape[1]\n",
        "        n_features = train_data.shape[2]\n",
        "\n",
        "        maxval = train_data.shape[0]\n",
        "        count_train = int(math.ceil(0.8*maxval))\n",
        "        x_train = train_data[:count_train]\n",
        "        x_test = train_data[count_train:]\n",
        "\n",
        "        # Clear all previously registered custom objects\n",
        "        saving.get_custom_objects().clear()\n",
        "\n",
        "        # Create a custom layer\n",
        "        @saving.register_keras_serializable(package=\"MyLayers\")\n",
        "        class Sampling(layers.Layer):\n",
        "            \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "            def __init__(self, factor):\n",
        "                super().__init__()\n",
        "                self.factor = factor\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "            def get_config(self):\n",
        "                return {\"factor\": self.factor}\n",
        "\n",
        "        # Build the encoder\n",
        "        latent_dim = 5\n",
        "        intermediate_dim = 256\n",
        "\n",
        "        # Encoder\n",
        "        encoder_inputs =  layers.Input(shape=(window_size, n_features),name=\"encoder_input\")\n",
        "        x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "        xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "        x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\" )(xx)\n",
        "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "        z = Sampling(1)([z_mean, z_log_var])\n",
        "        encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "        encoder.summary()\n",
        "\n",
        "        # Decoder\n",
        "        inp_z = Input(shape=(latent_dim,),name=\"decoder\")\n",
        "        x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "        x2= layers.Dense(int(intermediate_dim/2),  name=\"Dense2\")(x1)\n",
        "        x22= layers.LSTM(int(intermediate_dim/2),activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "        x3 = layers.LSTM(intermediate_dim,activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "        decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "        decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "        decoder.summary()\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "        # Parameters - your code\n",
        "        n_epochs = 150\n",
        "        klstart = 20\n",
        "        kl_annealtime = n_epochs-klstart\n",
        "        weight = K.variable(0.0)\n",
        "\n",
        "        # Define the VAE as a Model with a custom train_step\n",
        "        class VAE(keras.Model):\n",
        "            def __init__(self, encoder, decoder, **kwargs):\n",
        "                super(VAE, self).__init__(**kwargs)\n",
        "                self.encoder = encoder\n",
        "                self.decoder = decoder\n",
        "                self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "                self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "                    name=\"reconstruction_loss\"\n",
        "                )\n",
        "                self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "            @property\n",
        "            def metrics(self):\n",
        "                return [\n",
        "                    self.total_loss_tracker,\n",
        "                    self.reconstruction_loss_tracker,\n",
        "                    self.kl_loss_tracker,\n",
        "                ]\n",
        "\n",
        "            def train_step(self, data):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                        )\n",
        "\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                    total_loss = reconstruction_loss + (weight*kl_loss)\n",
        "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "                self.total_loss_tracker.update_state(total_loss)\n",
        "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "                self.kl_loss_tracker.update_state(kl_loss)\n",
        "                return {\n",
        "                    \"loss\": self.total_loss_tracker.result(),\n",
        "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                    \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                }\n",
        "\n",
        "            def test_step(self, data):\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                        )\n",
        "\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "                    total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "                    return {\n",
        "                        \"loss\": self.total_loss_tracker.result(),\n",
        "                        \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                        \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                          }\n",
        "\n",
        "        # CALLBACKS\n",
        "        es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "        class AnnealingCallback(Callback):\n",
        "            def __init__(self, weight):\n",
        "                super().__init__()\n",
        "                self.weight = weight\n",
        "            def on_epoch_end(self, epoch, logs={}):\n",
        "                if epoch > klstart and epoch < klstart*1.2:\n",
        "                    new_weight = min(K.get_value(self.weight) + (1./ kl_annealtime), 1.)\n",
        "                    K.set_value(self.weight, new_weight)\n",
        "                print (\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "        # Train the VAE\n",
        "        vae = VAE(encoder, decoder)\n",
        "        vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "        history=vae.fit( x_train,\n",
        "                         epochs=n_epochs,\n",
        "                         batch_size=32,\n",
        "                         validation_split=0.1,\n",
        "                         callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "        # Save models\n",
        "        encoder.save(f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras')\n",
        "        decoder.save(f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras')\n",
        "\n",
        "        # Store for data generation\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.window_size = window_size\n",
        "        self.n_features = n_features\n",
        "\n",
        "        print(\"VAE training complete and models saved!\")\n",
        "        return history\n",
        "\n",
        "    def generate_synthetic_dataset_fast(self, train_data_path, cohort_size=350000, checkpoint_every=5000):\n",
        "        \"\"\"\n",
        "        FASTER synthetic dataset generation with optimizations\n",
        "        \"\"\"\n",
        "        print(f\"Generating synthetic dataset FAST - {cohort_size} samples...\")\n",
        "\n",
        "        # Load models if needed with proper custom layer registration\n",
        "        if self.encoder is None:\n",
        "            if not self.load_vae_models():\n",
        "                raise ValueError(\"VAE models not found! Train VAE first.\")\n",
        "\n",
        "        # Load and process data\n",
        "        train_data = np.load(train_data_path)\n",
        "        window_size = train_data.shape[1]\n",
        "        n_features = train_data.shape[2]\n",
        "\n",
        "        # Keep original generator_multiply = 100 (as requested)\n",
        "        generator_multiply = 100\n",
        "        print(f\"Using generator_multiply = {generator_multiply}\")\n",
        "\n",
        "        # Process all base samples (no reduction)\n",
        "        max_base_samples = min(cohort_size // generator_multiply, train_data.shape[0])\n",
        "        if max_base_samples < train_data.shape[0]:\n",
        "            print(f\"Processing {max_base_samples} base samples to reach target {cohort_size}\")\n",
        "            train_data = train_data[:max_base_samples]\n",
        "\n",
        "        # Get encodings\n",
        "        print(\"Computing encodings...\")\n",
        "        X_train_encoded = self.encoder.predict(train_data, batch_size=64, verbose=1)  # Larger batch\n",
        "        mu, logvar, z = X_train_encoded\n",
        "        sigma = tf.exp(0.5 * logvar)\n",
        "\n",
        "        # OPTIMIZATION: Batch processing for decoder\n",
        "        print(\"Generating synthetic data in batches...\")\n",
        "        all_results = []\n",
        "        batch_size = 50  # Process 50 samples at once\n",
        "\n",
        "        for i in range(0, len(mu), batch_size):\n",
        "            batch_end = min(i + batch_size, len(mu))\n",
        "            batch_mu = mu[i:batch_end]\n",
        "            batch_sigma = sigma[i:batch_end]\n",
        "\n",
        "            # Generate all Z vectors for this batch\n",
        "            batch_z = []\n",
        "            for j in range(len(batch_mu)):\n",
        "                z_samples = tf.random.normal(\n",
        "                    shape=(generator_multiply, mu.shape[1]),\n",
        "                    mean=batch_mu[j],\n",
        "                    stddev=batch_sigma[j]\n",
        "                )\n",
        "                batch_z.append(z_samples)\n",
        "\n",
        "            # Decode all at once\n",
        "            all_z = tf.concat(batch_z, axis=0)\n",
        "            decoded_batch = self.decoder.predict(all_z, batch_size=128, verbose=0)  # Large batch\n",
        "            decoded_batch = decoded_batch.reshape((decoded_batch.shape[0], window_size * n_features))\n",
        "            all_results.append(decoded_batch)\n",
        "\n",
        "            if (i // batch_size + 1) % 10 == 0:\n",
        "                print(f\"Processed {i + batch_size}/{len(mu)} base samples...\")\n",
        "\n",
        "        results1 = np.concatenate(all_results, axis=0)\n",
        "        np.save(f'{self.output_dir}generated_large_subsquence_data.npy', results1)\n",
        "        print(f\"Fast synthetic dataset complete: {results1.shape}\")\n",
        "        return results1\n",
        "\n",
        "    def compute_var_windows_fast(self, data_path, start_idx=0, batch_size=5000):\n",
        "        \"\"\"\n",
        "        FASTER VAR computation with optimizations\n",
        "        \"\"\"\n",
        "        print(f\"Computing VAR windows FAST for batch starting at {start_idx}...\")\n",
        "\n",
        "        # OPTIMIZATION: Extended window range 2->30 (as requested)\n",
        "        max_var_lag = 30\n",
        "        print(f\"Testing VAR lags from 2 to {max_var_lag}\")\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        window_size = self.window_size if self.window_size else 50\n",
        "        n_features = self.n_features if self.n_features else 13\n",
        "\n",
        "        x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "        n_future = 1\n",
        "\n",
        "        end_idx = min(start_idx + batch_size, x_3d.shape[0])\n",
        "        best_window_for_long_seq = []\n",
        "\n",
        "        # OPTIMIZATION: Batch processing\n",
        "        for i in range(start_idx, end_idx):\n",
        "            rmse_list = []\n",
        "\n",
        "            # Test lags from 2 to 30 (as requested)\n",
        "            for k in range(2, min(max_var_lag + 1, window_size//2)):\n",
        "                cur_seq = x_3d[i,:,:]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "\n",
        "                try:\n",
        "                    model = VAR(df_train)\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            best_window_for_long_seq.append(min_sw)\n",
        "\n",
        "            if (i - start_idx) % 500 == 0:\n",
        "                print(f'FAST processed {i - start_idx}/{end_idx - start_idx} sequences...')\n",
        "\n",
        "        Window = np.array(best_window_for_long_seq)\n",
        "        batch_file = f'{self.output_dir}generated-data-true-window-BATCH_{start_idx}_{end_idx}.npy'\n",
        "        np.save(batch_file, Window)\n",
        "        print(f\"FAST VAR windows computed for batch {start_idx}-{end_idx}\")\n",
        "        return Window\n",
        "\n",
        "    def run_smart_pipeline(self, train_data_path, target_samples=350000):\n",
        "        \"\"\"\n",
        "        SMART pipeline that checks for existing files and skips completed work\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"STARTING SMART VAE PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # File paths\n",
        "        synthetic_file = f'{self.output_dir}generated_large_subsquence_data.npy'\n",
        "        var_windows_file = f'{self.output_dir}generated-data-true-window-OPTIMIZED.npy'\n",
        "        final_data_file = f'{self.output_dir}generated-data-OPTIMIZED.npy'\n",
        "\n",
        "        # Step 1: Check VAE models\n",
        "        if not self.load_vae_models():\n",
        "            print(\"❌ No VAE models found! Please train VAE first.\")\n",
        "            print(\"Run: generator.train_vae_pipeline(train_data_path)\")\n",
        "            return None\n",
        "        else:\n",
        "            print(\"✅ VAE models loaded successfully!\")\n",
        "            train_data = np.load(train_data_path)\n",
        "            self.window_size = train_data.shape[1]\n",
        "            self.n_features = train_data.shape[2]\n",
        "\n",
        "        # Step 2: Check synthetic data\n",
        "        if os.path.exists(synthetic_file):\n",
        "            print(\"✅ Synthetic data already exists! Loading...\")\n",
        "            synthetic_data = np.load(synthetic_file)\n",
        "            print(f\"   Loaded {synthetic_data.shape[0]} synthetic samples\")\n",
        "        else:\n",
        "            print(\"🔄 Generating synthetic data...\")\n",
        "            synthetic_data = self.generate_synthetic_dataset_fast(train_data_path, target_samples)\n",
        "\n",
        "        # Step 3: Check VAR windows\n",
        "        if os.path.exists(var_windows_file) and os.path.exists(final_data_file):\n",
        "            print(\"✅ VAR windows already computed! Loading...\")\n",
        "            final_windows = np.load(var_windows_file)\n",
        "            synthetic_data = np.load(final_data_file)  # Load final version\n",
        "            print(f\"   Loaded {final_windows.shape[0]} VAR windows\")\n",
        "            print(f\"   Loaded {synthetic_data.shape[0]} final synthetic samples\")\n",
        "        else:\n",
        "            print(\"🔄 Computing VAR windows...\")\n",
        "\n",
        "            # Check if we have partial batch results\n",
        "            batch_files = []\n",
        "            batch_size = 5000\n",
        "            total_samples = synthetic_data.shape[0]\n",
        "\n",
        "            # Look for existing batch files\n",
        "            all_batches_exist = True\n",
        "            for start_idx in range(0, total_samples, batch_size):\n",
        "                end_idx = min(start_idx + batch_size, total_samples)\n",
        "                batch_file = f'{self.output_dir}generated-data-true-window-BATCH_{start_idx}_{end_idx}.npy'\n",
        "                if os.path.exists(batch_file):\n",
        "                    batch_files.append(batch_file)\n",
        "                    print(f\"   ✅ Found existing batch: {start_idx}-{end_idx}\")\n",
        "                else:\n",
        "                    all_batches_exist = False\n",
        "                    print(f\"   ❌ Missing batch: {start_idx}-{end_idx}\")\n",
        "\n",
        "            if all_batches_exist and len(batch_files) > 0:\n",
        "                print(\"✅ All VAR batch files exist! Combining...\")\n",
        "                all_windows = []\n",
        "                for batch_file in batch_files:\n",
        "                    batch_windows = np.load(batch_file)\n",
        "                    all_windows.append(batch_windows)\n",
        "\n",
        "                final_windows = np.concatenate(all_windows, axis=0)\n",
        "\n",
        "                # Save combined results\n",
        "                np.save(var_windows_file, final_windows)\n",
        "                np.save(final_data_file, synthetic_data)\n",
        "                print(f\"   ✅ Combined and saved {final_windows.shape[0]} VAR windows\")\n",
        "            else:\n",
        "                print(\"🔄 Computing missing VAR windows...\")\n",
        "                all_windows = []\n",
        "\n",
        "                for start_idx in range(0, total_samples, batch_size):\n",
        "                    end_idx = min(start_idx + batch_size, total_samples)\n",
        "                    batch_file = f'{self.output_dir}generated-data-true-window-BATCH_{start_idx}_{end_idx}.npy'\n",
        "\n",
        "                    if os.path.exists(batch_file):\n",
        "                        print(f\"   ✅ Loading existing batch {start_idx}-{end_idx}\")\n",
        "                        batch_windows = np.load(batch_file)\n",
        "                    else:\n",
        "                        print(f\"   🔄 Computing batch {start_idx}-{end_idx}\")\n",
        "                        batch_windows = self.compute_var_windows_fast(synthetic_file, start_idx, batch_size)\n",
        "\n",
        "                    all_windows.append(batch_windows)\n",
        "\n",
        "                final_windows = np.concatenate(all_windows, axis=0)\n",
        "                np.save(var_windows_file, final_windows)\n",
        "                np.save(final_data_file, synthetic_data)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"✅ SMART PIPELINE COMPLETE!\")\n",
        "        print(f\"   Synthetic samples: {synthetic_data.shape[0]}\")\n",
        "        print(f\"   VAR windows: {final_windows.shape[0]}\")\n",
        "        print(\"   🚀 All work completed - no computation needed!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {'synthetic_data': synthetic_data, 'var_windows': final_windows}\n",
        "\n",
        "    def run_fast_generation(self, train_data_path, target_samples=350000, force_retrain=False):\n",
        "        \"\"\"\n",
        "        Optimized pipeline with your requested changes\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"STARTING OPTIMIZED VAE DATA GENERATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Load existing VAE or train\n",
        "        if force_retrain:\n",
        "            print(\"FORCE RETRAIN: Training new VAE models...\")\n",
        "            self.train_vae_pipeline(train_data_path)\n",
        "        elif not self.load_vae_models():\n",
        "            print(\"No existing models found. Training VAE...\")\n",
        "            self.train_vae_pipeline(train_data_path)\n",
        "        else:\n",
        "            print(\"VAE models loaded successfully!\")\n",
        "            # Get data properties\n",
        "            train_data = np.load(train_data_path)\n",
        "            self.window_size = train_data.shape[1]\n",
        "            self.n_features = train_data.shape[2]\n",
        "\n",
        "        # Step 2: Generate synthetic dataset (full size, generator_multiply=100)\n",
        "        print(f\"Generating {target_samples} synthetic samples...\")\n",
        "        synthetic_data = self.generate_synthetic_dataset_fast(train_data_path, cohort_size=target_samples)\n",
        "\n",
        "        # Step 3: Compute VAR windows (testing lags 2->30)\n",
        "        print(\"Computing VAR windows (testing lags 2-30)...\")\n",
        "        batch_size = 5000\n",
        "        total_samples = synthetic_data.shape[0]\n",
        "        all_windows = []\n",
        "\n",
        "        for start_idx in range(0, total_samples, batch_size):\n",
        "            batch_windows = self.compute_var_windows_fast(\n",
        "                f'{self.output_dir}generated_large_subsquence_data.npy',\n",
        "                start_idx,\n",
        "                batch_size\n",
        "            )\n",
        "            all_windows.append(batch_windows)\n",
        "\n",
        "        # Combine and save\n",
        "        final_windows = np.concatenate(all_windows, axis=0)\n",
        "\n",
        "        # Save with timestamp if retraining\n",
        "        if force_retrain:\n",
        "            import datetime\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            np.save(f'{self.output_dir}generated-data-true-window-RETRAINED_{timestamp}.npy', final_windows)\n",
        "            np.save(f'{self.output_dir}generated-data-RETRAINED_{timestamp}.npy', synthetic_data)\n",
        "            print(f\"Retrained models saved with timestamp: {timestamp}\")\n",
        "        else:\n",
        "            np.save(f'{self.output_dir}generated-data-true-window-OPTIMIZED.npy', final_windows)\n",
        "            np.save(f'{self.output_dir}generated-data-OPTIMIZED.npy', synthetic_data)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"OPTIMIZED VAE DATA GENERATION COMPLETE!\")\n",
        "        print(f\"Generated {synthetic_data.shape[0]} synthetic samples\")\n",
        "        print(f\"Computed {final_windows.shape[0]} VAR windows\")\n",
        "        print(f\"VAR lag range: 2-30\")\n",
        "        print(f\"Generator multiply: 100 (kept original)\")\n",
        "        if force_retrain:\n",
        "            print(\"🔄 RETRAINED from scratch!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'synthetic_data': synthetic_data,\n",
        "            'var_windows': final_windows\n",
        "        }\n",
        "\n",
        "    def evaluate_vae_performance(self, train_data_path, num_samples=5):\n",
        "        \"\"\"\n",
        "        Evaluate VAE reconstruction performance with visualizations\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"EVALUATING VAE RECONSTRUCTION PERFORMANCE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load models if needed\n",
        "        if self.encoder is None:\n",
        "            if not self.load_vae_models():\n",
        "                raise ValueError(\"VAE models not found! Train VAE first.\")\n",
        "\n",
        "        # Load test data\n",
        "        train_data = np.load(train_data_path)\n",
        "\n",
        "        # Take a few samples for evaluation\n",
        "        test_samples = train_data[:num_samples]\n",
        "        print(f\"Testing reconstruction on {num_samples} samples...\")\n",
        "\n",
        "        # Encode and decode\n",
        "        encoded = self.encoder.predict(test_samples)\n",
        "        mu, logvar, z = encoded\n",
        "        reconstructed = self.decoder.predict(z)\n",
        "\n",
        "        # Calculate reconstruction metrics\n",
        "        mse_scores = []\n",
        "        mae_scores = []\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            original = test_samples[i]\n",
        "            recon = reconstructed[i]\n",
        "\n",
        "            mse = mean_squared_error(original.flatten(), recon.flatten())\n",
        "            mae = mean_absolute_error(original.flatten(), recon.flatten())\n",
        "\n",
        "            mse_scores.append(mse)\n",
        "            mae_scores.append(mae)\n",
        "\n",
        "            print(f\"Sample {i+1}: MSE = {mse:.6f}, MAE = {mae:.6f}\")\n",
        "\n",
        "        avg_mse = np.mean(mse_scores)\n",
        "        avg_mae = np.mean(mae_scores)\n",
        "\n",
        "        print(f\"\\nOverall Performance:\")\n",
        "        print(f\"Average MSE: {avg_mse:.6f}\")\n",
        "        print(f\"Average MAE: {avg_mae:.6f}\")\n",
        "\n",
        "        # Visualize reconstructions\n",
        "        self.plot_reconstructions(test_samples, reconstructed, num_samples)\n",
        "\n",
        "        return {\n",
        "            'mse_scores': mse_scores,\n",
        "            'mae_scores': mae_scores,\n",
        "            'avg_mse': avg_mse,\n",
        "            'avg_mae': avg_mae,\n",
        "            'original_samples': test_samples,\n",
        "            'reconstructed_samples': reconstructed\n",
        "        }\n",
        "\n",
        "    def plot_reconstructions(self, original, reconstructed, num_samples=5):\n",
        "        \"\"\"\n",
        "        Plot original vs reconstructed subsequences\n",
        "        \"\"\"\n",
        "        # Determine number of features to plot\n",
        "        n_features_to_plot = min(4, original.shape[2])  # Plot max 4 features\n",
        "\n",
        "        fig, axes = plt.subplots(num_samples, n_features_to_plot,\n",
        "                                figsize=(16, 3*num_samples))\n",
        "\n",
        "        if num_samples == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for sample_idx in range(num_samples):\n",
        "            for feature_idx in range(n_features_to_plot):\n",
        "                ax = axes[sample_idx, feature_idx]\n",
        "\n",
        "                # Plot original and reconstructed\n",
        "                original_seq = original[sample_idx, :, feature_idx]\n",
        "                recon_seq = reconstructed[sample_idx, :, feature_idx]\n",
        "\n",
        "                ax.plot(original_seq, 'b-', label='Original', linewidth=2)\n",
        "                ax.plot(recon_seq, 'r--', label='Reconstructed', linewidth=2)\n",
        "\n",
        "                ax.set_title(f'Sample {sample_idx+1}, Feature {feature_idx+1}')\n",
        "                ax.set_xlabel('Time Steps')\n",
        "                ax.set_ylabel('Value')\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle('VAE Reconstruction Performance', y=1.02, fontsize=16)\n",
        "        plt.show()\n",
        "\n",
        "        # Feature-wise reconstruction quality\n",
        "        self.plot_feature_reconstruction_quality(original, reconstructed)\n",
        "\n",
        "    def plot_feature_reconstruction_quality(self, original, reconstructed):\n",
        "        \"\"\"\n",
        "        Plot reconstruction quality per feature\n",
        "        \"\"\"\n",
        "        n_features = original.shape[2]\n",
        "        mse_per_feature = []\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            feature_mse = []\n",
        "            for sample_idx in range(original.shape[0]):\n",
        "                orig_feature = original[sample_idx, :, feature_idx]\n",
        "                recon_feature = reconstructed[sample_idx, :, feature_idx]\n",
        "                mse = mean_squared_error(orig_feature, recon_feature)\n",
        "                feature_mse.append(mse)\n",
        "            mse_per_feature.append(np.mean(feature_mse))\n",
        "\n",
        "        # Plot feature reconstruction quality\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.bar(range(1, n_features+1), mse_per_feature)\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('Average MSE')\n",
        "        plt.title('Reconstruction Quality per Feature')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot latent space representation\n",
        "        if original.shape[0] > 1:\n",
        "            encoded = self.encoder.predict(original)\n",
        "            mu, logvar, z = encoded\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.scatter(z[:, 0], z[:, 1], c=range(len(z)), cmap='viridis')\n",
        "            plt.xlabel('Latent Dimension 1')\n",
        "            plt.ylabel('Latent Dimension 2')\n",
        "            plt.title('Latent Space Representation')\n",
        "            plt.colorbar(label='Sample Index')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nFeature Reconstruction Quality (MSE):\")\n",
        "        for i, mse in enumerate(mse_per_feature):\n",
        "            print(f\"Feature {i+1}: {mse:.6f}\")\n",
        "\n",
        "    def quick_vae_check(self, train_data_path):\n",
        "        \"\"\"\n",
        "        Quick VAE performance check - just print metrics\n",
        "        \"\"\"\n",
        "        print(\"Running quick VAE performance check...\")\n",
        "\n",
        "        if self.encoder is None:\n",
        "            if not self.load_vae_models():\n",
        "                print(\"❌ VAE models not found!\")\n",
        "                return\n",
        "\n",
        "        # Load a small sample\n",
        "        train_data = np.load(train_data_path)\n",
        "        test_sample = train_data[:10]  # Just 10 samples\n",
        "\n",
        "        # Test reconstruction\n",
        "        encoded = self.encoder.predict(test_sample, verbose=0)\n",
        "        mu, logvar, z = encoded\n",
        "        reconstructed = self.decoder.predict(z, verbose=0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(test_sample.flatten(), reconstructed.flatten())\n",
        "        mae = mean_absolute_error(test_sample.flatten(), reconstructed.flatten())\n",
        "\n",
        "        print(f\"✅ VAE Performance Check:\")\n",
        "        print(f\"   MSE: {mse:.6f}\")\n",
        "        print(f\"   MAE: {mae:.6f}\")\n",
        "        print(f\"   Input shape: {test_sample.shape}\")\n",
        "        print(f\"   Output shape: {reconstructed.shape}\")\n",
        "\n",
        "        if mse < 0.1:\n",
        "            print(\"✅ Good reconstruction quality!\")\n",
        "        elif mse < 0.5:\n",
        "            print(\"⚠️  Moderate reconstruction quality\")\n",
        "        else:\n",
        "            print(\"❌ Poor reconstruction quality - consider retraining\")\n",
        "\n",
        "    def force_retrain_pipeline(self, train_data_path, target_samples=350000):\n",
        "        \"\"\"\n",
        "        Force complete retraining - deletes existing models and starts fresh\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"FORCE RETRAINING - STARTING FRESH\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Delete existing models\n",
        "        encoder_path = f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras'\n",
        "        decoder_path = f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "        if os.path.exists(encoder_path):\n",
        "            os.remove(encoder_path)\n",
        "            print(\"🗑️ Deleted existing encoder\")\n",
        "\n",
        "        if os.path.exists(decoder_path):\n",
        "            os.remove(decoder_path)\n",
        "            print(\"🗑️ Deleted existing decoder\")\n",
        "\n",
        "        # Reset internal models\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.vae = None\n",
        "\n",
        "        print(\"Starting fresh VAE training...\")\n",
        "\n",
        "        # Run pipeline with force retrain\n",
        "        return self.run_fast_generation(train_data_path, target_samples, force_retrain=True)\n",
        "\n",
        "# Usage Examples - Choose your approach:\n",
        "if __name__ == \"__main__\":\n",
        "    generator = VAEDataGenerator()\n",
        "\n",
        "    # Define your data path\n",
        "    data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy'\n",
        "\n",
        "    # RECOMMENDED: Smart pipeline (checks existing files)\n",
        "    print(\"=\"*60)\n",
        "    print(\"RUNNING SMART PIPELINE (RECOMMENDED)\")\n",
        "    print(\"=\"*60)\n",
        "    results = generator.run_smart_pipeline(data_path, target_samples=350000)\n",
        "\n",
        "    # Alternative options (uncomment to use):\n",
        "\n",
        "    # Option 1: Force regeneration (always runs everything)\n",
        "    # results = generator.run_fast_generation(data_path, target_samples=350000)\n",
        "\n",
        "    # Option 2: Force complete retraining\n",
        "    # results = generator.force_retrain_pipeline(data_path, target_samples=350000)\n",
        "\n",
        "    # Option 3: Force retrain but keep existing as backup\n",
        "    # results = generator.run_fast_generation(data_path, target_samples=350000, force_retrain=True)\n",
        "\n",
        "    # Performance evaluation\n",
        "    if results is not None:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATING VAE PERFORMANCE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Quick performance check\n",
        "        generator.quick_vae_check(data_path)\n",
        "\n",
        "        # Detailed evaluation with plots (optional - uncomment to use)\n",
        "        # detailed_performance = generator.evaluate_vae_performance(data_path, num_samples=3)\n",
        "\n",
        "        print(\"\\n🎉 PIPELINE COMPLETE!\")\n",
        "        print(f\"   Synthetic data shape: {results['synthetic_data'].shape}\")\n",
        "        print(f\"   VAR windows shape: {results['var_windows'].shape}\")\n",
        "    else:\n",
        "        print(\"❌ Pipeline failed - please train VAE first!\")\n",
        "\n",
        "# Quick usage for existing users:\n",
        "\"\"\"\n",
        "# Just copy and paste this for immediate use:\n",
        "\n",
        "generator = VAEDataGenerator()\n",
        "results = generator.run_smart_pipeline(\n",
        "    r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy',\n",
        "    target_samples=350000\n",
        ")\n",
        "\n",
        "# Check performance\n",
        "generator.quick_vae_check(\n",
        "    r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy'\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb",
      "authorship_tag": "ABX9TyMUCJ0Gm9nra6zk2XlZJowS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}