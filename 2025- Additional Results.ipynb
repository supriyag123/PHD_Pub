{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/2025-%20Additional%20Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "8a536a16-0758-492e-dafa-7deb4b2a2c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting sequence:  112591  out of  136300\n",
            "starting point:  18  out of  174\n",
            "small_w_k:  12  large_w_k:  18\n",
            "best_window_k:  12\n",
            "starting point:  24  out of  174\n",
            "small_w_k:  6  large_w_k:  12\n",
            "best_window_k:  12\n",
            "starting point:  30  out of  174\n",
            "small_w_k:  12  large_w_k:  18\n",
            "best_window_k:  18\n",
            "starting point:  36  out of  174\n",
            "small_w_k:  18  large_w_k:  24\n",
            "best_window_k:  24\n",
            "starting point:  42  out of  174\n",
            "small_w_k:  24  large_w_k:  30\n",
            "best_window_k:  30\n",
            "starting point:  48  out of  174\n",
            "small_w_k:  30  large_w_k:  36\n",
            "best_window_k:  36\n",
            "starting point:  54  out of  174\n",
            "small_w_k:  36  large_w_k:  42\n",
            "best_window_k:  42\n",
            "starting point:  60  out of  174\n",
            "small_w_k:  42  large_w_k:  48\n",
            "best_window_k:  48\n",
            "starting point:  66  out of  174\n",
            "small_w_k:  48  large_w_k:  54\n",
            "best_window_k:  54\n",
            "starting point:  72  out of  174\n",
            "small_w_k:  54  large_w_k:  60\n",
            "best_window_k:  60\n",
            "starting point:  78  out of  174\n",
            "small_w_k:  60  large_w_k:  66\n",
            "best_window_k:  60\n",
            "starting point:  84  out of  174\n",
            "small_w_k:  54  large_w_k:  60\n",
            "best_window_k:  54\n",
            "starting point:  90  out of  174\n",
            "small_w_k:  48  large_w_k:  54\n",
            "best_window_k:  54\n",
            "starting point:  96  out of  174\n",
            "small_w_k:  54  large_w_k:  60\n",
            "best_window_k:  60\n",
            "starting point:  102  out of  174\n",
            "small_w_k:  60  large_w_k:  66\n",
            "best_window_k:  66\n",
            "starting point:  108  out of  174\n",
            "small_w_k:  66  large_w_k:  72\n",
            "best_window_k:  72\n",
            "starting point:  114  out of  174\n",
            "small_w_k:  72  large_w_k:  78\n",
            "best_window_k:  78\n",
            "starting point:  120  out of  174\n",
            "small_w_k:  78  large_w_k:  84\n",
            "best_window_k:  84\n",
            "starting point:  126  out of  174\n",
            "small_w_k:  84  large_w_k:  90\n",
            "best_window_k:  90\n",
            "starting point:  132  out of  174\n",
            "small_w_k:  90  large_w_k:  96\n",
            "best_window_k:  96\n",
            "starting point:  138  out of  174\n",
            "small_w_k:  96  large_w_k:  102\n",
            "best_window_k:  96\n",
            "starting point:  144  out of  174\n",
            "small_w_k:  90  large_w_k:  96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_window_k:  90\n",
            "starting point:  150  out of  174\n",
            "small_w_k:  84  large_w_k:  90\n",
            "best_window_k:  84\n",
            "starting point:  156  out of  174\n",
            "small_w_k:  78  large_w_k:  84\n",
            "best_window_k:  84\n",
            "starting point:  162  out of  174\n",
            "small_w_k:  84  large_w_k:  90\n",
            "best_window_k:  90\n",
            "starting point:  168  out of  174\n",
            "small_w_k:  90  large_w_k:  96\n",
            "best_window_k:  96\n",
            "starting point:  174  out of  174\n",
            "small_w_k:  96  large_w_k:  102\n",
            "best_window_k:  102\n",
            "getting best window for this sequenc 112591 small 96 large=  102 best window=  102\n",
            "[102]\n",
            "time: 3.43 s (started: 2025-03-25 15:18:34 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "\n",
        "\n",
        "\n",
        "##### Take one dataset at a time\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_AIRQUALITY/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/multivariate_long_sequences-TRAIN-DATA-FullSearch-VAR.npy')\n",
        "\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "##For VAR\n",
        "#TestSample = 10\n",
        "\n",
        "##For DyforGP\n",
        "TestSample = 1\n",
        "\n",
        "\n",
        "##Get large set of subsequence from previous steps (VAE)... this is not labelled yet.\n",
        "#results1=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/generated_large_subsquence2_data_V2.npy')\n",
        "#results1=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_AIRQUALITY/generated_large_subsquence2_data.npy')\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/generated_large_subsquence2_data.npy')\n",
        "\n",
        "x=results1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############### VAR PREDICTION TIMING ####################################################################################################################\n",
        "\n",
        "!pip install ipython-autotime\n",
        "##Start measuring time-----THIS ONE IS FOR VAR --------------------------------\n",
        "%load_ext autotime\n",
        "\n",
        "rmse_list = list()\n",
        "min_window_list = list()\n",
        "best_window_for_long_seq = list()\n",
        "best_window_for_long_seq.clear()\n",
        "#for i in range(2) :\n",
        "x_3d = x.reshape((x.shape[0],window_size,n_features))\n",
        "n_future = 1\n",
        "K= window_size\n",
        "\n",
        "import random\n",
        "rd = list()\n",
        "rd = random.sample(range(1, x_3d.shape[0]), TestSample)\n",
        "\n",
        "\n",
        "best_window_for_long_seq.clear()\n",
        "\n",
        "from statsmodels.tsa.api import VAR\n",
        "\n",
        "## Note for some reason first run takes longer. So run it twice to get a better time\n",
        "for i in rd :\n",
        "\n",
        "  rmse_list.clear()\n",
        "  for k in range(2,(round(K))):\n",
        "    cur_seq = x_3d[i]\n",
        "    df = pd.DataFrame(cur_seq, columns=['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12'])\n",
        "    #df = pd.DataFrame(cur_seq, columns=['V1','V2','V3','V4','V5','V6'])\n",
        "    df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "    model= VAR(df_train)\n",
        "    try:\n",
        "      model_fitted1 = model.fit(k)\n",
        "      forecast_input1 = df_train.values[-k:]\n",
        "      fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "      df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "      mse =  mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "      rmse_list.append(mse)\n",
        "    except:\n",
        "      rmse_list.append(99999)\n",
        "      print('VAR could not solve row number')\n",
        "      print(i, k)\n",
        "\n",
        "  #For this i, find minimum rmse for all sliding window, and corresponding sw size\n",
        "  min_index = rmse_list.index(min(rmse_list))\n",
        "  min_sw = min_index + 2\n",
        "  print('i=', i,'SW =', min_sw, rmse_list)\n",
        "  best_window_for_long_seq.append(min_sw)\n",
        "\n",
        "\n",
        "#OUT OF ALL LOOPS NOW\n",
        "#best_window_for_long_seq now contains the best multivariate window size for each of the long sequence i\n",
        "print(best_window_for_long_seq)\n",
        "Window = np.array(best_window_for_long_seq)\n",
        "y= Window\n",
        "\n",
        "##Commenting this for now but they may be needed\n",
        "#np.save(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/generated-data-true-window2-COHORT1.npy',y)\n",
        "#np.save(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/generated-data2-COHORT1.npy',x)\n",
        "\n",
        "\n",
        "########################## MLP PREDICTION TIMING ###################################################################################\n",
        "#---------------------------------------------------------Get parts of the results and combine them --------------------------------\n",
        "\n",
        "\n",
        "#y = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "#x = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/generated-data2.npy')\n",
        "\n",
        "#y = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_AIRQUALITY/generated-data-true-window2.npy')\n",
        "#x = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_AIRQUALITY/generated-data2.npy')\n",
        "\n",
        "x=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/generated-data2.npy')\n",
        "y=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/generated-data-true-window2.npy')\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "y_transformed = transformer.fit_transform(y.reshape(-1,1)).flatten()\n",
        "\n",
        "##Simply take first 100 records - no need to sample... we just need to check prediction time\n",
        "\n",
        "x_test = x[rd]\n",
        "y_test = y_transformed[rd]\n",
        "#------------------------------------------------------Retrieve Trained MLP Model----------\n",
        "\n",
        "#base_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras')\n",
        "#base_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/AQ_MLP_model_Daily.keras')\n",
        "base_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/ELEC_MLP_model_Daily.keras')\n",
        "\n",
        "#-----------------------------------------------------Run Prediction on MLP------------------------\n",
        "## Note for some reason first run takes longer. So run it twice to get a better time\n",
        "\n",
        "y_pred_raw = base_model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "print(y_test_pred)\n",
        "\n",
        "############### DyFor GP PREDICTION TIMING ####################################################################################################################\n",
        "############### VAR PREDICTION TIMING ####################################################################################################################\n",
        "\n",
        "\n",
        "##############DyFOR GP Heuristic#######################################################\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "n_future = n_features\n",
        "\n",
        "def get_window_size (series, pred_point, small_w, large_w):\n",
        "  # pred_point ==> the point of prediction (starting point)\n",
        "  # small_w ==> smaller window size preceding the pred_point\n",
        "  # large_w ==> larger window size preceding the pred_point\n",
        "  # series is flttened long subsequence --> multivariate converted into univariate by flattening...\n",
        "\n",
        "\n",
        "  pred_l_start = pred_point - large_w\n",
        "  pred_s_start = pred_point - small_w\n",
        "  pred_l_window = series[pred_l_start:pred_point]\n",
        "  pred_s_window = series[pred_s_start:pred_point]\n",
        "  modell = ARIMA(pred_l_window, order=(2,0,0))\n",
        "  fitl = modell.fit()\n",
        "  forecastl = fitl.forecast(steps=n_future)\n",
        "  models = ARIMA(pred_s_window, order=(2,0,0))\n",
        "  fits = models.fit()\n",
        "  forecasts = fits.forecast(steps=n_future)\n",
        "  mse_l = mean_squared_error(series[pred_point:pred_point+n_future], forecastl)\n",
        "  mse_s = mean_squared_error(series[pred_point:pred_point+n_future], forecasts)\n",
        "  if mse_l < mse_s:\n",
        "    return large_w\n",
        "  else:\n",
        "    return small_w\n",
        "\n",
        "\n",
        "best_window_for_long_seq = list()\n",
        "best_window_for_long_seq.clear()\n",
        "\n",
        "\n",
        "#for i in range(x.shape[0]):\n",
        "for i in rd:\n",
        "  print(\"starting sequence: \", i, \" out of \", x.shape[0])\n",
        "  cur_seq = x[i,:]\n",
        "  start_small_w = 2\n",
        "  start_large_w = 3\n",
        "  start_index = start_large_w\n",
        "  ### we need to increment by number of variables, as we have flattened the ime series\n",
        "  for k in range (start_index*n_features, x.shape[1]-n_features+1, n_features):\n",
        "    print(\"starting point: \", k, \" out of \", x.shape[1]-n_features)\n",
        "    pred_point_k = k\n",
        "    small_w_k = start_small_w*n_features\n",
        "    large_w_k = start_large_w*n_features\n",
        "    print(\"small_w_k: \", small_w_k, \" large_w_k: \", large_w_k)\n",
        "    best_window_k = get_window_size(cur_seq, pred_point_k, small_w_k, large_w_k)\n",
        "    print(\"best_window_k: \", best_window_k)\n",
        "    if best_window_k == large_w_k:\n",
        "      start_large_w = start_large_w + 1\n",
        "      start_small_w = start_small_w + 1\n",
        "    if best_window_k == small_w_k:\n",
        "      if start_small_w > 1:\n",
        "        start_large_w = start_large_w -1\n",
        "        start_small_w = start_small_w -1\n",
        "      else:\n",
        "        start_large_w = start_large_w\n",
        "        start_small_w = start_small_w\n",
        "  print(\"getting best window for this sequenc\",  i, \"small\",small_w_k, \"large= \",large_w_k, \"best window= \", best_window_k)\n",
        "  #final_best_window =  get_window_size(cur_seq, k, small_w_k, large_w_k)\n",
        "    #print(\"final_best_window: \", final_best_window)\n",
        "  best_window_for_long_seq.append(best_window_k)\n",
        "\n",
        "\n",
        "print(best_window_for_long_seq)\n",
        "Window = np.array(best_window_for_long_seq)/n_features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/2025-%20Additional%20Results.ipynb",
      "authorship_tag": "ABX9TyOl3bcw02u5J3x4Vg9TuePZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}