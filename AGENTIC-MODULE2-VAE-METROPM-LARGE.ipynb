{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE-METROPM-LARGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bx-5b_puABG1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "import gc\n",
        "from tqdm import tqdm   # nice progress bar\n",
        "\n",
        "# Load data\n",
        "data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy'\n",
        "labelpath = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/window_labels_3class.npy'\n",
        "train_mask_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/train_mask.npy'\n",
        "data = np.load(data_path)\n",
        "label = np.load(labelpath)\n",
        "train_mask = np.load(train_mask_path)\n",
        "mlp_mask = np.logical_and(train_mask, label == 0)\n",
        "train_data = data[mlp_mask]\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "maxval = train_data.shape[0]\n",
        "count_train = int(math.ceil(0.8*maxval))\n",
        "x_train = train_data[:count_train]\n",
        "x_test = train_data[count_train:]\n",
        "\n",
        "# Always define the custom Sampling layer (needed for loading existing models)\n",
        "saving.get_custom_objects().clear()\n",
        "\n",
        "@saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def __init__(self, factor):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "\n",
        "def safe_decode(decoder, z, batch_size=32):\n",
        "    outputs = []\n",
        "    for i in range(0, len(z), batch_size):\n",
        "        batch = z[i:i+batch_size]\n",
        "        out = decoder(batch)\n",
        "        outputs.append(out.numpy())\n",
        "    return np.concatenate(outputs, axis=0)\n",
        "\n",
        "# Check if trained VAE models already exist\n",
        "encoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-encoder-latent5-dim256.keras'\n",
        "decoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "    print(\"Found existing trained VAE models. Loading...\")\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "    print(\"VAE models loaded successfully!\")\n",
        "    history = None  # No training history since we didn't train\n",
        "\n",
        "else:\n",
        "    print(\"No existing VAE models found. Training new VAE...\")\n",
        "\n",
        "    # Build the encoder\n",
        "    latent_dim = 5\n",
        "    intermediate_dim = 256\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(window_size, n_features), name=\"encoder_input\")\n",
        "    x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "    xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "    x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\")(xx)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "    z = Sampling(1)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "\n",
        "    # Decoder\n",
        "    inp_z = Input(shape=(latent_dim,), name=\"decoder\")\n",
        "    x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "    x2 = layers.Dense(int(intermediate_dim/2), name=\"Dense2\")(x1)\n",
        "    x22 = layers.LSTM(int(intermediate_dim/2), activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "    x3 = layers.LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "    decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "    decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "    # Parameters\n",
        "    n_epochs = 20\n",
        "    klstart = 5\n",
        "    kl_annealtime = n_epochs - klstart\n",
        "    weight = K.variable(0.0)\n",
        "\n",
        "    # Define the VAE as a Model with a custom train_step\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(VAE, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(data)\n",
        "                reconstruction = self.decoder(z)\n",
        "                reconstruction_loss = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "                )\n",
        "\n",
        "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = reconstruction_loss + (weight * kl_loss)\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        def test_step(self, data):\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "    # CALLBACKS\n",
        "    es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "    class AnnealingCallback(Callback):\n",
        "        def __init__(self, weight):\n",
        "            self.weight = weight\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if epoch > klstart and epoch < klstart * 1.2:\n",
        "                new_weight = min(K.get_value(self.weight) + (1. / kl_annealtime), 1.)\n",
        "                K.set_value(self.weight, new_weight)\n",
        "            print(\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "    # Train the VAE\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "    history = vae.fit(x_train,\n",
        "                      epochs=n_epochs,\n",
        "                      batch_size=200,\n",
        "                      validation_split=0.1,\n",
        "                      callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "    # Save models\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "    print(\"VAE training complete and models saved!\")\n",
        "\n",
        "    # Reload models to ensure consistency\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "\n",
        "# Plot training history (only if we actually trained)\n",
        "if history is not None:\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['reconstruction_loss'], label='reconstruction_loss')\n",
        "    plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "    # Just Loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training plots since VAE was loaded from existing models\")\n",
        "\n",
        "# PLOT TRAIN RECONSTRUCTION\n",
        "x_train_small = x_train[:10000]\n",
        "X_test_encoded = encoder.predict(x_train_small)\n",
        "z = X_test_encoded[2]     # shape (10000, 5)\n",
        "# Decode safely\n",
        "X_test_predict = safe_decode(decoder, z, batch_size=32)\n",
        "\n",
        "\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_train[0:10000, :, 5], \"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\", label=\"reconstructed\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT TEST RECONSTRUCTION\n",
        "x_test_small = x_test[:10000]\n",
        "X_test_encoded = encoder.predict(x_test_small)\n",
        "z = X_test_encoded[2]     # shape (10000, 5)\n",
        "# Decode safely\n",
        "X_test_predict = safe_decode(decoder, z, batch_size=32)\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_test_small[0:1000, :, 5], \"r\")\n",
        "plt.plot(X_test_predict[0:1000, :, 5], \"b\")\n",
        "plt.show()\n",
        "\n",
        "#ANother test - pick a specific example\n",
        "i = 0  # pick a specific example\n",
        "x_true  = x_test[i, :, 5]          # feature 6\n",
        "z_mean, z_log_var, z = encoder.predict(x_test[i:i+1])\n",
        "x_recon = decoder.predict(z)[0, :, 5]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(x_true, 'r', label='True')\n",
        "plt.plot(x_recon, 'b', label='Recon')\n",
        "plt.legend()\n",
        "plt.title(f'Window {i} â€“ Feature 6')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Check if generated data already exists\n",
        "generated_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated_large_subsquence2_data.npy'\n",
        "\n",
        "if os.path.exists(generated_data_path):\n",
        "    print(\"Found existing generated data. Loading...\")\n",
        "    results1 = np.load(generated_data_path)\n",
        "    print(f\"Loaded generated data with shape: {results1.shape}\")\n",
        "else:\n",
        "    print(\"No existing generated data found. Generating new data...\")\n",
        "    # Generate data for MLP\n",
        "\n",
        "    N_SAMPLES = 10000          # number of real windows to sample\n",
        "    generator_multiply =   5            # synthetic per real window\n",
        "    BATCH_SIZE = 2000           # safe batch size for VAE inference\n",
        "\n",
        "    # =========================================================\n",
        "    # 1. RANDOMLY SAMPLE 50,000 WINDOWS\n",
        "    # =========================================================\n",
        "    idx = np.random.choice(n_seq, N_SAMPLES, replace=False)\n",
        "    sampled_data = train_data[idx]\n",
        "\n",
        "        # =========================================================\n",
        "    # 2. USE VAE (encoder + decoder) TO GENERATE SYNTHETIC WINDOWS\n",
        "    # =========================================================\n",
        "\n",
        "    generated_list = []\n",
        "\n",
        "    num_batches = int(np.ceil(N_SAMPLES / BATCH_SIZE))\n",
        "    print(f\"Generating synthetic data in {num_batches} batches...\")\n",
        "\n",
        "    for b in range(num_batches):\n",
        "        start = b * BATCH_SIZE\n",
        "        end = min((b+1)*BATCH_SIZE, N_SAMPLES)\n",
        "        batch = sampled_data[start:end]\n",
        "\n",
        "        # Encode\n",
        "        z_mean, z_log_var, _ = encoder.predict(batch, verbose=0)\n",
        "\n",
        "        # Generate MULTIPLIER random z for each window\n",
        "        sigma = tf.exp(0.5 * z_log_var)\n",
        "\n",
        "        for m in range(generator_multiply):\n",
        "            z_new = tf.random.normal(\n",
        "                shape=z_mean.shape,\n",
        "                mean=z_mean,\n",
        "                stddev=sigma\n",
        "            )\n",
        "            decoded = decoder.predict(z_new, verbose=0)\n",
        "\n",
        "            # decoded shape: (batch, window_size, n_features)\n",
        "            generated_list.append(decoded)\n",
        "\n",
        "        print(f\" Batch {b+1}/{num_batches} complete\")\n",
        "          # Stack all generated batches\n",
        "    generated_data = np.concatenate(generated_list, axis=0)\n",
        "    print(f\"Synthetic windows: {generated_data.shape}\")\n",
        "    # =========================================================\n",
        "    # 3. COMBINE REAL + SYNTHETIC DATA\n",
        "    # =========================================================\n",
        "    combined_data = np.concatenate([train_data, generated_data], axis=0)\n",
        "    results1 =combined_data\n",
        "    np.save(generated_data_path, results1)\n",
        "    print(f\"Generated and saved new data with shape: {results1.shape}\")\n",
        "\n",
        "# Generate window labels using VAR analysis - Dynamic batching with checkpoints\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "# Check if final VAR analysis results already exist\n",
        "final_window_labels_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy'\n",
        "final_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data2.npy'\n",
        "\n",
        "if os.path.exists(final_window_labels_path):\n",
        "    print(\"Found existing window labels...\")\n",
        "    optimal_K_for_ALL = np.load(final_window_labels_path)\n",
        "    print(f\"Loaded optimal K for ALL windows with shape: {optimal_K_for_ALL.shape}\")\n",
        "else:\n",
        "    # ----------------------------------------------------------\n",
        "    # 1) TRAIN LSTM NEXT-STEP PREDICTOR ON A SUBSET\n",
        "    # ----------------------------------------------------------\n",
        "    print(\"\\n=== STEP 1: Training LSTM Next-Step predictor on subset ===\")\n",
        "    N_total = results1.shape[0]\n",
        "    train_subset = min(50000, N_total)  # in case you have fewer than 50k\n",
        "\n",
        "    idx = np.random.choice(N_total, train_subset, replace=False)\n",
        "    nsp_train = results1[idx]       # shape: (train_subset, window_size, n_features)\n",
        "\n",
        "    # VARIABLE LENGTH NSP MODEL\n",
        "    inputs = keras.Input(shape=(None, n_features))\n",
        "    x = layers.LSTM(128)(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(n_features)(x)\n",
        "\n",
        "    nsp_model = keras.Model(inputs, outputs)\n",
        "    nsp_model.compile(optimizer='adam', loss='mse')\n",
        "    nsp_model.summary()\n",
        "\n",
        "    # TRAIN on full K-length windows\n",
        "    X = nsp_train[:, :-1, :]   # (subset, window_size-1, n_features)\n",
        "    y = nsp_train[:, -1, :]    # (subset, n_features)\n",
        "\n",
        "    nsp_model.fit(X, y, epochs=5, batch_size=512)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # 2) LABEL ALL WINDOWS WITH OPTIMAL K (CHECKPOINTED)\n",
        "    # ----------------------------------------------------------\n",
        "    print(\"\\n=== STEP 2: Generating optimal K for ALL windows with checkpointing ===\")\n",
        "\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/NEXTSTEP\"\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    CHECKPOINT_FILE = f\"{CHECKPOINT_DIR}/klabel_progress.pkl\"\n",
        "\n",
        "    BATCH_SIZE_SAVE = 20000       # save every 20k sequences\n",
        "    K_MAX = window_size           # search K in range 2..window_size\n",
        "\n",
        "    SEQ_DATA = results1           # full dataset\n",
        "    N = SEQ_DATA.shape[0]\n",
        "\n",
        "    print(f\"Total sequences to label: {N:,}\")\n",
        "    print(f\"Searching K in [2, {K_MAX}]\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Helper function: compute optimal K for one sequence\n",
        "    # -----------------------------\n",
        "    def get_optimal_k(window, model, K):\n",
        "        W = window.shape[0]      # window_size\n",
        "        F = window.shape[1]      # n_features\n",
        "\n",
        "        target = window[-1].reshape(1, F)\n",
        "\n",
        "        num_k = K - 1\n",
        "        past_batch = np.zeros((num_k, W-1, F), dtype=np.float32)\n",
        "\n",
        "        for k in range(2, K+1):\n",
        "            seq_len = k - 1\n",
        "            past = window[-k:-1, :]      # (k-1, F)\n",
        "\n",
        "            # IMPORTANT: reset row\n",
        "            past_batch[k-2, :, :] = 0\n",
        "            past_batch[k-2, -seq_len:, :] = past\n",
        "\n",
        "        pred_batch = model.predict(past_batch, verbose=0)\n",
        "\n",
        "        mse = np.mean((pred_batch - target)**2, axis=1)\n",
        "        return int(np.argmin(mse) + 2)\n",
        "\n",
        "    # --------------------------------\n",
        "    # RESUME SUPPORT\n",
        "    # --------------------------------\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        print(\"\\nResuming from checkpoint...\")\n",
        "        with open(CHECKPOINT_FILE, \"rb\") as f:\n",
        "            start_idx, collected_labels = pickle.load(f)\n",
        "        print(f\"Resuming at index {start_idx:,}\")\n",
        "    else:\n",
        "        print(\"\\nStarting fresh labeling run...\")\n",
        "        start_idx = 0\n",
        "        collected_labels = []\n",
        "\n",
        "    # Ensure it's a Python list (pickle-safe)\n",
        "    if not isinstance(collected_labels, list):\n",
        "        collected_labels = collected_labels.tolist()\n",
        "\n",
        "    # --------------------------------\n",
        "    # MAIN PROCESSING LOOP\n",
        "    # --------------------------------\n",
        "    for i in tqdm(range(start_idx, N), total=N, initial=start_idx):\n",
        "        seq = SEQ_DATA[i]                      # one window: (window_size, n_features)\n",
        "        klabel = get_optimal_k(seq, nsp_model, K=K_MAX)\n",
        "        collected_labels.append(klabel)\n",
        "\n",
        "        # ------- SAVE CHECKPOINT ----------\n",
        "        if (i + 1) % BATCH_SIZE_SAVE == 0:\n",
        "            with open(CHECKPOINT_FILE, \"wb\") as f:\n",
        "                pickle.dump((i + 1, collected_labels), f)\n",
        "\n",
        "            batch_id = (i + 1) // BATCH_SIZE_SAVE\n",
        "            np.save(f\"{CHECKPOINT_DIR}/KLABEL_BATCH_{batch_id}.npy\",\n",
        "                    np.array(collected_labels[-BATCH_SIZE_SAVE:]))\n",
        "\n",
        "            print(f\"\\nBatch {batch_id} saved safely at index {i+1}\")\n",
        "            gc.collect()\n",
        "\n",
        "    # ----------------------------\n",
        "    # FINAL SAVE\n",
        "    # ----------------------------\n",
        "    print(\"\\nFinal save...\")\n",
        "    optimal_K_for_ALL = np.array(collected_labels)\n",
        "    np.save(final_window_labels_path, optimal_K_for_ALL)\n",
        "    np.save(final_data_path, results1)\n",
        "\n",
        "\n",
        "print(\"VAE training complete. Generated data and window labels saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUj3FPEbLTUz",
        "outputId": "bcdede25-f8d4-4889-edc2-6e3c186b1130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  1 04:31:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             51W /  400W |    1523MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    # If you need to reload the model:\n",
        "    # nsp_model = keras.models.load_model('your_model_path')\n",
        "\n",
        "    # Or just verify it's on GPU\n",
        "    print(f\"Model device: {nsp_model.layers[0].weights[0].device if nsp_model.layers else 'checking...'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "kv2mNjRjXdPT",
        "outputId": "acc63690-ff67-423b-dcfb-db7dcf68b981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nsp_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4004207739.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Or just verify it's on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model device: {nsp_model.layers[0].weights[0].device if nsp_model.layers else 'checking...'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nsp_model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE-METROPM-LARGE.ipynb",
      "authorship_tag": "ABX9TyOpUkQ2PtawGkUKDQjXzxmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}