{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/SensorAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "outputId": "38ceea3c-ad53-4198-9f03-c72814074973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n",
            "RMSE: 1.3503097119121505 1.5002465652043568 2.837338449071969 1.5963914298880122 1.020955371098352\n",
            "MAPE: 0.6891745407226914 0.7660456714610211 1.7857976818782204 0.750466542640601 0.377035791594537\n",
            "MAE: 0.1685213302116819 0.18072839798571852 0.20651606722068036 0.1586292978244769 0.13245363757094158\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import deque\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning (choose one: keras or torch)\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "# Statistical modeling\n",
        "try:\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "\n",
        "\n",
        "class SensorAgent:\n",
        "    \"\"\"\n",
        "    Autonomous Sensor Agent for IoT time series anomaly and drift detection.\n",
        "\n",
        "    This agent operates independently on a single sensor's data stream,\n",
        "    maintaining its own model, memory, and adaptive thresholding mechanisms.\n",
        "    Designed for integration into larger multi-agent frameworks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: str,\n",
        "                 model_type: str = 'lstm_autoencoder',  # 'lstm_autoencoder', 'vae', 'arima'\n",
        "                 window_length: int = 50,\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1,\n",
        "                 min_training_samples: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize Sensor Agent.\n",
        "\n",
        "        Args:\n",
        "            sensor_id: Unique identifier for this sensor\n",
        "            model_type: Type of model ('lstm_autoencoder', 'vae', 'arima')\n",
        "            window_length: Length of input subsequences\n",
        "            memory_size: Size of rolling memory buffer\n",
        "            threshold_k: Multiplier for adaptive threshold (mean + k*std)\n",
        "            drift_threshold: Threshold for drift detection\n",
        "            min_training_samples: Minimum samples needed before training\n",
        "        \"\"\"\n",
        "        self.sensor_id = sensor_id\n",
        "        self.model_type = model_type\n",
        "        self.window_length = window_length\n",
        "        self.memory_size = memory_size\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.min_training_samples = min_training_samples\n",
        "\n",
        "        # Memory buffers - core of agentic behavior\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)  # For drift detection\n",
        "\n",
        "        # Model and preprocessing\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "\n",
        "        # Adaptive statistics\n",
        "        self.rolling_stats = {\n",
        "            'mean': 0.0,\n",
        "            'std': 1.0,\n",
        "            'q95': 0.0,\n",
        "            'q99': 0.0\n",
        "        }\n",
        "\n",
        "        # Drift detection state\n",
        "        self.baseline_errors = None\n",
        "        self.drift_detected_count = 0\n",
        "\n",
        "        # Agent state\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.last_retrain_time = None\n",
        "\n",
        "    def _build_lstm_autoencoder(self) -> Model:\n",
        "        \"\"\"Build LSTM Autoencoder model.\"\"\"\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise ImportError(\"Keras/TensorFlow not available. Install with: pip install tensorflow\")\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1))\n",
        "        encoded = LSTM(32, activation='relu', return_sequences=False)(inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = RepeatVector(self.window_length)(encoded)\n",
        "        decoded = LSTM(32, activation='relu', return_sequences=True)(decoded)\n",
        "        outputs = TimeDistributed(Dense(1, activation='linear'))(decoded)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _build_vae(self) -> Model:\n",
        "        \"\"\"Build Variational Autoencoder model.\"\"\"\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise ImportError(\"Keras/TensorFlow not available. Install with: pip install tensorflow\")\n",
        "\n",
        "        latent_dim = 16\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1))\n",
        "        x = LSTM(32, return_sequences=False)(inputs)\n",
        "        z_mean = Dense(latent_dim)(x)\n",
        "        z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "        # Sampling function\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = RepeatVector(self.window_length)(z)\n",
        "        decoded = LSTM(32, return_sequences=True)(decoder_input)\n",
        "        outputs = TimeDistributed(Dense(1))(decoded)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "\n",
        "        # VAE loss\n",
        "        reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        vae_loss = reconstruction_loss + 0.1 * kl_loss\n",
        "        model.add_loss(vae_loss)\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, training_data: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Train the sensor model on historical data.\n",
        "\n",
        "        Args:\n",
        "            training_data: Historical sensor data for training\n",
        "        \"\"\"\n",
        "        if len(training_data) < self.min_training_samples:\n",
        "            print(f\"Sensor {self.sensor_id}: Insufficient training data ({len(training_data)} < {self.min_training_samples})\")\n",
        "            return\n",
        "\n",
        "        print(f\"Sensor {self.sensor_id}: Training {self.model_type} model...\")\n",
        "\n",
        "        # Normalize data\n",
        "        training_data_scaled = self.scaler.fit_transform(training_data.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Create sequences for training\n",
        "        sequences = []\n",
        "        for i in range(len(training_data_scaled) - self.window_length + 1):\n",
        "            sequences.append(training_data_scaled[i:i + self.window_length])\n",
        "\n",
        "        X_train = np.array(sequences)\n",
        "\n",
        "        if self.model_type == 'lstm_autoencoder':\n",
        "            self.model = self._build_lstm_autoencoder()\n",
        "            X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "            self.model.fit(X_train_reshaped, X_train_reshaped,\n",
        "                          epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "        elif self.model_type == 'vae':\n",
        "            self.model = self._build_vae()\n",
        "            X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "            self.model.fit(X_train_reshaped, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "        elif self.model_type == 'arima':\n",
        "            if not STATSMODELS_AVAILABLE:\n",
        "                raise ImportError(\"Statsmodels not available. Install with: pip install statsmodels\")\n",
        "            # For ARIMA, we'll store parameters and fit on-demand\n",
        "            self.model = 'arima_fitted'  # Placeholder\n",
        "\n",
        "        # Compute baseline errors for drift detection\n",
        "        baseline_errors = []\n",
        "        for seq in X_train:\n",
        "            error = self._compute_anomaly_score(seq)\n",
        "            baseline_errors.append(error)\n",
        "\n",
        "        self.baseline_errors = np.array(baseline_errors)\n",
        "        self._update_rolling_stats(baseline_errors)\n",
        "\n",
        "        self.is_fitted = True\n",
        "        self.last_retrain_time = datetime.now()\n",
        "        print(f\"Sensor {self.sensor_id}: Model training completed\")\n",
        "\n",
        "    def _compute_anomaly_score(self, sequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute anomaly score for a sequence.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            return 0.0\n",
        "\n",
        "        # Normalize sequence\n",
        "        sequence_scaled = self.scaler.transform(sequence.reshape(-1, 1)).flatten()\n",
        "\n",
        "        if self.model_type in ['lstm_autoencoder', 'vae']:\n",
        "            # Reshape for model input\n",
        "            X = sequence_scaled.reshape(1, len(sequence_scaled), 1)\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "            reconstruction = reconstruction.reshape(-1)\n",
        "\n",
        "            # Compute reconstruction error\n",
        "            error = mean_squared_error(sequence_scaled, reconstruction)\n",
        "\n",
        "        elif self.model_type == 'arima':\n",
        "            try:\n",
        "                # Fit ARIMA model and compute forecast error\n",
        "                model = ARIMA(sequence_scaled, order=(2, 1, 2))\n",
        "                fitted_model = model.fit()\n",
        "                forecast = fitted_model.forecast(steps=1)[0]\n",
        "                error = abs(sequence_scaled[-1] - forecast)\n",
        "            except:\n",
        "                error = 0.0  # Fallback for ARIMA fitting issues\n",
        "\n",
        "        return float(error)\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]) -> None:\n",
        "        \"\"\"Update rolling statistics for adaptive thresholding.\"\"\"\n",
        "        if len(errors) == 0:\n",
        "            return\n",
        "\n",
        "        errors_array = np.array(errors)\n",
        "        self.rolling_stats['mean'] = np.mean(errors_array)\n",
        "        self.rolling_stats['std'] = np.std(errors_array) + 1e-8  # Avoid division by zero\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "\n",
        "    def _detect_drift(self) -> Tuple[bool, float]:\n",
        "        \"\"\"\n",
        "        Detect distribution drift in recent errors vs baseline.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (drift_detected, drift_score)\n",
        "        \"\"\"\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False, 0.0\n",
        "\n",
        "        recent_errors_array = np.array(list(self.recent_errors))\n",
        "\n",
        "        # Use Jensen-Shannon divergence for distribution comparison\n",
        "        try:\n",
        "            # Create histograms\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(recent_errors_array, bins=bins, density=True)\n",
        "\n",
        "            # Add small epsilon to avoid zero probabilities\n",
        "            hist_baseline += 1e-10\n",
        "            hist_recent += 1e-10\n",
        "\n",
        "            # Normalize to probabilities\n",
        "            hist_baseline /= hist_baseline.sum()\n",
        "            hist_recent /= hist_recent.sum()\n",
        "\n",
        "            # Compute Jensen-Shannon divergence\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            drift_detected = js_divergence > self.drift_threshold\n",
        "\n",
        "            return drift_detected, float(js_divergence)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to KS test if JS divergence fails\n",
        "            try:\n",
        "                ks_stat, p_value = stats.ks_2samp(self.baseline_errors, recent_errors_array)\n",
        "                drift_detected = p_value < 0.05\n",
        "                return drift_detected, float(ks_stat)\n",
        "            except:\n",
        "                return False, 0.0\n",
        "\n",
        "    def detect(self, subsequence: np.ndarray, timestamp: datetime = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Main detection method - processes a single subsequence.\n",
        "\n",
        "        Args:\n",
        "            subsequence: Input sensor data subsequence (shape: window_length,)\n",
        "            timestamp: Timestamp for this subsequence\n",
        "\n",
        "        Returns:\n",
        "            Detection results dictionary\n",
        "        \"\"\"\n",
        "        if timestamp is None:\n",
        "            timestamp = datetime.now()\n",
        "\n",
        "        if len(subsequence) != self.window_length:\n",
        "            raise ValueError(f\"Expected subsequence length {self.window_length}, got {len(subsequence)}\")\n",
        "\n",
        "        # Compute anomaly score\n",
        "        anomaly_score = self._compute_anomaly_score(subsequence)\n",
        "\n",
        "        # Update memory (agentic behavior)\n",
        "        self.data_memory.append(subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # Adaptive thresholding\n",
        "        current_threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        is_anomaly = anomaly_score > current_threshold\n",
        "\n",
        "        # Compute confidence based on how far the score is from threshold\n",
        "        confidence = min(1.0, abs(anomaly_score - current_threshold) / (self.rolling_stats['std'] + 1e-8))\n",
        "\n",
        "        # Drift detection\n",
        "        drift_flag, drift_score = self._detect_drift()\n",
        "\n",
        "        # Update statistics periodically (agentic adaptation)\n",
        "        if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "            self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly:\n",
        "            self.anomalies_detected += 1\n",
        "\n",
        "        if drift_flag:\n",
        "            self.drift_detected_count += 1\n",
        "\n",
        "        # Prepare output\n",
        "        result = {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"confidence\": float(confidence),\n",
        "            # Additional fields for master agent escalation\n",
        "            \"threshold_used\": float(current_threshold),\n",
        "            \"drift_score\": float(drift_score) if drift_flag else 0.0,\n",
        "            \"total_processed\": self.total_processed,\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def update_memory(self, new_data: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Update agent memory with new data batch.\n",
        "        For continuous learning and adaptation.\n",
        "        \"\"\"\n",
        "        for i in range(len(new_data) - self.window_length + 1):\n",
        "            subsequence = new_data[i:i + self.window_length]\n",
        "            self.data_memory.append(subsequence)\n",
        "\n",
        "    def should_retrain(self) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if model should be retrained based on drift and performance.\n",
        "        Agentic decision-making capability.\n",
        "        \"\"\"\n",
        "        if self.last_retrain_time is None:\n",
        "            return False\n",
        "\n",
        "        # Retrain if significant drift detected or poor recent performance\n",
        "        time_since_retrain = datetime.now() - self.last_retrain_time\n",
        "        drift_rate = self.drift_detected_count / max(1, self.total_processed)\n",
        "\n",
        "        return (time_since_retrain.days > 7 or  # Weekly retraining\n",
        "                drift_rate > 0.1 or  # High drift rate\n",
        "                (len(self.error_memory) > 500 and\n",
        "                 np.mean(list(self.error_memory)[-100:]) > 2 * self.rolling_stats['mean']))\n",
        "\n",
        "    def get_status(self) -> Dict:\n",
        "        \"\"\"Get current agent status and diagnostics.\"\"\"\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"model_type\": self.model_type,\n",
        "            \"is_fitted\": self.is_fitted,\n",
        "            \"total_processed\": self.total_processed,\n",
        "            \"anomalies_detected\": self.anomalies_detected,\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_detected_count\": self.drift_detected_count,\n",
        "            \"memory_utilization\": len(self.error_memory) / self.memory_size,\n",
        "            \"current_threshold\": self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std'],\n",
        "            \"rolling_stats\": self.rolling_stats.copy(),\n",
        "            \"should_retrain\": self.should_retrain()\n",
        "        }\n",
        "\n",
        "\n",
        "def simulate_sensor_data(num_sensors: int = 12, num_samples: int = 2000,\n",
        "                        anomaly_prob: float = 0.05) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Simulate IoT sensor data for testing.\n",
        "\n",
        "    Args:\n",
        "        num_sensors: Number of sensors to simulate\n",
        "        num_samples: Number of time steps per sensor\n",
        "        anomaly_prob: Probability of anomaly at each time step\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of sensor_id -> time series data\n",
        "    \"\"\"\n",
        "    sensors_data = {}\n",
        "\n",
        "    for i in range(num_sensors):\n",
        "        sensor_id = f\"sensor_{i+1:02d}\"\n",
        "\n",
        "        # Generate base signal (seasonal + trend + noise)\n",
        "        t = np.linspace(0, 4*np.pi, num_samples)\n",
        "        base_signal = (2 * np.sin(t) + 0.5 * np.sin(5*t) +\n",
        "                      0.1 * t + np.random.normal(0, 0.2, num_samples))\n",
        "\n",
        "        # Add anomalies\n",
        "        anomaly_mask = np.random.random(num_samples) < anomaly_prob\n",
        "        base_signal[anomaly_mask] += np.random.normal(0, 3, np.sum(anomaly_mask))\n",
        "\n",
        "        sensors_data[sensor_id] = base_signal\n",
        "\n",
        "    return sensors_data\n",
        "\n",
        "\n",
        "# Example usage and multi-sensor loop\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Example demonstration of multi-sensor agent framework.\n",
        "    Shows how to deploy multiple independent sensor agents.\n",
        "    \"\"\"\n",
        "    print(\"=== IoT Sensor Agent Framework Demo ===\\n\")\n",
        "\n",
        "    # Simulate sensor data\n",
        "    print(\"1. Generating simulated sensor data...\")\n",
        "    sensors_data = simulate_sensor_data(num_sensors=12, num_samples=1500)\n",
        "    window_length = 50\n",
        "\n",
        "    # Initialize sensor agents\n",
        "    print(\"2. Initializing sensor agents...\")\n",
        "    agents = {}\n",
        "    for sensor_id in sensors_data.keys():\n",
        "        agent = SensorAgent(\n",
        "            sensor_id=sensor_id,\n",
        "            model_type='lstm_autoencoder',  # Can vary per sensor\n",
        "            window_length=window_length,\n",
        "            memory_size=500,\n",
        "            threshold_k=2.5,\n",
        "            drift_threshold=0.15\n",
        "        )\n",
        "        agents[sensor_id] = agent\n",
        "\n",
        "    # Train agents\n",
        "    print(\"3. Training agents on historical data...\")\n",
        "    for sensor_id, agent in agents.items():\n",
        "        # Use first 70% for training\n",
        "        training_data = sensors_data[sensor_id][:int(0.7 * len(sensors_data[sensor_id]))]\n",
        "        agent.fit(training_data)\n",
        "\n",
        "    # Real-time detection simulation\n",
        "    print(\"4. Running real-time anomaly detection...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Process remaining data as streaming\n",
        "    test_start = int(0.7 * len(list(sensors_data.values())[0]))\n",
        "    anomaly_results = []\n",
        "\n",
        "    for time_step in range(test_start, len(list(sensors_data.values())[0]) - window_length + 1):\n",
        "        timestamp = datetime.now() + timedelta(seconds=time_step)\n",
        "        step_anomalies = []\n",
        "\n",
        "        # Process each sensor independently (parallel in real deployment)\n",
        "        for sensor_id, agent in agents.items():\n",
        "            if agent.is_fitted:\n",
        "                # Extract current window\n",
        "                subsequence = sensors_data[sensor_id][time_step:time_step + window_length]\n",
        "\n",
        "                # Run detection\n",
        "                result = agent.detect(subsequence, timestamp)\n",
        "\n",
        "                # Collect anomalies for master agent escalation\n",
        "                if result['is_anomaly'] or result['drift_flag']:\n",
        "                    step_anomalies.append(result)\n",
        "\n",
        "        # Report significant findings\n",
        "        if step_anomalies:\n",
        "            print(f\"Time {time_step:4d}: {len(step_anomalies)} agents detected issues\")\n",
        "            for result in step_anomalies:\n",
        "                status = \"ANOMALY\" if result['is_anomaly'] else \"\"\n",
        "                status += \" DRIFT\" if result['drift_flag'] else \"\"\n",
        "                print(f\"  {result['sensor_id']}: {status} (score: {result['anomaly_score']:.3f}, \"\n",
        "                      f\"conf: {result['confidence']:.2f})\")\n",
        "\n",
        "        anomaly_results.extend(step_anomalies)\n",
        "\n",
        "    # Final status report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"5. Final Agent Status Report:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for sensor_id, agent in agents.items():\n",
        "        status = agent.get_status()\n",
        "        print(f\"{sensor_id}:\")\n",
        "        print(f\"  Processed: {status['total_processed']} | \"\n",
        "              f\"Anomalies: {status['anomalies_detected']} ({status['anomaly_rate']:.1%}) | \"\n",
        "              f\"Drift Events: {status['drift_detected_count']} | \"\n",
        "              f\"Memory: {status['memory_utilization']:.1%} | \"\n",
        "              f\"Retrain: {'Yes' if status['should_retrain'] else 'No'}\")\n",
        "\n",
        "    print(f\"\\nTotal anomalies across all sensors: {len(anomaly_results)}\")\n",
        "    print(\"\\n=== Demo Complete ===\")\n",
        "\n",
        "    return agents, anomaly_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demonstration\n",
        "    agents, results = main()\n",
        "\n",
        "    # Example of accessing individual agent for further analysis\n",
        "    print(\"\\nExample: Detailed status of first agent:\")\n",
        "    first_agent = list(agents.values())[0]\n",
        "    detailed_status = first_agent.get_status()\n",
        "    for key, value in detailed_status.items():\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/PAPER_GDRNet_METROPM_Step4-VAR-LATEST.ipynb",
      "authorship_tag": "ABX9TyNqJkHxwiW/Bn1D+b08wQj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}