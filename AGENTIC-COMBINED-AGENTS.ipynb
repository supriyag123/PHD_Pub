{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-COMBINED-AGENTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68398dbc-921e-4a7f-96c1-ac9d2ae50dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Creating robust system with 12 sensors\n",
            "✅ AE model loaded for sensor 0\n",
            "✅ AE model loaded for sensor 1\n",
            "✅ AE model loaded for sensor 2\n",
            "✅ AE model loaded for sensor 3\n",
            "✅ AE model loaded for sensor 4\n",
            "✅ AE model loaded for sensor 5\n",
            "✅ AE model loaded for sensor 6\n",
            "✅ AE model loaded for sensor 7\n",
            "✅ AE model loaded for sensor 8\n",
            "✅ AE model loaded for sensor 9\n",
            "✅ AE model loaded for sensor 10\n",
            "✅ AE model loaded for sensor 11\n",
            "✅ Created system: 12/12 models loaded\n",
            "✅ Loaded MLP model from /content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\n",
            "AdaptiveWindowAgent adaptive_window_agent initialized\n"
          ]
        }
      ],
      "source": [
        "# ====== Imports ======\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, json, os, logging, warnings\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras, tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ====== RobustSensorAgent & RobustMasterAgent ======\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 50,\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1):\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {'mean': 0.0, 'std': 1.0, 'q95': 0.0, 'q99': 0.0}\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Metadata sidecar file\n",
        "                metadata_path = model_path.replace('.h5', '_metadata.pkl')\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "                   # self.scaler = metadata.get('scaler', StandardScaler())\n",
        "                    self.rolling_stats = metadata.get('rolling_stats', self.rolling_stats)\n",
        "                    if 'error_history' in metadata:\n",
        "                        self.baseline_errors = np.array(metadata['error_history'])\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format – expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"✅ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3. Update rolling stats periodically\n",
        "        if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "            self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model.\"\"\"\n",
        "        try:\n",
        "            #data_scaled = self.scaler.transform(subsequence.reshape(-1, 1))\n",
        "            X = subsequence.reshape(1, self.window_length, 1)  # [batch, timesteps, features]\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "            error = mean_squared_error(subsequence.flatten(), reconstruction.flatten())\n",
        "            return max(0.0, error)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            return np.var(subsequence)\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "        self.rolling_stats['mean'] = np.mean(errors_array)\n",
        "        self.rolling_stats['std'] = np.std(errors_array) + 1e-8\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        return score > threshold\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            return js_divergence > self.drift_threshold\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        if self.rolling_stats['std'] == 0: return 0.5\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        distance_from_threshold = abs(score - threshold) / self.rolling_stats['std']\n",
        "        return min(1.0, distance_from_threshold / 3.0)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SENSOR SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"🚀 Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=50,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1)\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"✅ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "    return sensor_agents, master\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "# =====================================================\n",
        "# AdaptiveWindowAgent (improved version)\n",
        "# =====================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, os, logging, datetime as dt\n",
        "from collections import deque\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Adaptive Window Agent:\n",
        "    - Predicts window size using MLP\n",
        "    - Evaluates forecast with VAR\n",
        "    - Monitors anomalies & drift with adaptive thresholds\n",
        "    - Outputs severity scores + suppresses redundant events\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id=\"adaptive_window_agent\",\n",
        "                 model_path=None, checkpoint_path=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # Core model\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Histories\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # Event detection params\n",
        "        self.drift_detection_window = 20\n",
        "        self.drift_threshold_mse = 1.5   # stricter\n",
        "        self.drift_threshold_mae = 1.5\n",
        "        self.consecutive_poor_predictions = 0\n",
        "        self.cooldown_counter = 0\n",
        "\n",
        "        # Stats\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        # Retraining buffers\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "\n",
        "    # ------------------- Model -------------------\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"✅ Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                # Try to load transformer\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                else:\n",
        "                    # Fit transformer from true window labels\n",
        "                    y_original = np.load(\n",
        "                        \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy\"\n",
        "                    )\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(\"⚠️ No transformer found, fitted a new one.\")\n",
        "            else:\n",
        "                print(f\"❌ Model not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {e}\")\n",
        "\n",
        "    # ------------------- Forecast Eval -------------------\n",
        "    def evaluate_forecast_performance(self, sequence_3d, predicted_window, n_future=1):\n",
        "        try:\n",
        "            df = pd.DataFrame(sequence_3d, columns=[f'V{i+1}' for i in range(sequence_3d.shape[1])])\n",
        "            df_train, df_test = df[:-n_future], df[-n_future:]\n",
        "\n",
        "            # Drop constant cols\n",
        "            constant_cols = [c for c in df_train.columns if df_train[c].nunique() <= 1]\n",
        "            df_train = df_train.drop(columns=constant_cols, errors=\"ignore\")\n",
        "            df_test = df_test.drop(columns=constant_cols, errors=\"ignore\")\n",
        "\n",
        "            # If too few variables, fall back immediately\n",
        "            if len(df_train.columns) < 1:\n",
        "                return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            k = min(predicted_window, len(df_train) - 2)\n",
        "            if k < 1: k = 1\n",
        "\n",
        "            # Try VAR\n",
        "            try:\n",
        "                model = VAR(df_train)\n",
        "                model_fitted = model.fit(maxlags=k, trend=\"c\")\n",
        "                forecast_input = df_train.values[-model_fitted.k_ar:]\n",
        "                fc = model_fitted.forecast(y=forecast_input, steps=n_future)\n",
        "                df_forecast = pd.DataFrame(fc, index=df.index[-n_future:], columns=df_train.columns)\n",
        "\n",
        "                actual = df_test[df_forecast.columns].values.flatten()\n",
        "                predicted = df_forecast.values.flatten()\n",
        "\n",
        "            except Exception:\n",
        "                # Try AutoReg\n",
        "                try:\n",
        "                    from statsmodels.tsa.ar_model import AutoReg\n",
        "                    col = df_train.columns[0]\n",
        "                    model = AutoReg(df_train[col], lags=min(k, len(df_train)//2)).fit()\n",
        "                    predicted = model.predict(start=len(df_train), end=len(df_train)+n_future-1).values\n",
        "                    actual = df_test[col].values\n",
        "                except Exception:\n",
        "                    # Fallback to persistence\n",
        "                    return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            mse = np.mean((actual - predicted) ** 2)\n",
        "            mae = np.mean(np.abs(actual - predicted))\n",
        "\n",
        "            # If forecast is unstable, fallback\n",
        "            if np.isnan(mse) or np.isnan(mae) or mse > 10 or mae > 10:\n",
        "                return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            return {\n",
        "                'mse': float(mse),\n",
        "                'mae': float(mae),\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual.tolist(),\n",
        "                'predicted_values': predicted.tolist(),\n",
        "                'used_columns': list(df_test.columns)\n",
        "            }\n",
        "\n",
        "        except Exception:\n",
        "            return self._persistence_forecast(df, df_test)\n",
        "\n",
        "# ------------------- Persistence fallback -------------------\n",
        "\n",
        "    def _persistence_forecast(self, df, df_test):\n",
        "        \"\"\"Simple last-value-carried-forward forecast.\"\"\"\n",
        "        try:\n",
        "            last_values = df.iloc[-1].values\n",
        "            predicted = np.tile(last_values, (len(df_test), 1))\n",
        "            actual = df_test.values.flatten()\n",
        "\n",
        "            mse = np.mean((actual - predicted.flatten()) ** 2)\n",
        "            mae = np.mean(np.abs(actual - predicted.flatten()))\n",
        "\n",
        "            return {\n",
        "                'mse': float(mse),\n",
        "                'mae': float(mae),\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual.tolist(),\n",
        "                'predicted_values': predicted.flatten().tolist(),\n",
        "                'used_columns': list(df_test.columns),\n",
        "                'note': 'persistence_fallback'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'mse': 9999, 'mae': 9999, 'forecast_success': True, 'error': str(e), 'note': 'persistence_fallback_failed'}\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------- Prediction -------------------\n",
        "\n",
        "    def predict_window_size(self, feature_vector, sequence_3d):\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'error': \"Model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            pred_raw = self.model.predict(feature_vector, verbose=0)\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = int(round(self.transformer.inverse_transform(pred_raw)[0, 0]))\n",
        "            else:\n",
        "                predicted_window = int(round(pred_raw[0, 0]))\n",
        "\n",
        "            # Evaluate\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            if forecast_metrics.get(\"forecast_success\", False):\n",
        "                self.mse_history.append(forecast_metrics[\"mse\"])\n",
        "                self.mae_history.append(forecast_metrics[\"mae\"])\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = np.mean(self.mse_history)\n",
        "                self.performance_stats['avg_mae'] = np.mean(self.mae_history)\n",
        "\n",
        "            # Event check\n",
        "            event, sev = self._check_for_event()\n",
        "\n",
        "            # Save history\n",
        "            record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'event_type': event,\n",
        "                'severity': sev\n",
        "            }\n",
        "            self.prediction_history.append(record)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'event_type': event,\n",
        "                'severity': sev,\n",
        "                'performance_stats': self.get_recent_performance()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'error': str(e)}\n",
        "\n",
        "    # ------------------- Event Logic -------------------\n",
        "\n",
        "    def _check_for_event(self):\n",
        "        if len(self.mse_history) < self.drift_detection_window:\n",
        "            return None, 0.0\n",
        "\n",
        "        mse_vals = np.array(self.mse_history)[-self.drift_detection_window:]\n",
        "        mae_vals = np.array(self.mae_history)[-self.drift_detection_window:]\n",
        "\n",
        "        # Rolling stats\n",
        "        mean_mse, std_mse = np.mean(mse_vals), np.std(mse_vals) + 1e-8\n",
        "        last_mse = mse_vals[-1]\n",
        "\n",
        "        # Normalized error\n",
        "        norm_error = (last_mse - mean_mse) / std_mse\n",
        "\n",
        "        # Severity\n",
        "        anomaly_severity = max(0, norm_error)\n",
        "        drift_severity = max(0, (np.mean(mse_vals) / (np.median(mse_vals)+1e-5)) - 1)\n",
        "\n",
        "        # Check anomaly\n",
        "        if last_mse > mean_mse + 2.0 * std_mse:\n",
        "            self.performance_stats['anomaly_events'] += 1\n",
        "            return \"ANOMALY\", anomaly_severity\n",
        "\n",
        "        # Check drift (with persistence + cooldown)\n",
        "        ema_mse = 0.3*np.mean(mse_vals) + 0.7*np.median(mse_vals)\n",
        "        ema_mae = 0.3*np.mean(mae_vals) + 0.7*np.median(mae_vals)\n",
        "        mse_ratio = ema_mse / max(np.median(mse_vals), 1e-5)\n",
        "        mae_ratio = ema_mae / max(np.median(mae_vals), 1e-5)\n",
        "\n",
        "        if mse_ratio > self.drift_threshold_mse and mae_ratio > self.drift_threshold_mae:\n",
        "            self.consecutive_poor_predictions += 1\n",
        "            if self.consecutive_poor_predictions >= 5 and self.cooldown_counter == 0:\n",
        "                self.performance_stats['drift_events'] += 1\n",
        "                self.cooldown_counter = 10\n",
        "                return \"DRIFT\", drift_severity\n",
        "        else:\n",
        "            self.consecutive_poor_predictions = 0\n",
        "\n",
        "        if self.cooldown_counter > 0:\n",
        "            self.cooldown_counter -= 1\n",
        "\n",
        "        return None, 0.0\n",
        "\n",
        "    # ------------------- Helpers -------------------\n",
        "\n",
        "    def get_recent_performance(self):\n",
        "        successful_predictions = [\n",
        "            p for p in list(self.prediction_history)[-50:]\n",
        "            if p.get('forecast_metrics', {}).get('forecast_success', False)\n",
        "        ]\n",
        "        return {\n",
        "            'total_predictions': len(self.prediction_history),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(self.prediction_history), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': float(np.mean(list(self.mse_history)[-10:])) if self.mse_history else 0,\n",
        "            'avg_mse': float(np.mean(self.mse_history)) if self.mse_history else 0,\n",
        "            'recent_mae': float(np.mean(list(self.mae_history)[-10:])) if self.mae_history else 0,\n",
        "            'avg_mae': float(np.mean(self.mae_history)) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save performance statistics + prediction history to JSON\"\"\"\n",
        "        try:\n",
        "            state = {\n",
        "                'performance_stats': self.performance_stats.copy(),\n",
        "                'prediction_history': list(self.prediction_history)[-100:],  # last 100\n",
        "                'mse_history': list(self.mse_history),\n",
        "                'mae_history': list(self.mae_history),\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "            import json\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, indent=2, default=str)\n",
        "            print(f\"✅ Performance state saved to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save performance state: {e}\")\n",
        "\n",
        "# ==================== PLOTTING ====================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_all_features_forecasts(test_sequences, agent, max_features=12):\n",
        "    all_actual, all_forecast, event_points = [], [], []\n",
        "    for i, seq in enumerate(test_sequences):\n",
        "        result = agent.predict_window_size(seq.flatten(), seq)\n",
        "        fm = result.get(\"forecast_metrics\", {})\n",
        "        if not fm.get(\"forecast_success\", False): continue\n",
        "        actual_row = [np.nan] * max_features\n",
        "        forecast_row = [np.nan] * max_features\n",
        "        for j, col in enumerate(fm.get(\"used_columns\", [])):\n",
        "            idx = int(col.replace(\"V\", \"\")) - 1\n",
        "            if idx < max_features:\n",
        "                actual_row[idx] = fm[\"actual_values\"][j]\n",
        "                forecast_row[idx] = fm[\"predicted_values\"][j]\n",
        "        all_actual.append(actual_row)\n",
        "        all_forecast.append(forecast_row)\n",
        "        if result.get(\"event_type\", None):\n",
        "            event_points.append((i, result[\"event_type\"]))\n",
        "    if not all_actual: return\n",
        "    actual_matrix = np.array(all_actual)\n",
        "    forecast_matrix = np.array(all_forecast)\n",
        "    timestamps = np.arange(len(actual_matrix))\n",
        "    fig, axes = plt.subplots(max_features, 1, figsize=(12, 2.5 * max_features), sharex=True)\n",
        "    for idx, ax in enumerate(axes):\n",
        "        ax.plot(timestamps, actual_matrix[:, idx], label=\"Actual\", marker=\"o\", alpha=0.6)\n",
        "        ax.plot(timestamps, forecast_matrix[:, idx], label=\"Forecast\", marker=\"x\", alpha=0.6)\n",
        "        for (t, etype) in event_points:\n",
        "            ax.scatter(t, actual_matrix[t, idx], color=\"red\" if etype==\"ANOMALY\" else \"orange\",\n",
        "                       marker=\"D\", label=etype if idx==0 else \"\")\n",
        "        ax.set_ylabel(f\"V{idx+1}\"); ax.grid(True, alpha=0.3)\n",
        "        if idx == 0: ax.legend()\n",
        "    axes[-1].set_xlabel(\"Sequence index\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ====== CoordinatorAgent ======\n",
        "\n",
        "class CoordinatorAgent:\n",
        "    \"\"\"\n",
        "    Coordinator Agent - orchestrates outputs from:\n",
        "      1. RobustMasterAgent (sensor-level aggregation)\n",
        "      2. AdaptiveWindowAgent (global window-level perspective)\n",
        "\n",
        "    Combines them into a final system decision:\n",
        "      - True anomalies (local + global agreement)\n",
        "      - Concept drift (global window instability)\n",
        "      - Retraining triggers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, anomaly_weight: float = 0.6, window_weight: float = 0.4):\n",
        "        self.anomaly_weight = anomaly_weight\n",
        "        self.window_weight = window_weight\n",
        "        self.history = []\n",
        "\n",
        "    def fuse(self, master_output: dict, window_output: dict) -> dict:\n",
        "        timestamp = datetime.now()\n",
        "\n",
        "        # Master outputs\n",
        "        sys_dec = master_output.get(\"system_decisions\", {})\n",
        "        sys_anomaly = sys_dec.get(\"system_anomaly\", False)\n",
        "        sys_drift = sys_dec.get(\"system_drift\", False)\n",
        "        sys_retrain = sys_dec.get(\"system_needs_retrain\", False)\n",
        "        anomaly_rate = sys_dec.get(\"anomaly_rate\", 0.0)\n",
        "        drift_rate = sys_dec.get(\"drift_rate\", 0.0)\n",
        "\n",
        "        # Window outputs\n",
        "        win_anomaly = window_output.get(\"window_anomaly_flag\", False) \\\n",
        "                      or (window_output.get(\"event_type\") == \"ANOMALY\")\n",
        "        win_drift = window_output.get(\"window_drift_flag\", False) \\\n",
        "                    or (window_output.get(\"event_type\") == \"DRIFT\")\n",
        "        predicted_window = window_output.get(\"predicted_window\", None)\n",
        "\n",
        "        # Fusion logic\n",
        "        anomaly_score = (\n",
        "            self.anomaly_weight * anomaly_rate +\n",
        "            self.window_weight * (1.0 if win_anomaly else 0.0)\n",
        "        )\n",
        "        drift_score = (\n",
        "            self.anomaly_weight * drift_rate +\n",
        "            self.window_weight * (1.0 if win_drift else 0.0)\n",
        "        )\n",
        "\n",
        "        final_anomaly = anomaly_score >= 0.3 or (sys_anomaly and win_anomaly)\n",
        "        final_drift = drift_score >= 0.2 or (sys_drift or win_drift)\n",
        "        final_retrain = sys_retrain or (win_drift and anomaly_score > 0.2)\n",
        "\n",
        "        if final_anomaly and final_drift:\n",
        "            alert = \"CRITICAL\"\n",
        "        elif final_anomaly:\n",
        "            alert = \"HIGH\"\n",
        "        elif final_drift:\n",
        "            alert = \"MEDIUM\"\n",
        "        else:\n",
        "            alert = \"NORMAL\"\n",
        "\n",
        "        decision = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"final_anomaly\": final_anomaly,\n",
        "            \"final_drift\": final_drift,\n",
        "            \"final_retrain\": final_retrain,\n",
        "            \"alert_level\": alert,\n",
        "            \"scores\": {\n",
        "                \"anomaly_score\": anomaly_score,\n",
        "                \"drift_score\": drift_score,\n",
        "                \"sensor_anomaly_rate\": anomaly_rate,\n",
        "                \"sensor_drift_rate\": drift_rate\n",
        "            },\n",
        "            \"window_agent\": {\n",
        "                \"predicted_window\": predicted_window,\n",
        "                \"window_anomaly\": win_anomaly,\n",
        "                \"window_drift\": win_drift\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.history.append(decision)\n",
        "        return decision\n",
        "\n",
        "##################################################\n",
        "def extract_flags(entry):\n",
        "    # Coordinator flags\n",
        "    if \"coordinator\" in entry:\n",
        "        coord = entry[\"coordinator\"]\n",
        "        return int(coord.get(\"final_anomaly\", False)), int(coord.get(\"final_drift\", False))\n",
        "\n",
        "    # Master flags\n",
        "    if \"master\" in entry and \"system_decisions\" in entry[\"master\"]:\n",
        "        sys = entry[\"master\"][\"system_decisions\"]\n",
        "        return int(sys.get(\"system_anomaly\", False)), int(sys.get(\"system_drift\", False))\n",
        "\n",
        "    return 0, 0\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "#GROUND TRUTH ANOMALY VS WHICH AGENT AGENT IS RIGHT\n",
        "############################################################\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_agent_vs_groundtruth(\n",
        "    results,\n",
        "    detection_labels=None,\n",
        "    h1_labels=None,\n",
        "    h3_labels=None,\n",
        "    h12_labels=None,\n",
        "    max_samples=200\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot comparison between agent decisions, ground-truth labels,\n",
        "    coordinator outputs, and AdaptiveWindowAgent predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    n = min(max_samples, len(results))\n",
        "    t = np.arange(n)\n",
        "\n",
        "    # Extract coordinator anomaly/drift\n",
        "    agent_anomaly = [1 if r[\"coordinator\"][\"final_anomaly\"] else 0 for r in results[:n]]\n",
        "    agent_drift   = [1 if r[\"coordinator\"][\"final_drift\"] else 0 for r in results[:n]]\n",
        "\n",
        "    # Ground-truth labels\n",
        "    det = detection_labels[:n] if detection_labels is not None else None\n",
        "    h1  = h1_labels[:n] if h1_labels is not None else None\n",
        "    h3  = h3_labels[:n] if h3_labels is not None else None\n",
        "    h12 = h12_labels[:n] if h12_labels is not None else None\n",
        "\n",
        "    # Window agent outputs\n",
        "    window_sizes  = [r[\"window\"][\"predicted_window\"] for r in results[:n]]\n",
        "    window_events = [r[\"window\"].get(\"event_type\") for r in results[:n]]\n",
        "\n",
        "    # ---- Create subplots ----\n",
        "    fig, axes = plt.subplots(5, 1, figsize=(15, 14), sharex=True)\n",
        "\n",
        "    # Agent outputs\n",
        "    axes[0].plot(t, agent_anomaly, label=\"Agent Anomaly\", color=\"red\")\n",
        "    axes[0].plot(t, agent_drift,   label=\"Agent Drift\", color=\"orange\")\n",
        "    axes[0].set_ylabel(\"Agent\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Detection labels\n",
        "    if det is not None:\n",
        "        axes[1].plot(t, det, label=\"Detection Labels\", color=\"blue\")\n",
        "        axes[1].set_ylabel(\"Detect\")\n",
        "        axes[1].legend()\n",
        "\n",
        "    # Prediction labels (1h, 3h, 12h)\n",
        "    if h1 is not None:\n",
        "        axes[2].plot(t, h1, label=\"H1\", color=\"green\")\n",
        "    if h3 is not None:\n",
        "        axes[2].plot(t, h3, label=\"H3\", color=\"purple\")\n",
        "    if h12 is not None:\n",
        "        axes[2].plot(t, h12, label=\"H12\", color=\"brown\")\n",
        "    axes[2].set_ylabel(\"Prediction\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    # Coordinator alert level\n",
        "    alert_map = {\"NORMAL\": 0, \"MEDIUM\": 1, \"HIGH\": 2, \"CRITICAL\": 3}\n",
        "    alerts = [alert_map.get(r[\"coordinator\"][\"alert_level\"], 0) for r in results[:n]]\n",
        "    axes[3].plot(t, alerts, label=\"Alert Level\", color=\"black\")\n",
        "    axes[3].set_yticks([0,1,2,3])\n",
        "    axes[3].set_yticklabels([\"NORMAL\",\"MED\",\"HIGH\",\"CRIT\"])\n",
        "    axes[3].set_ylabel(\"Coordinator\")\n",
        "    axes[3].legend()\n",
        "\n",
        "    # AdaptiveWindowAgent subplot\n",
        "    axes[4].plot(t, window_sizes, label=\"Predicted Window\", color=\"blue\")\n",
        "\n",
        "    # Mark drift/anomaly events\n",
        "    for i, evt in enumerate(window_events):\n",
        "        if evt == \"DRIFT\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"orange\", marker=\"x\", label=\"Window Drift\" if i==0 else \"\")\n",
        "        elif evt == \"ANOMALY\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"red\", marker=\"o\", label=\"Window Anomaly\" if i==0 else \"\")\n",
        "\n",
        "    # Overlay ground-truth fault events (vertical lines)\n",
        "    if det is not None:\n",
        "        for i, val in enumerate(det):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Fault (Detection)\" if i==0 else \"\")\n",
        "    if h1 is not None:\n",
        "        for i, val in enumerate(h1):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"green\", linestyle=\"--\", alpha=0.2, label=\"Fault (H1)\" if i==0 else \"\")\n",
        "    if h3 is not None:\n",
        "        for i, val in enumerate(h3):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"purple\", linestyle=\"--\", alpha=0.2, label=\"Fault (H3)\" if i==0 else \"\")\n",
        "    if h12 is not None:\n",
        "        for i, val in enumerate(h12):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"brown\", linestyle=\"--\", alpha=0.2, label=\"Fault (H12)\" if i==0 else \"\")\n",
        "\n",
        "    axes[4].set_ylabel(\"Window Size\")\n",
        "    axes[4].legend()\n",
        "\n",
        "    axes[-1].set_xlabel(\"Sample index\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =========================\n",
        "# DEMO LOOP\n",
        "# =========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "    mlp_path = \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "    data_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\"\n",
        "\n",
        "\n",
        "\n",
        "    # create agents\n",
        "    sensor_agents, master = create_robust_system(num_sensors=12, models_dir=models_dir)\n",
        "    window_agent = AdaptiveWindowAgent(model_path=mlp_path)\n",
        "    coordinator = CoordinatorAgent()\n",
        "\n",
        "    # load test data\n",
        "    subsequences = np.load(data_path)\n",
        "    holdout_value = 100\n",
        "    holdout = subsequences[1100:1200]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, seq in enumerate(holdout, 1):\n",
        "        features = seq.flatten()\n",
        "        master_out = master.process_system_input(seq)\n",
        "        window_out = window_agent.predict_window_size(features, seq)\n",
        "        final = coordinator.fuse(master_out, window_out)\n",
        "        results.append({\n",
        "        \"master\": master_out,\n",
        "        \"window\": window_out,\n",
        "        \"coordinator\": final\n",
        "        })\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Sample {i}\")\n",
        "        print(\"Master:\", master_out[\"system_decisions\"])\n",
        "        print(\"Window:\", {\"predicted_window\": window_out.get(\"predicted_window\"),\n",
        "                          \"event\": window_out.get(\"event_type\")})\n",
        "        print(\"Coordinator:\", final[\"alert_level\"], final[\"scores\"])\n",
        "\n",
        "    anomaly_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_detection.npy\")\n",
        "    h1_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H1.npy\")\n",
        "    h3_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H3.npy\")\n",
        "    h12_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H12.npy\")\n",
        "\n",
        "    detection_labels_test = anomaly_labels[1100:1200]\n",
        "    h1_labels_test = h1_labels[1100:1200]\n",
        "    h3_labels_test = h3_labels[1100:1200]\n",
        "    h12_labels_test = h12_labels[1100:1200]\n",
        "\n",
        "    plot_agent_vs_groundtruth(\n",
        "        results,\n",
        "        detection_labels=detection_labels_test,\n",
        "        h1_labels=h1_labels_test,\n",
        "        h3_labels=h3_labels_test,\n",
        "        h12_labels=h12_labels_test,\n",
        "        max_samples=200   # optional\n",
        "    )\n",
        "\n",
        "\n",
        "    ############Optional - find where anomalies lie**************\n",
        "    print(\"Detection anomaly indices:\", np.where(anomaly_labels == 1)[0])\n",
        "    print(\"H+1 anomaly indices:\", np.where(h1_labels == 1)[0])\n",
        "    print(\"H+3 anomaly indices:\", np.where(h3_labels == 1)[0])\n",
        "    print(\"H+12 anomaly indices:\", np.where(h12_labels == 1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-COMBINED-AGENTS.ipynb",
      "authorship_tag": "ABX9TyP+RWLS5ANXkn7o+1oHIfuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}