{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4nAdWPggqfaMDame8AFUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/MLP%20Troubleshooting%20Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "9-Uv2sVHVxHo",
        "outputId": "45dfe599-fbba-4046-db44-cbec9f9b717c"
      },
      "source": [
        "# Troubleshooting MLP Agent - Enhanced Diagnostics and Fixes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l1_l2\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TroubleshootingMLPAgent:\n",
        "    \"\"\"\n",
        "    MLP Agent with comprehensive troubleshooting and diagnostics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "\n",
        "        # Diagnostic variables\n",
        "        self.data_issues = []\n",
        "        self.training_issues = []\n",
        "\n",
        "        # Create directories\n",
        "        self.mlp_dir = f\"{output_dir}MLP_Models/\"\n",
        "        os.makedirs(self.mlp_dir, exist_ok=True)\n",
        "\n",
        "    def comprehensive_data_diagnosis(self, x, y):\n",
        "        \"\"\"\n",
        "        Comprehensive data quality diagnosis\n",
        "        \"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"üîç COMPREHENSIVE DATA DIAGNOSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        issues = []\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"üìä Basic Data Info:\")\n",
        "        print(f\"   Features shape: {x.shape}\")\n",
        "        print(f\"   Target shape: {y.shape}\")\n",
        "        print(f\"   Memory usage: {(x.nbytes + y.nbytes) / 1024**2:.1f} MB\")\n",
        "\n",
        "        # 1. Check for NaN/Inf values\n",
        "        print(f\"\\nüîç Missing/Invalid Values:\")\n",
        "        x_nan = np.isnan(x).sum()\n",
        "        y_nan = np.isnan(y).sum()\n",
        "        x_inf = np.isinf(x).sum()\n",
        "        y_inf = np.isinf(y).sum()\n",
        "\n",
        "        print(f\"   X NaN values: {x_nan}\")\n",
        "        print(f\"   y NaN values: {y_nan}\")\n",
        "        print(f\"   X Inf values: {x_inf}\")\n",
        "        print(f\"   y Inf values: {y_inf}\")\n",
        "\n",
        "        if x_nan > 0 or y_nan > 0 or x_inf > 0 or y_inf > 0:\n",
        "            issues.append(\"Missing or infinite values detected\")\n",
        "\n",
        "        # 2. Feature analysis\n",
        "        print(f\"\\nüìà Feature Analysis:\")\n",
        "        feature_means = np.mean(x, axis=0)\n",
        "        feature_stds = np.std(x, axis=0)\n",
        "        feature_ranges = np.max(x, axis=0) - np.min(x, axis=0)\n",
        "\n",
        "        print(f\"   Feature means range: [{np.min(feature_means):.4f}, {np.max(feature_means):.4f}]\")\n",
        "        print(f\"   Feature stds range: [{np.min(feature_stds):.4f}, {np.max(feature_stds):.4f}]\")\n",
        "\n",
        "        # Check for constant features\n",
        "        constant_features = np.sum(feature_stds < 1e-8)\n",
        "        print(f\"   Constant features: {constant_features}/{x.shape[1]}\")\n",
        "        if constant_features > 0:\n",
        "            issues.append(f\"{constant_features} constant features found\")\n",
        "\n",
        "        # Check if features are already scaled\n",
        "        feature_scale_check = np.abs(feature_means) < 0.1 and np.abs(feature_stds - 1.0) < 0.1\n",
        "        print(f\"   Features appear scaled: {np.all(feature_scale_check)}\")\n",
        "\n",
        "        # 3. Target analysis\n",
        "        print(f\"\\nüéØ Target Analysis:\")\n",
        "        print(f\"   Target mean: {np.mean(y):.4f}\")\n",
        "        print(f\"   Target std: {np.std(y):.4f}\")\n",
        "        print(f\"   Target range: [{np.min(y):.1f}, {np.max(y):.1f}]\")\n",
        "        print(f\"   Unique target values: {len(np.unique(y))}\")\n",
        "\n",
        "        # Check target distribution\n",
        "        skewness = stats.skew(y)\n",
        "        kurtosis = stats.kurtosis(y)\n",
        "        print(f\"   Target skewness: {skewness:.4f}\")\n",
        "        print(f\"   Target kurtosis: {kurtosis:.4f}\")\n",
        "\n",
        "        if abs(skewness) > 2:\n",
        "            issues.append(f\"Highly skewed target distribution (skew={skewness:.2f})\")\n",
        "\n",
        "        # 4. Feature-target correlations\n",
        "        print(f\"\\nüîó Feature-Target Relationships:\")\n",
        "        correlations = []\n",
        "        for i in range(min(x.shape[1], 10)):  # Check first 10 features\n",
        "            try:\n",
        "                corr = np.corrcoef(x[:, i], y)[0, 1]\n",
        "                if not np.isnan(corr):\n",
        "                    correlations.append(abs(corr))\n",
        "            except:\n",
        "                correlations.append(0)\n",
        "\n",
        "        if correlations:\n",
        "            max_corr = max(correlations)\n",
        "            mean_corr = np.mean(correlations)\n",
        "            print(f\"   Max |correlation|: {max_corr:.4f}\")\n",
        "            print(f\"   Mean |correlation|: {mean_corr:.4f}\")\n",
        "\n",
        "            if max_corr < 0.05:\n",
        "                issues.append(\"Extremely weak feature-target correlations\")\n",
        "            elif max_corr < 0.1:\n",
        "                issues.append(\"Very weak feature-target correlations\")\n",
        "\n",
        "        # 5. Sample size analysis\n",
        "        print(f\"\\nüìè Sample Size Analysis:\")\n",
        "        n_samples, n_features = x.shape\n",
        "        samples_per_feature = n_samples / n_features\n",
        "        print(f\"   Samples per feature: {samples_per_feature:.1f}\")\n",
        "\n",
        "        if samples_per_feature < 10:\n",
        "            issues.append(\"Very few samples per feature (overfitting risk)\")\n",
        "        elif samples_per_feature < 50:\n",
        "            issues.append(\"Limited samples per feature\")\n",
        "\n",
        "        # 6. Data distribution visualization\n",
        "        self.plot_data_distributions(x, y)\n",
        "\n",
        "        # 7. Summary\n",
        "        print(f\"\\n‚ö†Ô∏è Issues Found: {len(issues)}\")\n",
        "        for i, issue in enumerate(issues, 1):\n",
        "            print(f\"   {i}. {issue}\")\n",
        "\n",
        "        self.data_issues = issues\n",
        "\n",
        "        # 8. Recommendations\n",
        "        print(f\"\\nüí° Recommendations:\")\n",
        "        if \"Missing or infinite values detected\" in issues:\n",
        "            print(\"   - Clean missing/infinite values before training\")\n",
        "        if any(\"correlation\" in issue for issue in issues):\n",
        "            print(\"   - Consider feature engineering or polynomial features\")\n",
        "            print(\"   - Try different model architectures (deeper/wider)\")\n",
        "            print(\"   - Consider ensemble methods\")\n",
        "        if \"skewed\" in str(issues):\n",
        "            print(\"   - Consider log transformation of target\")\n",
        "        if \"constant\" in str(issues):\n",
        "            print(\"   - Remove constant features\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        return issues\n",
        "\n",
        "    def plot_data_distributions(self, x, y):\n",
        "        \"\"\"Plot data distributions for visual inspection\"\"\"\n",
        "        plt.figure(figsize=(20, 12))\n",
        "\n",
        "        # Target distribution\n",
        "        plt.subplot(3, 4, 1)\n",
        "        plt.hist(y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        plt.title('Target Distribution')\n",
        "        plt.xlabel('Target Value')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Target boxplot\n",
        "        plt.subplot(3, 4, 2)\n",
        "        plt.boxplot(y)\n",
        "        plt.title('Target Boxplot')\n",
        "        plt.ylabel('Target Value')\n",
        "\n",
        "        # Feature distributions (first 6 features)\n",
        "        for i in range(min(6, x.shape[1])):\n",
        "            plt.subplot(3, 4, i + 3)\n",
        "            plt.hist(x[:, i], bins=30, alpha=0.7, color='lightcoral')\n",
        "            plt.title(f'Feature {i+1}')\n",
        "            plt.xlabel('Value')\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "        # Feature-target scatter plots (first 4 features)\n",
        "        for i in range(min(4, x.shape[1])):\n",
        "            plt.subplot(3, 4, i + 9)\n",
        "            # Sample for performance\n",
        "            sample_size = min(5000, len(x))\n",
        "            indices = np.random.choice(len(x), sample_size, replace=False)\n",
        "            plt.scatter(x[indices, i], y[indices], alpha=0.5, s=1)\n",
        "            corr = np.corrcoef(x[:, i], y)[0, 1]\n",
        "            plt.title(f'Feature {i+1} vs Target\\nr={corr:.3f}')\n",
        "            plt.xlabel(f'Feature {i+1}')\n",
        "            plt.ylabel('Target')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def intelligent_preprocessing(self, x, y):\n",
        "        \"\"\"\n",
        "        Intelligent preprocessing based on data characteristics\n",
        "        \"\"\"\n",
        "        print(\"\\nüß† INTELLIGENT PREPROCESSING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        x_processed = x.copy()\n",
        "        y_processed = y.copy()\n",
        "        preprocessing_steps = []\n",
        "\n",
        "        # 1. Handle missing values\n",
        "        if np.isnan(x_processed).any() or np.isnan(y_processed).any():\n",
        "            print(\"üîß Handling missing values...\")\n",
        "            x_processed = np.nan_to_num(x_processed, nan=np.nanmean(x_processed))\n",
        "            y_processed = np.nan_to_num(y_processed, nan=np.nanmean(y_processed))\n",
        "            preprocessing_steps.append(\"Missing values filled\")\n",
        "\n",
        "        # 2. Remove constant features\n",
        "        feature_stds = np.std(x_processed, axis=0)\n",
        "        non_constant_mask = feature_stds > 1e-8\n",
        "        if not np.all(non_constant_mask):\n",
        "            print(f\"üîß Removing {np.sum(~non_constant_mask)} constant features...\")\n",
        "            x_processed = x_processed[:, non_constant_mask]\n",
        "            preprocessing_steps.append(f\"Removed {np.sum(~non_constant_mask)} constant features\")\n",
        "\n",
        "        # 3. Handle extreme outliers in target\n",
        "        y_q1, y_q3 = np.percentile(y_processed, [25, 75])\n",
        "        y_iqr = y_q3 - y_q1\n",
        "        y_lower = y_q1 - 3 * y_iqr\n",
        "        y_upper = y_q3 + 3 * y_iqr\n",
        "\n",
        "        extreme_outliers = (y_processed < y_lower) | (y_processed > y_upper)\n",
        "        if np.sum(extreme_outliers) > 0:\n",
        "            print(f\"üîß Clipping {np.sum(extreme_outliers)} extreme target outliers...\")\n",
        "            y_processed = np.clip(y_processed, y_lower, y_upper)\n",
        "            preprocessing_steps.append(f\"Clipped {np.sum(extreme_outliers)} extreme outliers\")\n",
        "\n",
        "        # 4. Target transformation if highly skewed\n",
        "        skewness = stats.skew(y_processed)\n",
        "        if abs(skewness) > 2:\n",
        "            print(f\"üîß Target is highly skewed ({skewness:.2f}), applying log transformation...\")\n",
        "            if np.min(y_processed) > 0:\n",
        "                y_processed = np.log1p(y_processed)\n",
        "                preprocessing_steps.append(\"Log transformation applied to target\")\n",
        "            else:\n",
        "                # Shift to positive values first\n",
        "                y_processed = y_processed - np.min(y_processed) + 1\n",
        "                y_processed = np.log1p(y_processed)\n",
        "                preprocessing_steps.append(\"Shifted and log-transformed target\")\n",
        "\n",
        "        print(f\"‚úÖ Preprocessing complete: {len(preprocessing_steps)} steps\")\n",
        "        for step in preprocessing_steps:\n",
        "            print(f\"   - {step}\")\n",
        "\n",
        "        return x_processed, y_processed, preprocessing_steps\n",
        "\n",
        "    def build_adaptive_mlp(self, input_dim, data_characteristics):\n",
        "        \"\"\"\n",
        "        Build MLP architecture adapted to data characteristics\n",
        "        \"\"\"\n",
        "        print(f\"\\nüèóÔ∏è Building adaptive MLP architecture...\")\n",
        "\n",
        "        # Determine architecture based on data\n",
        "        n_samples = data_characteristics.get('n_samples', 10000)\n",
        "        n_features = input_dim\n",
        "        max_corr = data_characteristics.get('max_correlation', 0.1)\n",
        "\n",
        "        # Base architecture decisions\n",
        "        if max_corr < 0.05:\n",
        "            # Very weak relationships - need more capacity\n",
        "            architecture = 'deep_wide'\n",
        "            dropout_rate = 0.3\n",
        "            l1_reg = 0.001\n",
        "            l2_reg = 0.01\n",
        "        elif max_corr < 0.2:\n",
        "            # Weak relationships - moderate capacity\n",
        "            architecture = 'deep'\n",
        "            dropout_rate = 0.2\n",
        "            l1_reg = 0.0005\n",
        "            l2_reg = 0.005\n",
        "        else:\n",
        "            # Strong relationships - standard architecture\n",
        "            architecture = 'standard'\n",
        "            dropout_rate = 0.1\n",
        "            l1_reg = 0.0001\n",
        "            l2_reg = 0.001\n",
        "\n",
        "        print(f\"   Selected architecture: {architecture}\")\n",
        "        print(f\"   Dropout rate: {dropout_rate}\")\n",
        "        print(f\"   L1/L2 regularization: {l1_reg}/{l2_reg}\")\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        if architecture == 'deep_wide':\n",
        "            # For very weak relationships\n",
        "            model.add(Dense(512, input_dim=input_dim, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(256, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(128, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(64, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate/2))\n",
        "\n",
        "            model.add(Dense(32, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate/2))\n",
        "\n",
        "        elif architecture == 'deep':\n",
        "            # For weak relationships\n",
        "            model.add(Dense(256, input_dim=input_dim, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(128, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(64, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(32, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate/2))\n",
        "\n",
        "        else:\n",
        "            # Standard architecture\n",
        "            model.add(Dense(128, input_dim=input_dim, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(64, activation='relu',\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "            model.add(Dense(32, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate/2))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "\n",
        "        # Adaptive optimizer settings\n",
        "        if max_corr < 0.1:\n",
        "            # Weak relationships - higher learning rate, more aggressive\n",
        "            learning_rate = 0.002\n",
        "            optimizer = keras.optimizers.Adam(\n",
        "                learning_rate=learning_rate,\n",
        "                clipnorm=1.0,\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999\n",
        "            )\n",
        "            loss = 'huber'  # More robust for weak relationships\n",
        "        else:\n",
        "            # Stronger relationships - standard settings\n",
        "            learning_rate = 0.001\n",
        "            optimizer = keras.optimizers.Adam(\n",
        "                learning_rate=learning_rate,\n",
        "                clipnorm=1.0\n",
        "            )\n",
        "            loss = 'mse'\n",
        "\n",
        "        model.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mae', 'mse']\n",
        "        )\n",
        "\n",
        "        print(f\"   Total parameters: {model.count_params():,}\")\n",
        "        print(f\"   Learning rate: {learning_rate}\")\n",
        "        print(f\"   Loss function: {loss}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_with_advanced_callbacks(self, model, x_train, y_train, x_val, y_val,\n",
        "                                    epochs=1000, batch_size=32):\n",
        "        \"\"\"\n",
        "        Training with advanced callbacks and monitoring\n",
        "        \"\"\"\n",
        "        print(f\"\\nüöÄ Training with advanced monitoring...\")\n",
        "\n",
        "        # Advanced callbacks\n",
        "        callbacks = [\n",
        "            # Model checkpointing\n",
        "            ModelCheckpoint(\n",
        "                f\"{self.mlp_dir}best_weights.h5\",\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Early stopping with patience\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=100,  # More patience for difficult problems\n",
        "                verbose=1,\n",
        "                min_delta=0.0001,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "\n",
        "            # Learning rate reduction\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=30,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def comprehensive_evaluation(self, model, x_test, y_test, title=\"Model Evaluation\"):\n",
        "        \"\"\"\n",
        "        Comprehensive model evaluation with multiple metrics\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìä {title.upper()}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_scaled = model.predict(x_test, verbose=0)\n",
        "        y_pred = self.transformer.inverse_transform(y_pred_scaled).flatten()\n",
        "        y_true = self.transformer.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Calculate comprehensive metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Additional metrics\n",
        "        mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "\n",
        "        # Accuracy metrics for different tolerances\n",
        "        tolerances = [0.5, 1.0, 1.5, 2.0]\n",
        "        accuracies = {}\n",
        "        for tol in tolerances:\n",
        "            acc = np.mean(np.abs(y_true - y_pred) <= tol) * 100\n",
        "            accuracies[tol] = acc\n",
        "\n",
        "        # Print results\n",
        "        print(f\"R¬≤ Score: {r2:.6f}\")\n",
        "        print(f\"MSE: {mse:.4f}\")\n",
        "        print(f\"RMSE: {rmse:.4f}\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"MAPE: {mape:.2f}%\")\n",
        "        print(\"Accuracy within tolerance:\")\n",
        "        for tol, acc in accuracies.items():\n",
        "            print(f\"  ¬±{tol}: {acc:.1f}%\")\n",
        "\n",
        "        # Visual evaluation\n",
        "        self.plot_comprehensive_results(y_true, y_pred, title)\n",
        "\n",
        "        return {\n",
        "            'r2': r2, 'mse': mse, 'rmse': rmse, 'mae': mae, 'mape': mape,\n",
        "            'accuracies': accuracies, 'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "    def plot_comprehensive_results(self, y_true, y_pred, title):\n",
        "        \"\"\"Comprehensive results visualization\"\"\"\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Scatter plot\n",
        "        plt.subplot(3, 4, 1)\n",
        "        plt.scatter(y_true, y_pred, alpha=0.5, s=1)\n",
        "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "        plt.xlabel('True Values')\n",
        "        plt.ylabel('Predictions')\n",
        "        plt.title(f'{title} - Scatter Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Residuals vs Predictions\n",
        "        plt.subplot(3, 4, 2)\n",
        "        residuals = y_true - y_pred\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5, s=1)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predictions')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Residuals vs Predictions')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Residual distribution\n",
        "        plt.subplot(3, 4, 3)\n",
        "        plt.hist(residuals, bins=50, alpha=0.7, density=True)\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Residual Distribution')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Q-Q plot for residuals\n",
        "        plt.subplot(3, 4, 4)\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "        plt.title('Q-Q Plot (Residuals)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. Time series comparison (sample)\n",
        "        plt.subplot(3, 4, 5)\n",
        "        sample_indices = np.random.choice(len(y_true), min(200, len(y_true)), replace=False)\n",
        "        sample_indices = np.sort(sample_indices)\n",
        "        plt.plot(y_true[sample_indices], 'b-', label='True', alpha=0.7)\n",
        "        plt.plot(y_pred[sample_indices], 'r--', label='Predicted', alpha=0.7)\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Sample Predictions')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Absolute errors vs predictions\n",
        "        plt.subplot(3, 4, 6)\n",
        "        abs_errors = np.abs(residuals)\n",
        "        plt.scatter(y_pred, abs_errors, alpha=0.5, s=1)\n",
        "        plt.xlabel('Predictions')\n",
        "        plt.ylabel('Absolute Error')\n",
        "        plt.title('Absolute Error vs Prediction')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Percentage errors\n",
        "        plt.subplot(3, 4, 7)\n",
        "        pct_errors = 100 * residuals / (y_true + 1e-8)\n",
        "        plt.hist(pct_errors, bins=50, alpha=0.7)\n",
        "        plt.xlabel('Percentage Error (%)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Percentage Error Distribution')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Accuracy at different tolerances\n",
        "        plt.subplot(3, 4, 8)\n",
        "        tolerances = np.arange(0.1, 5.1, 0.1)\n",
        "        accuracies = [np.mean(abs_errors <= tol) * 100 for tol in tolerances]\n",
        "        plt.plot(tolerances, accuracies, 'b-', linewidth=2)\n",
        "        plt.xlabel('Error Tolerance')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.title('Accuracy vs Tolerance')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 9. True vs Predicted (binned)\n",
        "        plt.subplot(3, 4, 9)\n",
        "        n_bins = 10\n",
        "        true_bins = np.linspace(y_true.min(), y_true.max(), n_bins)\n",
        "        bin_indices = np.digitize(y_true, true_bins)\n",
        "        bin_means_true = [y_true[bin_indices == i].mean() for i in range(1, n_bins)]\n",
        "        bin_means_pred = [y_pred[bin_indices == i].mean() for i in range(1, n_bins)]\n",
        "        bin_means_true = [x for x in bin_means_true if not np.isnan(x)]\n",
        "        bin_means_pred = [x for x in bin_means_pred if not np.isnan(x)]\n",
        "        plt.plot(bin_means_true, bin_means_pred, 'bo-')\n",
        "        plt.plot([min(bin_means_true), max(bin_means_true)],\n",
        "                [min(bin_means_true), max(bin_means_true)], 'r--')\n",
        "        plt.xlabel('True (Binned)')\n",
        "        plt.ylabel('Predicted (Binned)')\n",
        "        plt.title('Binned True vs Predicted')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 10. Error by true value range\n",
        "        plt.subplot(3, 4, 10)\n",
        "        sorted_indices = np.argsort(y_true)\n",
        "        window_size = len(y_true) // 20\n",
        "        windowed_errors = []\n",
        "        windowed_trues = []\n",
        "        for i in range(0, len(sorted_indices) - window_size, window_size):\n",
        "            window_indices = sorted_indices[i:i+window_size]\n",
        "            windowed_errors.append(np.mean(abs_errors[window_indices]))\n",
        "            windowed_trues.append(np.mean(y_true[window_indices]))\n",
        "        plt.plot(windowed_trues, windowed_errors, 'ro-')\n",
        "        plt.xlabel('True Value (Windowed)')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.title('Error vs True Value Range')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 11. Prediction confidence intervals\n",
        "        plt.subplot(3, 4, 11)\n",
        "        sorted_pred_indices = np.argsort(y_pred)\n",
        "        n_points = min(100, len(y_pred))\n",
        "        step = len(y_pred) // n_points\n",
        "        sample_indices = sorted_pred_indices[::step]\n",
        "\n",
        "        pred_sample = y_pred[sample_indices]\n",
        "        true_sample = y_true[sample_indices]\n",
        "        error_sample = abs_errors[sample_indices]\n",
        "\n",
        "        plt.errorbar(pred_sample, true_sample, yerr=error_sample,\n",
        "                    fmt='o', alpha=0.7, capsize=2)\n",
        "        plt.plot([pred_sample.min(), pred_sample.max()],\n",
        "                [pred_sample.min(), pred_sample.max()], 'r--')\n",
        "        plt.xlabel('Predictions')\n",
        "        plt.ylabel('True Values')\n",
        "        plt.title('Predictions with Error Bars')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 12. Feature importance proxy (if possible)\n",
        "        plt.subplot(3, 4, 12)\n",
        "        plt.text(0.5, 0.5, f'R¬≤ = {r2_score(y_true, y_pred):.4f}\\n'\n",
        "                           f'RMSE = {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}\\n'\n",
        "                           f'MAE = {mean_absolute_error(y_true, y_pred):.4f}',\n",
        "                ha='center', va='center', transform=plt.gca().transAxes,\n",
        "                fontsize=14, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "        plt.title('Summary Metrics')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def run_complete_troubleshooting_pipeline(self, data_file=None, windows_file=None):\n",
        "        \"\"\"\n",
        "        Complete troubleshooting pipeline with comprehensive diagnostics\n",
        "        \"\"\"\n",
        "        print(\"=\"*100)\n",
        "        print(\"üîß COMPREHENSIVE TROUBLESHOOTING PIPELINE\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Step 1: Load data\n",
        "        print(\"\\n1Ô∏è‚É£ LOADING DATA\")\n",
        "        try:\n",
        "            if data_file is None:\n",
        "                data_file = f'{self.output_dir}generated-data-OPTIMIZED.npy'\n",
        "            if windows_file is None:\n",
        "                windows_file = f'{self.output_dir}generated-data-true-window-OPTIMIZED.npy'\n",
        "\n",
        "            x = np.load(data_file)\n",
        "            y = np.load(windows_file)\n",
        "            print(f\"‚úÖ Data loaded: X={x.shape}, y={y.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Comprehensive diagnosis\n",
        "        print(\"\\n2Ô∏è‚É£ COMPREHENSIVE DIAGNOSIS\")\n",
        "        issues = self.comprehensive_data_diagnosis(x, y)\n",
        "\n",
        "        # Step 3: Intelligent preprocessing\n",
        "        print(\"\\n3Ô∏è‚É£ INTELLIGENT PREPROCESSING\")\n",
        "        x_processed, y_processed, preprocessing_steps = self.intelligent_preprocessing(x, y)\n",
        "\n",
        "        # Step 4: Calculate data characteristics for adaptive architecture\n",
        "        print(\"\\n4Ô∏è‚É£ CALCULATING DATA CHARACTERISTICS\")\n",
        "        correlations = []\n",
        "        for i in range(min(x_processed.shape[1], 20)):  # Check first 20 features\n",
        "            try:\n",
        "                corr = np.corrcoef(x_processed[:, i], y_processed)[0, 1]\n",
        "                if not np.isnan(corr):\n",
        "                    correlations.append(abs(corr))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        data_characteristics = {\n",
        "            'n_samples': len(x_processed),\n",
        "            'n_features': x_processed.shape[1],\n",
        "            'max_correlation': max(correlations) if correlations else 0.001,\n",
        "            'mean_correlation': np.mean(correlations) if correlations else 0.001,\n",
        "            'target_std': np.std(y_processed),\n",
        "            'preprocessing_steps': preprocessing_steps\n",
        "        }\n",
        "\n",
        "        print(f\"   Max correlation: {data_characteristics['max_correlation']:.4f}\")\n",
        "        print(f\"   Mean correlation: {data_characteristics['mean_correlation']:.4f}\")\n",
        "\n",
        "        # Step 5: Prepare data\n",
        "        print(\"\\n5Ô∏è‚É£ PREPARING DATA\")\n",
        "        # Split data\n",
        "        x_temp, x_test, y_temp, y_test = train_test_split(\n",
        "            x_processed, y_processed, test_size=0.15, random_state=42\n",
        "        )\n",
        "        x_train, x_val, y_train, y_val = train_test_split(\n",
        "            x_temp, y_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 ‚âà 0.15\n",
        "        )\n",
        "\n",
        "        # Transform targets\n",
        "        y_train_scaled = self.transformer.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "        y_val_scaled = self.transformer.transform(y_val.reshape(-1, 1)).flatten()\n",
        "        y_test_scaled = self.transformer.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        print(f\"   Train: {x_train.shape[0]} samples\")\n",
        "        print(f\"   Validation: {x_val.shape[0]} samples\")\n",
        "        print(f\"   Test: {x_test.shape[0]} samples\")\n",
        "\n",
        "        # Step 6: Build adaptive model\n",
        "        print(\"\\n6Ô∏è‚É£ BUILDING ADAPTIVE MODEL\")\n",
        "        self.model = self.build_adaptive_mlp(x_train.shape[1], data_characteristics)\n",
        "\n",
        "        # Step 7: Train with advanced monitoring\n",
        "        print(\"\\n7Ô∏è‚É£ TRAINING WITH ADVANCED MONITORING\")\n",
        "        history = self.train_with_advanced_callbacks(\n",
        "            self.model, x_train, y_train_scaled, x_val, y_val_scaled,\n",
        "            epochs=1000, batch_size=32\n",
        "        )\n",
        "\n",
        "        self.is_fitted = True\n",
        "\n",
        "        # Step 8: Plot training history\n",
        "        print(\"\\n8Ô∏è‚É£ TRAINING HISTORY\")\n",
        "        self.plot_detailed_training_history(history)\n",
        "\n",
        "        # Step 9: Comprehensive evaluation\n",
        "        print(\"\\n9Ô∏è‚É£ COMPREHENSIVE EVALUATION\")\n",
        "        train_results = self.comprehensive_evaluation(\n",
        "            self.model, x_train, y_train_scaled, \"Training Set\"\n",
        "        )\n",
        "        val_results = self.comprehensive_evaluation(\n",
        "            self.model, x_val, y_val_scaled, \"Validation Set\"\n",
        "        )\n",
        "        test_results = self.comprehensive_evaluation(\n",
        "            self.model, x_test, y_test_scaled, \"Test Set\"\n",
        "        )\n",
        "\n",
        "        # Step 10: Final diagnosis and recommendations\n",
        "        print(\"\\nüîü FINAL DIAGNOSIS AND RECOMMENDATIONS\")\n",
        "        self.final_diagnosis_and_recommendations(\n",
        "            test_results, data_characteristics, issues, preprocessing_steps\n",
        "        )\n",
        "\n",
        "        # Step 11: Save results\n",
        "        results = {\n",
        "            'model': self.model,\n",
        "            'data_characteristics': data_characteristics,\n",
        "            'preprocessing_steps': preprocessing_steps,\n",
        "            'train_results': train_results,\n",
        "            'val_results': val_results,\n",
        "            'test_results': test_results,\n",
        "            'issues_found': issues,\n",
        "            'history': history\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_detailed_training_history(self, history):\n",
        "        \"\"\"Plot detailed training history\"\"\"\n",
        "        plt.figure(figsize=(20, 10))\n",
        "\n",
        "        # Loss\n",
        "        plt.subplot(2, 4, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        # MAE\n",
        "        plt.subplot(2, 4, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "        plt.title('Mean Absolute Error')\n",
        "        plt.ylabel('MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # MSE\n",
        "        plt.subplot(2, 4, 3)\n",
        "        plt.plot(history.history['mse'], label='Training MSE', linewidth=2)\n",
        "        plt.plot(history.history['val_mse'], label='Validation MSE', linewidth=2)\n",
        "        plt.title('Mean Squared Error')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        # Learning Rate (if available)\n",
        "        plt.subplot(2, 4, 4)\n",
        "        if 'lr' in history.history:\n",
        "            plt.plot(history.history['lr'], label='Learning Rate', linewidth=2)\n",
        "            plt.ylabel('Learning Rate')\n",
        "            plt.yscale('log')\n",
        "        else:\n",
        "            # Show epoch vs best val_loss to see convergence\n",
        "            val_loss = history.history['val_loss']\n",
        "            best_val_loss = np.minimum.accumulate(val_loss)\n",
        "            plt.plot(best_val_loss, label='Best Val Loss', linewidth=2)\n",
        "            plt.ylabel('Best Validation Loss')\n",
        "            plt.yscale('log')\n",
        "        plt.title('Learning Rate / Convergence')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Training vs Validation Gap\n",
        "        plt.subplot(2, 4, 5)\n",
        "        train_loss = np.array(history.history['loss'])\n",
        "        val_loss = np.array(history.history['val_loss'])\n",
        "        gap = val_loss - train_loss\n",
        "        plt.plot(gap, label='Val - Train Loss', linewidth=2, color='red')\n",
        "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.7)\n",
        "        plt.title('Overfitting Monitor')\n",
        "        plt.ylabel('Validation - Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss smoothed (moving average)\n",
        "        plt.subplot(2, 4, 6)\n",
        "        window = min(10, len(history.history['loss']) // 10)\n",
        "        if window > 1:\n",
        "            train_smooth = np.convolve(history.history['loss'], np.ones(window)/window, mode='valid')\n",
        "            val_smooth = np.convolve(history.history['val_loss'], np.ones(window)/window, mode='valid')\n",
        "            plt.plot(train_smooth, label='Training (Smoothed)', linewidth=2)\n",
        "            plt.plot(val_smooth, label='Validation (Smoothed)', linewidth=2)\n",
        "        else:\n",
        "            plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "        plt.title('Smoothed Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        # Improvement rate\n",
        "        plt.subplot(2, 4, 7)\n",
        "        val_loss = np.array(history.history['val_loss'])\n",
        "        if len(val_loss) > 10:\n",
        "            improvement = val_loss[:-10] - val_loss[10:]  # Improvement over 10 epochs\n",
        "            plt.plot(improvement, label='10-Epoch Improvement', linewidth=2)\n",
        "            plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "        plt.title('Learning Progress')\n",
        "        plt.ylabel('Loss Improvement')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Final metrics summary\n",
        "        plt.subplot(2, 4, 8)\n",
        "        final_train_loss = history.history['loss'][-1]\n",
        "        final_val_loss = history.history['val_loss'][-1]\n",
        "        final_train_mae = history.history['mae'][-1]\n",
        "        final_val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "        metrics_text = f\"\"\"Final Training Metrics:\n",
        "\n",
        "Loss: {final_train_loss:.6f}\n",
        "MAE: {final_train_mae:.6f}\n",
        "\n",
        "Final Validation Metrics:\n",
        "\n",
        "Loss: {final_val_loss:.6f}\n",
        "MAE: {final_val_mae:.6f}\n",
        "\n",
        "Overfitting Gap:\n",
        "{final_val_loss - final_train_loss:.6f}\n",
        "\n",
        "Total Epochs: {len(history.history['loss'])}\"\"\"\n",
        "\n",
        "        plt.text(0.05, 0.95, metrics_text, transform=plt.gca().transAxes,\n",
        "                fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\"))\n",
        "        plt.title('Final Metrics Summary')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def final_diagnosis_and_recommendations(self, test_results, data_characteristics,\n",
        "                                          initial_issues, preprocessing_steps):\n",
        "        \"\"\"\n",
        "        Final diagnosis and actionable recommendations\n",
        "        \"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"üè• FINAL DIAGNOSIS AND RECOMMENDATIONS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        r2 = test_results['r2']\n",
        "        mae = test_results['mae']\n",
        "\n",
        "        # Performance classification\n",
        "        if r2 >= 0.8:\n",
        "            performance = \"EXCELLENT\"\n",
        "            color = \"üü¢\"\n",
        "        elif r2 >= 0.6:\n",
        "            performance = \"GOOD\"\n",
        "            color = \"üü°\"\n",
        "        elif r2 >= 0.3:\n",
        "            performance = \"MODERATE\"\n",
        "            color = \"üü†\"\n",
        "        else:\n",
        "            performance = \"POOR\"\n",
        "            color = \"üî¥\"\n",
        "\n",
        "        print(f\"{color} PERFORMANCE ASSESSMENT: {performance}\")\n",
        "        print(f\"   Test R¬≤: {r2:.6f}\")\n",
        "        print(f\"   Test MAE: {mae:.4f}\")\n",
        "        print(f\"   Accuracy ¬±1: {test_results['accuracies'][1.0]:.1f}%\")\n",
        "        print(f\"   Accuracy ¬±2: {test_results['accuracies'][2.0]:.1f}%\")\n",
        "\n",
        "        # Detailed diagnosis\n",
        "        print(f\"\\nüîç DETAILED DIAGNOSIS:\")\n",
        "\n",
        "        # Data quality issues\n",
        "        if initial_issues:\n",
        "            print(f\"   üìä Data Quality Issues Found: {len(initial_issues)}\")\n",
        "            for issue in initial_issues:\n",
        "                print(f\"      - {issue}\")\n",
        "        else:\n",
        "            print(f\"   üìä Data Quality: Good\")\n",
        "\n",
        "        # Model performance analysis\n",
        "        max_corr = data_characteristics['max_correlation']\n",
        "        mean_corr = data_characteristics['mean_correlation']\n",
        "\n",
        "        print(f\"   üîó Feature-Target Relationships:\")\n",
        "        print(f\"      - Max correlation: {max_corr:.4f}\")\n",
        "        print(f\"      - Mean correlation: {mean_corr:.4f}\")\n",
        "\n",
        "        if max_corr < 0.05:\n",
        "            print(f\"      ‚ö†Ô∏è Extremely weak relationships detected\")\n",
        "        elif max_corr < 0.1:\n",
        "            print(f\"      ‚ö†Ô∏è Very weak relationships detected\")\n",
        "        elif max_corr < 0.2:\n",
        "            print(f\"      ‚ÑπÔ∏è Weak but learnable relationships\")\n",
        "        else:\n",
        "            print(f\"      ‚úÖ Strong relationships present\")\n",
        "\n",
        "        # Recommendations\n",
        "        print(f\"\\nüí° SPECIFIC RECOMMENDATIONS:\")\n",
        "\n",
        "        if r2 < 0.3:\n",
        "            print(f\"   üö® URGENT IMPROVEMENTS NEEDED:\")\n",
        "            if max_corr < 0.05:\n",
        "                print(f\"      1. Feature Engineering Priority:\")\n",
        "                print(f\"         - Create polynomial features\")\n",
        "                print(f\"         - Try interaction terms\")\n",
        "                print(f\"         - Consider domain-specific transformations\")\n",
        "                print(f\"         - Investigate time-based features\")\n",
        "\n",
        "            print(f\"      2. Model Architecture:\")\n",
        "            print(f\"         - Try ensemble methods (Random Forest, XGBoost)\")\n",
        "            print(f\"         - Consider sequence models (LSTM, GRU)\")\n",
        "            print(f\"         - Experiment with deeper architectures\")\n",
        "\n",
        "            print(f\"      3. Data Investigation:\")\n",
        "            print(f\"         - Check if VAE generated meaningful synthetic data\")\n",
        "            print(f\"         - Verify target variable calculation\")\n",
        "            print(f\"         - Investigate data leakage or preprocessing errors\")\n",
        "\n",
        "        elif r2 < 0.6:\n",
        "            print(f\"   üîß IMPROVEMENT OPPORTUNITIES:\")\n",
        "            print(f\"      1. Hyperparameter Tuning:\")\n",
        "            print(f\"         - Grid search for learning rate and batch size\")\n",
        "            print(f\"         - Experiment with different optimizers\")\n",
        "            print(f\"         - Try different regularization strengths\")\n",
        "\n",
        "            print(f\"      2. Advanced Techniques:\")\n",
        "            print(f\"         - Cross-validation for better estimates\")\n",
        "            print(f\"         - Ensemble multiple models\")\n",
        "            print(f\"         - Feature selection methods\")\n",
        "\n",
        "        else:\n",
        "            print(f\"   ‚úÖ MODEL PERFORMING WELL:\")\n",
        "            print(f\"      1. Fine-tuning options:\")\n",
        "            print(f\"         - Slight hyperparameter adjustments\")\n",
        "            print(f\"         - Ensemble for marginal improvements\")\n",
        "            print(f\"         - Production deployment considerations\")\n",
        "\n",
        "        # Code recommendations\n",
        "        print(f\"\\nüíª IMMEDIATE CODE CHANGES TO TRY:\")\n",
        "\n",
        "        if max_corr < 0.1:\n",
        "            print(f\"\"\"\n",
        "# Try this enhanced architecture for weak relationships:\n",
        "model = Sequential([\n",
        "    Dense(1024, activation='relu', kernel_regularizer=l1_l2(0.001, 0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l1_l2(0.001, 0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Use higher learning rate and robust loss\n",
        "optimizer = Adam(learning_rate=0.003, clipnorm=1.0)\n",
        "model.compile(loss='huber', optimizer=optimizer)\n",
        "\"\"\")\n",
        "\n",
        "        if \"skewed\" in str(initial_issues):\n",
        "            print(f\"\"\"\n",
        "# Try log transformation of target:\n",
        "y_transformed = np.log1p(y - np.min(y) + 1)\n",
        "\"\"\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "# Alternative models to try:\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Random Forest (often works well with weak relationships)\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# XGBoost (excellent for tabular data)\n",
        "xgb = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42)\n",
        "xgb.fit(x_train, y_train)\n",
        "\"\"\")\n",
        "\n",
        "        # Preprocessing recommendations\n",
        "        if preprocessing_steps:\n",
        "            print(f\"\\nüîÑ PREPROCESSING APPLIED:\")\n",
        "            for step in preprocessing_steps:\n",
        "                print(f\"      ‚úÖ {step}\")\n",
        "\n",
        "        # Final action items\n",
        "        print(f\"\\nüìã ACTION ITEMS (Priority Order):\")\n",
        "        if r2 < 0.1:\n",
        "            print(f\"      1. üö® CRITICAL: Verify data quality and target calculation\")\n",
        "            print(f\"      2. üö® CRITICAL: Check VAE synthetic data quality\")\n",
        "            print(f\"      3. Try Random Forest as baseline comparison\")\n",
        "            print(f\"      4. Create polynomial/interaction features\")\n",
        "        elif r2 < 0.3:\n",
        "            print(f\"      1. Try Random Forest/XGBoost for comparison\")\n",
        "            print(f\"      2. Feature engineering (polynomial, interactions)\")\n",
        "            print(f\"      3. Deeper neural network architecture\")\n",
        "        elif r2 < 0.6:\n",
        "            print(f\"      1. Hyperparameter tuning\")\n",
        "            print(f\"      2. Ensemble methods\")\n",
        "            print(f\"      3. Cross-validation analysis\")\n",
        "        else:\n",
        "            print(f\"      1. Model is ready for production\")\n",
        "            print(f\"      2. Consider incremental learning setup\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# Quick usage functions\n",
        "def quick_diagnosis(data_file=None, windows_file=None):\n",
        "    \"\"\"Quick diagnosis of MLP training issues\"\"\"\n",
        "    agent = TroubleshootingMLPAgent()\n",
        "    return agent.run_complete_troubleshooting_pipeline(data_file, windows_file)\n",
        "\n",
        "def compare_models(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"Compare MLP with other models\"\"\"\n",
        "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "    from sklearn.metrics import r2_score\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Random Forest\n",
        "    print(\"Training Random Forest...\")\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(x_train, y_train)\n",
        "    rf_pred = rf.predict(x_test)\n",
        "    results['Random Forest'] = r2_score(y_test, rf_pred)\n",
        "\n",
        "    # Gradient Boosting\n",
        "    print(\"Training Gradient Boosting...\")\n",
        "    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    gb.fit(x_train, y_train)\n",
        "    gb_pred = gb.predict(x_test)\n",
        "    results['Gradient Boosting'] = r2_score(y_test, gb_pred)\n",
        "\n",
        "    print(\"\\nModel Comparison (R¬≤ scores):\")\n",
        "    for model, score in results.items():\n",
        "        print(f\"  {model}: {score:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üîß Running Comprehensive MLP Troubleshooting...\")\n",
        "\n",
        "    # Run complete troubleshooting pipeline\n",
        "    results = quick_diagnosis()\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\nüéØ SUMMARY:\")\n",
        "        print(f\"   Test R¬≤: {results['test_results']['r2']:.6f}\")\n",
        "        print(f\"   Issues found: {len(results['issues_found'])}\")\n",
        "        print(f\"   Preprocessing steps: {len(results['preprocessing_steps'])}\")\n",
        "\n",
        "        # If performance is still poor, suggest alternative approaches\n",
        "        if results['test_results']['r2'] < 0.3:\n",
        "            print(f\"\\n‚ö†Ô∏è Poor performance detected. Consider:\")\n",
        "            print(f\"   1. Check if VAE synthetic data is meaningful\")\n",
        "            print(f\"   2. Verify target variable (VAR window) calculation\")\n",
        "            print(f\"   3. Try completely different approach (e.g., time series methods)\")\n",
        "    else:\n",
        "        print(\"‚ùå Pipeline failed. Check data file paths and availability.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a1472cf4762e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/JNJ.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/JNJ.csv'"
          ]
        }
      ]
    }
  ]
}