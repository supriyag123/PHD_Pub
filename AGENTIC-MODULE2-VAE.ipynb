{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "913128f7-332f-40cd-99f0-61fd4e9594c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing trained VAE models. Loading...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {}}.\n\nException encountered: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-384004133.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found existing trained VAE models. Loading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VAE models loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    368\u001b[0m             )\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {}}.\n\nException encountered: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "\n",
        "# Load data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences_WINDOW-Daily-DIRECT-VAR.npy')\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "maxval = train_data.shape[0]\n",
        "count_train = int(math.ceil(0.8*maxval))\n",
        "x_train = train_data[:count_train]\n",
        "x_test = train_data[count_train:]\n",
        "\n",
        "# Always define the custom Sampling layer (needed for loading existing models)\n",
        "saving.get_custom_objects().clear()\n",
        "\n",
        "@saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def __init__(self, factor):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "\n",
        "# Check if trained VAE models already exist\n",
        "encoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-encoder-latent5-dim256.keras'\n",
        "decoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "    print(\"Found existing trained VAE models. Loading...\")\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "    print(\"VAE models loaded successfully!\")\n",
        "    history = None  # No training history since we didn't train\n",
        "\n",
        "else:\n",
        "    print(\"No existing VAE models found. Training new VAE...\")\n",
        "\n",
        "    # Build the encoder\n",
        "    latent_dim = 5\n",
        "    intermediate_dim = 256\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(window_size, n_features), name=\"encoder_input\")\n",
        "    x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "    xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "    x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\")(xx)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "    z = Sampling(1)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "\n",
        "    # Decoder\n",
        "    inp_z = Input(shape=(latent_dim,), name=\"decoder\")\n",
        "    x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "    x2 = layers.Dense(int(intermediate_dim/2), name=\"Dense2\")(x1)\n",
        "    x22 = layers.LSTM(int(intermediate_dim/2), activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "    x3 = layers.LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "    decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "    decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "    # Parameters\n",
        "    n_epochs = 150\n",
        "    klstart = 20\n",
        "    kl_annealtime = n_epochs - klstart\n",
        "    weight = K.variable(0.0)\n",
        "\n",
        "    # Define the VAE as a Model with a custom train_step\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(VAE, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(data)\n",
        "                reconstruction = self.decoder(z)\n",
        "                reconstruction_loss = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "                )\n",
        "\n",
        "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = reconstruction_loss + (weight * kl_loss)\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        def test_step(self, data):\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "    # CALLBACKS\n",
        "    es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "    class AnnealingCallback(Callback):\n",
        "        def __init__(self, weight):\n",
        "            self.weight = weight\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if epoch > klstart and epoch < klstart * 1.2:\n",
        "                new_weight = min(K.get_value(self.weight) + (1. / kl_annealtime), 1.)\n",
        "                K.set_value(self.weight, new_weight)\n",
        "            print(\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "    # Train the VAE\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "    history = vae.fit(x_train,\n",
        "                      epochs=n_epochs,\n",
        "                      batch_size=32,\n",
        "                      validation_split=0.1,\n",
        "                      callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "    # Save models\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "    print(\"VAE training complete and models saved!\")\n",
        "\n",
        "    # Reload models to ensure consistency\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "\n",
        "# Plot training history (only if we actually trained)\n",
        "if history is not None:\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['reconstruction_loss'], label='reconstruction_loss')\n",
        "    plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "    # Just Loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training plots since VAE was loaded from existing models\")\n",
        "\n",
        "# PLOT TRAIN RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_train)\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_train[:, :, 5], \"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\", label=\"reconstructed\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT TEST RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_test[:, :, :])\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_test[:, :, 5], \"r\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\")\n",
        "plt.show()\n",
        "\n",
        "# Get the corresponding window labels for test data\n",
        "window_label_test = window_label[count_train:]  # Get test portion of window_label\n",
        "fig = px.scatter(None, x=X_test_encoded[2][:, 0], y=X_test_encoded[2][:, 1], opacity=1, color=window_label_test.astype(str))\n",
        "fig.update_layout(dict(plot_bgcolor='white'))\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()\n",
        "\n",
        "# Check if generated data already exists\n",
        "generated_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated_large_subsquence2_data.npy'\n",
        "\n",
        "if os.path.exists(generated_data_path):\n",
        "    print(\"Found existing generated data. Loading...\")\n",
        "    results1 = np.load(generated_data_path)\n",
        "    print(f\"Loaded generated data with shape: {results1.shape}\")\n",
        "else:\n",
        "    print(\"No existing generated data found. Generating new data...\")\n",
        "    # Generate data for MLP\n",
        "    generator_multiply = 100\n",
        "\n",
        "    X_train_encoded = encoder.predict(train_data)\n",
        "    mu, logvar, z = X_train_encoded\n",
        "    sigma = tf.exp(0.5 * logvar)\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "    store = list()\n",
        "\n",
        "    for i in range(0, batch):\n",
        "        all_Z_i = tf.random.normal(shape=(generator_multiply, dim), mean=mu[i, :], stddev=sigma[i, :])\n",
        "        X_train_decoded = decoder.predict(all_Z_i)\n",
        "        X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0], window_size * n_features))\n",
        "        store.append(X_train_decoded)\n",
        "\n",
        "    results1 = np.concatenate(store, axis=0)\n",
        "    np.save(generated_data_path, results1)\n",
        "    print(f\"Generated and saved new data with shape: {results1.shape}\")\n",
        "\n",
        "# Generate window labels using VAR analysis - Dynamic batching with checkpoints\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "# Check if final VAR analysis results already exist\n",
        "final_window_labels_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy'\n",
        "final_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data2.npy'\n",
        "\n",
        "if os.path.exists(final_window_labels_path) and os.path.exists(final_data_path):\n",
        "    print(\"Found existing VAR analysis results. Loading...\")\n",
        "    y = np.load(final_window_labels_path)\n",
        "    x_final = np.load(final_data_path)\n",
        "    print(f\"Loaded final results - X: {x_final.shape}, Y: {y.shape}\")\n",
        "else:\n",
        "    print(\"No existing VAR analysis results found. Need to run VAR analysis...\")\n",
        "\n",
        "    # Make sure we have the generated data (results1)\n",
        "    x = results1\n",
        "    x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "    n_future = 1\n",
        "    K = window_size\n",
        "\n",
        "    from statsmodels.tsa.api import VAR\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    def process_with_dynamic_batching(x_3d, batch_size=50000, checkpoint_dir=\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM\"):\n",
        "        checkpoint_file = f\"{checkpoint_dir}/var_analysis_progress.pkl\"\n",
        "\n",
        "        # Load previous progress if exists\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                start_idx, all_windows = pickle.load(f)\n",
        "            print(f\"Resuming VAR analysis from sample {start_idx}. Already processed: {len(all_windows)} samples\")\n",
        "        else:\n",
        "            start_idx, all_windows = 0, []\n",
        "            print(\"Starting fresh VAR analysis\")\n",
        "\n",
        "        total_samples = x_3d.shape[0]\n",
        "        print(f\"Total samples to process: {total_samples}\")\n",
        "\n",
        "        batch_num = start_idx // batch_size + 1\n",
        "\n",
        "        for i in range(start_idx, total_samples):\n",
        "            # Your exact same VAR analysis logic\n",
        "            rmse_list = []\n",
        "            for k in range(2, round(K)):\n",
        "                cur_seq = x_3d[i, :, :]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "                model = VAR(df_train)\n",
        "                try:\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "                    if i % 5000 == 0:  # Reduce print frequency\n",
        "                        print('VAR could not solve row number', i, k)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            all_windows.append(min_sw)\n",
        "\n",
        "            # Progress reporting and checkpointing\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f'Processed {i + 1}/{total_samples} samples. Current SW = {min_sw}')\n",
        "\n",
        "            # Save checkpoint every 5000 samples\n",
        "            if (i + 1) % 5000 == 0:\n",
        "                with open(checkpoint_file, 'wb') as f:\n",
        "                    pickle.dump((i + 1, all_windows), f)\n",
        "                print(f\"Checkpoint saved at sample {i + 1}\")\n",
        "\n",
        "                # Optional: Clean up memory\n",
        "                gc.collect()\n",
        "\n",
        "            # Save batch results (compatible with original format) every batch_size samples\n",
        "            if (i + 1) % batch_size == 0 or (i + 1) == total_samples:\n",
        "                batch_start = ((i + 1) - 1) // batch_size * batch_size\n",
        "                batch_windows = all_windows[batch_start:i + 1]\n",
        "                batch_data = x[batch_start:i + 1, :]\n",
        "\n",
        "                np.save(f'{checkpoint_dir}/generated-data-true-window2-BATCH{batch_num}.npy', np.array(batch_windows))\n",
        "                np.save(f'{checkpoint_dir}/generated-data2-BATCH{batch_num}.npy', batch_data)\n",
        "\n",
        "                print(f\"Batch {batch_num} saved: samples {batch_start} to {i}\")\n",
        "                batch_num += 1\n",
        "\n",
        "        # Final save\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump((total_samples, all_windows), f)\n",
        "\n",
        "        # Clean up checkpoint file\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "\n",
        "        return np.array(all_windows)\n",
        "\n",
        "    # Run the dynamic batching process\n",
        "    print(\"Starting VAR analysis with dynamic batching...\")\n",
        "    y = process_with_dynamic_batching(x_3d)\n",
        "\n",
        "    # Save final combined results\n",
        "    np.save(final_window_labels_path, y)\n",
        "    np.save(final_data_path, x[:len(y)])\n",
        "    print(f\"VAR analysis complete. Saved final results - X: {x[:len(y)].shape}, Y: {y.shape}\")\n",
        "\n",
        "print(\"VAE training complete. Generated data and window labels saved.\")\n",
        "print(f\"Generated data shape: {x[:len(y)].shape}\")\n",
        "print(f\"Window labels shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb",
      "authorship_tag": "ABX9TyNXNjOA+xCVD9lmTN8HLkpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}