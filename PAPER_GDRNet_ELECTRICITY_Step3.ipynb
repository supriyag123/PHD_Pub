{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/PAPER_GDRNet_ELECTRICITY_Step3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "outputId": "047fc98b-f435-451b-f392-8f8073cb17dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "3577/3577 [==============================] - 24s 6ms/step - loss: 0.9185 - mean_squared_error: 0.9185 - val_loss: 1.0435 - val_mean_squared_error: 1.0435\n",
            "Epoch 2/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8991 - mean_squared_error: 0.8991 - val_loss: 1.0619 - val_mean_squared_error: 1.0619\n",
            "Epoch 3/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8945 - mean_squared_error: 0.8945 - val_loss: 1.0762 - val_mean_squared_error: 1.0762\n",
            "Epoch 4/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8922 - mean_squared_error: 0.8922 - val_loss: 1.0782 - val_mean_squared_error: 1.0782\n",
            "Epoch 5/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8805 - mean_squared_error: 0.8805 - val_loss: 1.0928 - val_mean_squared_error: 1.0928\n",
            "Epoch 6/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8824 - mean_squared_error: 0.8824 - val_loss: 1.0449 - val_mean_squared_error: 1.0449\n",
            "Epoch 7/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8951 - mean_squared_error: 0.8951 - val_loss: 1.1905 - val_mean_squared_error: 1.1905\n",
            "Epoch 8/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 1.1524 - val_mean_squared_error: 1.1524\n",
            "Epoch 9/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8692 - mean_squared_error: 0.8692 - val_loss: 1.2827 - val_mean_squared_error: 1.2827\n",
            "Epoch 10/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8690 - mean_squared_error: 0.8690 - val_loss: 1.2116 - val_mean_squared_error: 1.2116\n",
            "Epoch 11/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8674 - mean_squared_error: 0.8674 - val_loss: 1.2361 - val_mean_squared_error: 1.2361\n",
            "Epoch 12/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8608 - mean_squared_error: 0.8608 - val_loss: 1.4098 - val_mean_squared_error: 1.4098\n",
            "Epoch 13/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.9061 - mean_squared_error: 0.9061 - val_loss: 1.1587 - val_mean_squared_error: 1.1587\n",
            "Epoch 14/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8866 - mean_squared_error: 0.8866 - val_loss: 1.0676 - val_mean_squared_error: 1.0676\n",
            "Epoch 15/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8655 - mean_squared_error: 0.8655 - val_loss: 1.2327 - val_mean_squared_error: 1.2327\n",
            "Epoch 16/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8596 - mean_squared_error: 0.8596 - val_loss: 1.2794 - val_mean_squared_error: 1.2794\n",
            "Epoch 17/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.2192 - val_mean_squared_error: 1.2192\n",
            "Epoch 18/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 1.3751 - val_mean_squared_error: 1.3751\n",
            "Epoch 19/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8495 - mean_squared_error: 0.8495 - val_loss: 1.2974 - val_mean_squared_error: 1.2974\n",
            "Epoch 20/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8491 - mean_squared_error: 0.8491 - val_loss: 1.3180 - val_mean_squared_error: 1.3180\n",
            "Epoch 21/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 1.4365 - val_mean_squared_error: 1.4365\n",
            "Epoch 22/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 1.2841 - val_mean_squared_error: 1.2841\n",
            "Epoch 23/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 1.4520 - val_mean_squared_error: 1.4520\n",
            "Epoch 24/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 1.3606 - val_mean_squared_error: 1.3606\n",
            "Epoch 25/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 1.3398 - val_mean_squared_error: 1.3398\n",
            "Epoch 26/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 1.3201 - val_mean_squared_error: 1.3201\n",
            "Epoch 27/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 1.3433 - val_mean_squared_error: 1.3433\n",
            "Epoch 28/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 1.3229 - val_mean_squared_error: 1.3229\n",
            "Epoch 29/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8363 - mean_squared_error: 0.8363 - val_loss: 1.2124 - val_mean_squared_error: 1.2124\n",
            "Epoch 30/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8363 - mean_squared_error: 0.8363 - val_loss: 1.5107 - val_mean_squared_error: 1.5107\n",
            "Epoch 31/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 1.2928 - val_mean_squared_error: 1.2928\n",
            "Epoch 32/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 1.2695 - val_mean_squared_error: 1.2695\n",
            "Epoch 33/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 1.2768 - val_mean_squared_error: 1.2768\n",
            "Epoch 34/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 1.3598 - val_mean_squared_error: 1.3598\n",
            "Epoch 35/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 1.2422 - val_mean_squared_error: 1.2422\n",
            "Epoch 36/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8277 - mean_squared_error: 0.8277 - val_loss: 1.2889 - val_mean_squared_error: 1.2889\n",
            "Epoch 37/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 1.3006 - val_mean_squared_error: 1.3006\n",
            "Epoch 38/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8273 - mean_squared_error: 0.8273 - val_loss: 1.2373 - val_mean_squared_error: 1.2373\n",
            "Epoch 39/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 1.3591 - val_mean_squared_error: 1.3591\n",
            "Epoch 40/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 1.3215 - val_mean_squared_error: 1.3215\n",
            "Epoch 41/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 1.3934 - val_mean_squared_error: 1.3934\n",
            "Epoch 42/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 1.2583 - val_mean_squared_error: 1.2583\n",
            "Epoch 43/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8237 - mean_squared_error: 0.8237 - val_loss: 1.2327 - val_mean_squared_error: 1.2327\n",
            "Epoch 44/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8239 - mean_squared_error: 0.8239 - val_loss: 1.2672 - val_mean_squared_error: 1.2672\n",
            "Epoch 45/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8214 - mean_squared_error: 0.8214 - val_loss: 1.1610 - val_mean_squared_error: 1.1610\n",
            "Epoch 46/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8200 - mean_squared_error: 0.8200 - val_loss: 1.2404 - val_mean_squared_error: 1.2404\n",
            "Epoch 47/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8207 - mean_squared_error: 0.8207 - val_loss: 1.2829 - val_mean_squared_error: 1.2829\n",
            "Epoch 48/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8176 - mean_squared_error: 0.8176 - val_loss: 1.3396 - val_mean_squared_error: 1.3396\n",
            "Epoch 49/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8196 - mean_squared_error: 0.8196 - val_loss: 1.2733 - val_mean_squared_error: 1.2733\n",
            "Epoch 50/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8181 - mean_squared_error: 0.8181 - val_loss: 1.2351 - val_mean_squared_error: 1.2351\n",
            "Epoch 51/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8184 - mean_squared_error: 0.8184 - val_loss: 1.2265 - val_mean_squared_error: 1.2265\n",
            "Epoch 52/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8185 - mean_squared_error: 0.8185 - val_loss: 1.2013 - val_mean_squared_error: 1.2013\n",
            "Epoch 53/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8150 - mean_squared_error: 0.8150 - val_loss: 1.2961 - val_mean_squared_error: 1.2961\n",
            "Epoch 54/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8190 - mean_squared_error: 0.8190 - val_loss: 1.1869 - val_mean_squared_error: 1.1869\n",
            "Epoch 55/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8163 - mean_squared_error: 0.8163 - val_loss: 1.2072 - val_mean_squared_error: 1.2072\n",
            "Epoch 56/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8170 - mean_squared_error: 0.8170 - val_loss: 1.2842 - val_mean_squared_error: 1.2842\n",
            "Epoch 57/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8151 - mean_squared_error: 0.8151 - val_loss: 1.3565 - val_mean_squared_error: 1.3565\n",
            "Epoch 58/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8155 - mean_squared_error: 0.8155 - val_loss: 1.2979 - val_mean_squared_error: 1.2979\n",
            "Epoch 59/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8156 - mean_squared_error: 0.8156 - val_loss: 1.2540 - val_mean_squared_error: 1.2540\n",
            "Epoch 60/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8154 - mean_squared_error: 0.8154 - val_loss: 1.3956 - val_mean_squared_error: 1.3956\n",
            "Epoch 61/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8118 - mean_squared_error: 0.8118 - val_loss: 1.3581 - val_mean_squared_error: 1.3581\n",
            "Epoch 62/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8155 - mean_squared_error: 0.8155 - val_loss: 1.3175 - val_mean_squared_error: 1.3175\n",
            "Epoch 63/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8141 - mean_squared_error: 0.8141 - val_loss: 1.5088 - val_mean_squared_error: 1.5088\n",
            "Epoch 64/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8129 - mean_squared_error: 0.8129 - val_loss: 1.4523 - val_mean_squared_error: 1.4523\n",
            "Epoch 65/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 1.2681 - val_mean_squared_error: 1.2681\n",
            "Epoch 66/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8103 - mean_squared_error: 0.8103 - val_loss: 1.7314 - val_mean_squared_error: 1.7314\n",
            "Epoch 67/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8120 - mean_squared_error: 0.8120 - val_loss: 1.4525 - val_mean_squared_error: 1.4525\n",
            "Epoch 68/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8152 - mean_squared_error: 0.8152 - val_loss: 1.4631 - val_mean_squared_error: 1.4631\n",
            "Epoch 69/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8153 - mean_squared_error: 0.8153 - val_loss: 1.5238 - val_mean_squared_error: 1.5238\n",
            "Epoch 70/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8135 - mean_squared_error: 0.8135 - val_loss: 1.5553 - val_mean_squared_error: 1.5553\n",
            "Epoch 71/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8129 - mean_squared_error: 0.8129 - val_loss: 1.4255 - val_mean_squared_error: 1.4255\n",
            "Epoch 72/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8140 - mean_squared_error: 0.8140 - val_loss: 1.4434 - val_mean_squared_error: 1.4434\n",
            "Epoch 73/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8098 - mean_squared_error: 0.8098 - val_loss: 1.7324 - val_mean_squared_error: 1.7324\n",
            "Epoch 74/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8072 - mean_squared_error: 0.8072 - val_loss: 1.3952 - val_mean_squared_error: 1.3952\n",
            "Epoch 75/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8081 - mean_squared_error: 0.8081 - val_loss: 1.4726 - val_mean_squared_error: 1.4726\n",
            "Epoch 76/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8096 - mean_squared_error: 0.8096 - val_loss: 1.6784 - val_mean_squared_error: 1.6784\n",
            "Epoch 77/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8101 - mean_squared_error: 0.8101 - val_loss: 1.4531 - val_mean_squared_error: 1.4531\n",
            "Epoch 78/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8119 - mean_squared_error: 0.8119 - val_loss: 1.5958 - val_mean_squared_error: 1.5958\n",
            "Epoch 79/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8108 - mean_squared_error: 0.8108 - val_loss: 1.4356 - val_mean_squared_error: 1.4356\n",
            "Epoch 80/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8075 - mean_squared_error: 0.8075 - val_loss: 1.4269 - val_mean_squared_error: 1.4269\n",
            "Epoch 81/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8101 - mean_squared_error: 0.8101 - val_loss: 1.6829 - val_mean_squared_error: 1.6829\n",
            "Epoch 82/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8118 - mean_squared_error: 0.8118 - val_loss: 1.2836 - val_mean_squared_error: 1.2836\n",
            "Epoch 83/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8090 - mean_squared_error: 0.8090 - val_loss: 1.1014 - val_mean_squared_error: 1.1014\n",
            "Epoch 84/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8131 - mean_squared_error: 0.8131 - val_loss: 1.3594 - val_mean_squared_error: 1.3594\n",
            "Epoch 85/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 1.2384 - val_mean_squared_error: 1.2384\n",
            "Epoch 86/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8085 - mean_squared_error: 0.8085 - val_loss: 1.5333 - val_mean_squared_error: 1.5333\n",
            "Epoch 87/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8098 - mean_squared_error: 0.8098 - val_loss: 1.3572 - val_mean_squared_error: 1.3572\n",
            "Epoch 88/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8076 - mean_squared_error: 0.8076 - val_loss: 1.5300 - val_mean_squared_error: 1.5300\n",
            "Epoch 89/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8079 - mean_squared_error: 0.8079 - val_loss: 1.4932 - val_mean_squared_error: 1.4932\n",
            "Epoch 90/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8037 - mean_squared_error: 0.8037 - val_loss: 1.1828 - val_mean_squared_error: 1.1828\n",
            "Epoch 91/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8056 - mean_squared_error: 0.8056 - val_loss: 1.3882 - val_mean_squared_error: 1.3882\n",
            "Epoch 92/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8069 - mean_squared_error: 0.8069 - val_loss: 1.2695 - val_mean_squared_error: 1.2695\n",
            "Epoch 93/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8067 - mean_squared_error: 0.8067 - val_loss: 1.2806 - val_mean_squared_error: 1.2806\n",
            "Epoch 94/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8041 - mean_squared_error: 0.8041 - val_loss: 1.3681 - val_mean_squared_error: 1.3681\n",
            "Epoch 95/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8064 - mean_squared_error: 0.8064 - val_loss: 1.3909 - val_mean_squared_error: 1.3909\n",
            "Epoch 96/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8037 - mean_squared_error: 0.8037 - val_loss: 1.2738 - val_mean_squared_error: 1.2738\n",
            "Epoch 97/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.8056 - mean_squared_error: 0.8056 - val_loss: 1.8730 - val_mean_squared_error: 1.8730\n",
            "Epoch 98/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8067 - mean_squared_error: 0.8067 - val_loss: 1.3149 - val_mean_squared_error: 1.3149\n",
            "Epoch 99/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8039 - mean_squared_error: 0.8039 - val_loss: 1.4084 - val_mean_squared_error: 1.4084\n",
            "Epoch 100/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8084 - mean_squared_error: 0.8084 - val_loss: 1.4585 - val_mean_squared_error: 1.4585\n",
            "Epoch 101/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8066 - mean_squared_error: 0.8066 - val_loss: 1.4827 - val_mean_squared_error: 1.4827\n",
            "Epoch 102/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8003 - mean_squared_error: 0.8003 - val_loss: 1.4439 - val_mean_squared_error: 1.4439\n",
            "Epoch 103/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8043 - mean_squared_error: 0.8043 - val_loss: 1.2284 - val_mean_squared_error: 1.2284\n",
            "Epoch 104/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8028 - mean_squared_error: 0.8028 - val_loss: 1.3396 - val_mean_squared_error: 1.3396\n",
            "Epoch 105/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.8022 - mean_squared_error: 0.8022 - val_loss: 1.3265 - val_mean_squared_error: 1.3265\n",
            "Epoch 106/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8000 - mean_squared_error: 0.8000 - val_loss: 1.4116 - val_mean_squared_error: 1.4116\n",
            "Epoch 107/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8023 - mean_squared_error: 0.8023 - val_loss: 1.3045 - val_mean_squared_error: 1.3045\n",
            "Epoch 108/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8043 - mean_squared_error: 0.8043 - val_loss: 1.5271 - val_mean_squared_error: 1.5271\n",
            "Epoch 109/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7996 - mean_squared_error: 0.7996 - val_loss: 1.2275 - val_mean_squared_error: 1.2275\n",
            "Epoch 110/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8026 - mean_squared_error: 0.8026 - val_loss: 1.6381 - val_mean_squared_error: 1.6381\n",
            "Epoch 111/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8047 - mean_squared_error: 0.8047 - val_loss: 1.8809 - val_mean_squared_error: 1.8809\n",
            "Epoch 112/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8027 - mean_squared_error: 0.8027 - val_loss: 1.2060 - val_mean_squared_error: 1.2060\n",
            "Epoch 113/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7973 - mean_squared_error: 0.7973 - val_loss: 1.5090 - val_mean_squared_error: 1.5090\n",
            "Epoch 114/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7979 - mean_squared_error: 0.7979 - val_loss: 1.3571 - val_mean_squared_error: 1.3571\n",
            "Epoch 115/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8005 - mean_squared_error: 0.8005 - val_loss: 1.3197 - val_mean_squared_error: 1.3197\n",
            "Epoch 116/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8000 - mean_squared_error: 0.8000 - val_loss: 1.5800 - val_mean_squared_error: 1.5800\n",
            "Epoch 117/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 1.4896 - val_mean_squared_error: 1.4896\n",
            "Epoch 118/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8177 - mean_squared_error: 0.8177 - val_loss: 1.8178 - val_mean_squared_error: 1.8178\n",
            "Epoch 119/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8105 - mean_squared_error: 0.8105 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 120/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.8054 - mean_squared_error: 0.8054 - val_loss: 2.0222 - val_mean_squared_error: 2.0222\n",
            "Epoch 121/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8041 - mean_squared_error: 0.8041 - val_loss: 2.2190 - val_mean_squared_error: 2.2190\n",
            "Epoch 122/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8013 - mean_squared_error: 0.8013 - val_loss: 2.5137 - val_mean_squared_error: 2.5137\n",
            "Epoch 123/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.8005 - mean_squared_error: 0.8005 - val_loss: 2.9001 - val_mean_squared_error: 2.9001\n",
            "Epoch 124/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7978 - mean_squared_error: 0.7978 - val_loss: 2.8724 - val_mean_squared_error: 2.8724\n",
            "Epoch 125/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7974 - mean_squared_error: 0.7974 - val_loss: 2.3042 - val_mean_squared_error: 2.3042\n",
            "Epoch 126/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7944 - mean_squared_error: 0.7944 - val_loss: 2.5446 - val_mean_squared_error: 2.5446\n",
            "Epoch 127/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7961 - mean_squared_error: 0.7961 - val_loss: 2.4259 - val_mean_squared_error: 2.4259\n",
            "Epoch 128/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7975 - mean_squared_error: 0.7975 - val_loss: 2.0391 - val_mean_squared_error: 2.0391\n",
            "Epoch 129/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7956 - mean_squared_error: 0.7956 - val_loss: 1.9785 - val_mean_squared_error: 1.9785\n",
            "Epoch 130/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7940 - mean_squared_error: 0.7940 - val_loss: 2.5217 - val_mean_squared_error: 2.5217\n",
            "Epoch 131/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7927 - mean_squared_error: 0.7927 - val_loss: 2.0606 - val_mean_squared_error: 2.0606\n",
            "Epoch 132/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7915 - mean_squared_error: 0.7915 - val_loss: 2.3906 - val_mean_squared_error: 2.3906\n",
            "Epoch 133/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7927 - mean_squared_error: 0.7927 - val_loss: 2.3537 - val_mean_squared_error: 2.3537\n",
            "Epoch 134/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7926 - mean_squared_error: 0.7926 - val_loss: 3.0914 - val_mean_squared_error: 3.0914\n",
            "Epoch 135/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7925 - mean_squared_error: 0.7925 - val_loss: 1.8532 - val_mean_squared_error: 1.8532\n",
            "Epoch 136/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7868 - mean_squared_error: 0.7868 - val_loss: 2.1432 - val_mean_squared_error: 2.1432\n",
            "Epoch 137/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7896 - mean_squared_error: 0.7896 - val_loss: 1.2878 - val_mean_squared_error: 1.2878\n",
            "Epoch 138/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7880 - mean_squared_error: 0.7880 - val_loss: 2.1010 - val_mean_squared_error: 2.1010\n",
            "Epoch 139/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7860 - mean_squared_error: 0.7860 - val_loss: 1.8522 - val_mean_squared_error: 1.8522\n",
            "Epoch 140/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7877 - mean_squared_error: 0.7877 - val_loss: 2.1698 - val_mean_squared_error: 2.1698\n",
            "Epoch 141/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 1.9069 - val_mean_squared_error: 1.9069\n",
            "Epoch 142/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7871 - mean_squared_error: 0.7871 - val_loss: 2.6817 - val_mean_squared_error: 2.6817\n",
            "Epoch 143/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7883 - mean_squared_error: 0.7883 - val_loss: 2.4813 - val_mean_squared_error: 2.4813\n",
            "Epoch 144/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 2.0201 - val_mean_squared_error: 2.0201\n",
            "Epoch 145/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7829 - mean_squared_error: 0.7829 - val_loss: 1.9276 - val_mean_squared_error: 1.9276\n",
            "Epoch 146/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 2.1821 - val_mean_squared_error: 2.1821\n",
            "Epoch 147/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7854 - mean_squared_error: 0.7854 - val_loss: 1.4898 - val_mean_squared_error: 1.4898\n",
            "Epoch 148/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 1.8656 - val_mean_squared_error: 1.8656\n",
            "Epoch 149/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7835 - mean_squared_error: 0.7835 - val_loss: 2.4394 - val_mean_squared_error: 2.4394\n",
            "Epoch 150/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7817 - mean_squared_error: 0.7817 - val_loss: 1.7870 - val_mean_squared_error: 1.7870\n",
            "Epoch 151/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7830 - mean_squared_error: 0.7830 - val_loss: 1.6469 - val_mean_squared_error: 1.6469\n",
            "Epoch 152/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7810 - mean_squared_error: 0.7810 - val_loss: 1.6070 - val_mean_squared_error: 1.6070\n",
            "Epoch 153/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 1.1732 - val_mean_squared_error: 1.1732\n",
            "Epoch 154/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 1.9074 - val_mean_squared_error: 1.9074\n",
            "Epoch 155/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 1.7538 - val_mean_squared_error: 1.7538\n",
            "Epoch 156/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 1.9787 - val_mean_squared_error: 1.9787\n",
            "Epoch 157/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7834 - mean_squared_error: 0.7834 - val_loss: 1.9578 - val_mean_squared_error: 1.9578\n",
            "Epoch 158/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7816 - mean_squared_error: 0.7816 - val_loss: 1.5476 - val_mean_squared_error: 1.5476\n",
            "Epoch 159/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7831 - mean_squared_error: 0.7831 - val_loss: 1.4015 - val_mean_squared_error: 1.4015\n",
            "Epoch 160/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7836 - mean_squared_error: 0.7836 - val_loss: 2.6086 - val_mean_squared_error: 2.6086\n",
            "Epoch 161/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7835 - mean_squared_error: 0.7835 - val_loss: 1.7051 - val_mean_squared_error: 1.7051\n",
            "Epoch 162/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7811 - mean_squared_error: 0.7811 - val_loss: 1.7791 - val_mean_squared_error: 1.7791\n",
            "Epoch 163/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7823 - mean_squared_error: 0.7823 - val_loss: 2.0217 - val_mean_squared_error: 2.0217\n",
            "Epoch 164/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7818 - mean_squared_error: 0.7818 - val_loss: 3.2511 - val_mean_squared_error: 3.2511\n",
            "Epoch 165/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 1.6359 - val_mean_squared_error: 1.6359\n",
            "Epoch 166/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7816 - mean_squared_error: 0.7816 - val_loss: 1.4702 - val_mean_squared_error: 1.4702\n",
            "Epoch 167/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 1.9420 - val_mean_squared_error: 1.9420\n",
            "Epoch 168/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7854 - mean_squared_error: 0.7854 - val_loss: 1.9925 - val_mean_squared_error: 1.9925\n",
            "Epoch 169/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7830 - mean_squared_error: 0.7830 - val_loss: 1.5930 - val_mean_squared_error: 1.5930\n",
            "Epoch 170/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7868 - mean_squared_error: 0.7868 - val_loss: 1.6376 - val_mean_squared_error: 1.6376\n",
            "Epoch 171/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 1.5160 - val_mean_squared_error: 1.5160\n",
            "Epoch 172/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7828 - mean_squared_error: 0.7828 - val_loss: 2.0534 - val_mean_squared_error: 2.0534\n",
            "Epoch 173/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.7796 - mean_squared_error: 0.7796 - val_loss: 1.5905 - val_mean_squared_error: 1.5905\n",
            "Epoch 174/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7790 - mean_squared_error: 0.7790 - val_loss: 1.6245 - val_mean_squared_error: 1.6245\n",
            "Epoch 175/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7825 - mean_squared_error: 0.7825 - val_loss: 1.4552 - val_mean_squared_error: 1.4552\n",
            "Epoch 176/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7788 - mean_squared_error: 0.7788 - val_loss: 1.5380 - val_mean_squared_error: 1.5380\n",
            "Epoch 177/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7817 - mean_squared_error: 0.7817 - val_loss: 2.1076 - val_mean_squared_error: 2.1076\n",
            "Epoch 178/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7787 - mean_squared_error: 0.7787 - val_loss: 1.9466 - val_mean_squared_error: 1.9466\n",
            "Epoch 179/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7788 - mean_squared_error: 0.7788 - val_loss: 1.5988 - val_mean_squared_error: 1.5988\n",
            "Epoch 180/2000\n",
            "3577/3577 [==============================] - 21s 6ms/step - loss: 0.7777 - mean_squared_error: 0.7777 - val_loss: 1.4719 - val_mean_squared_error: 1.4719\n",
            "Epoch 181/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7760 - mean_squared_error: 0.7760 - val_loss: 2.2288 - val_mean_squared_error: 2.2288\n",
            "Epoch 182/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7789 - mean_squared_error: 0.7789 - val_loss: 1.5584 - val_mean_squared_error: 1.5584\n",
            "Epoch 183/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7771 - mean_squared_error: 0.7771 - val_loss: 2.1942 - val_mean_squared_error: 2.1942\n",
            "Epoch 184/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7772 - mean_squared_error: 0.7772 - val_loss: 1.4558 - val_mean_squared_error: 1.4558\n",
            "Epoch 185/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7762 - mean_squared_error: 0.7762 - val_loss: 1.3477 - val_mean_squared_error: 1.3477\n",
            "Epoch 186/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7769 - mean_squared_error: 0.7769 - val_loss: 1.3453 - val_mean_squared_error: 1.3453\n",
            "Epoch 187/2000\n",
            "3577/3577 [==============================] - 20s 6ms/step - loss: 0.7763 - mean_squared_error: 0.7763 - val_loss: 1.9967 - val_mean_squared_error: 1.9967\n",
            "Epoch 188/2000\n",
            "3044/3577 [========================>.....] - ETA: 2s - loss: 0.7792 - mean_squared_error: 0.7792"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "\n",
        "generator_multiply = 100 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "#get data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/multivariate_long_sequences_WINDOW-Daily-DIRECT-VAR.npy')\n",
        "#window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-Daily-May2024.npy')\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "#----------------check window distribution - we see upto 20 has very high fequency. so we remove that and take the rest to generate more samples, to create overall uniform distribution...\n",
        "#First we test window 20 to 30 and see if this method can fit the model well\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(window_label, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "\n",
        "##---------------------------IGNORE THIS IF NOT GENERATING FRESH VAE DATASET--------------------------------------------------------------------------------------------\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-round4-latent5-dim256.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-round4-latent5-dim256.model')\n",
        "\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/generated_large_labelled_subsquence_data',results1)\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_ELECTRICITY/generated_large_labelled_subsquence_data.npy')\n",
        "\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "#--------------------------------------IF REQUIRED REMOVE outlier....however we are not doing this now.--------------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x, y = x[mask, :], y[mask]\n",
        "\n",
        "\n",
        "###############Scale the target and then split the data into train test----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#Looking at the dist, we remove al y less than 20\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "y_transformed = transformer.fit_transform(y.reshape(-1,1)).flatten()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_transformed, test_size = 0.1, random_state = 42)\n",
        "\n",
        "#--------------------------------------------------------------------------------------CONSTRUCT, COMPILE, AND TRAIN THE MODEL------------------------------------------------------------------------------------------------------------------\n",
        "#------------MLP------------------------------------------------------\n",
        "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units = 64))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 32))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "#--------------------------------------------------------------LSTM--------------------------\n",
        "\n",
        "a =  x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "b =  x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(a.shape[1],a.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 16))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "#model.add(Dense(units = 4))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 1, activation = 'linear'))\n",
        "model.summary()\n",
        "x_train = a\n",
        "x_test = b\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.0003,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.0001, monitor='loss', mode='min', restore_best_weights=True)\n",
        "n_epochs = 2000\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=32,\n",
        "                   validation_split=0.1)\n",
        "                 #callbacks=[es])\n",
        "\n",
        "\n",
        "#-------------------------LOSS-------------------------------------\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "#-----------------------------------------------------TRAIN EVALUATION----------------------------------------------------------------\n",
        "y_train_pred_raw = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred_raw)\n",
        "y_train_true = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "\n",
        "score_train= r2_score(y_train_true,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "plt.plot(y_train_true[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------------------------TEST EVALUATION----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[100:200], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[100:200], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------SAVE MODEL AND RESULTS-----------------------------------------------------------------\n",
        "\n",
        "model.save(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily_V7-July24.keras')\n",
        "# It can be used to reconstruct the model identically.\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------Retrieve Model-----------------------------------------------------------------\n",
        "\n",
        "reconstructed_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily_V6A.keras')\n",
        "\n",
        "#-----------------------------------------------------Test with retrieved model-----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = reconstructed_model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[1000:1050], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[1000:1050], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/predicted_window_V4.csv',y_test_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/derived_window_label_V4.csv',y_test_true)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/test_data_V4.csv',x_test)\n",
        "\n",
        "#--------------------------NOW RUN THE WHOLE DATASET-----------------------------------------------------------------\n",
        "\n",
        "x_whole = x\n",
        "y_whole = y_transformed\n",
        "\n",
        "y_pred_raw_w = reconstructed_model.predict(x_whole)\n",
        "y_whole_pred = transformer.inverse_transform(y_pred_raw_w)\n",
        "y_whole_true = transformer.inverse_transform(y_whole.reshape(-1,1)).flatten()\n",
        "\n",
        "score= r2_score(y_whole_true,y_whole_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_whole_true[5000:10000], color = 'red', label = 'Real data')\n",
        "plt.plot(y_whole_pred[5000:10000], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/predicted_COSW_V4.csv',y_whole_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/Calculated_label_COSW_V4.csv',y_whole_true)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/whole_data_V4.csv',x_whole)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/PAPER_GDRNet_ELECTRICITY_Step3.ipynb",
      "authorship_tag": "ABX9TyPE76UL5BaMldi1GfCJt2hk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}