{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "3684cdf5-c93e-407e-acec-e28308ad3f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded MLP model from /content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\n",
            "Loaded saved transformer\n",
            "AdaptiveWindowAgent adaptive_window_agent initialized\n",
            "Model loaded: True\n",
            "Transformer fitted: True\n",
            "Loading your actual dataset...\n",
            "Loaded Long_train shape: (3627, 50, 12)\n",
            "Testing with last 10 sequences: (10, 50, 12)\n",
            "\n",
            "Starting real-time VAR testing...\n",
            "======================================================================\n",
            "\n",
            "Sample 1: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.07623464]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.9237654209136963\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   1: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 2: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.0308454]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.9691545963287354\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   2: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 3: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.16598861]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.834011435508728\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   3: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 4: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.37979627]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 2.379796266555786\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   4: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 5: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.8375915]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.162408471107483\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   5: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 6: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.44911033]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.5508897304534912\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   6: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 7: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.02855273]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 1.971447229385376\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   7: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 8: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.9754352]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 2.975435256958008\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   8: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "Sample 9: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.49484247]]\n",
            "VAR ground truth: 4\n",
            "After inverse transform: 1.505157470703125\n",
            "Final predicted window (after clamping): 5\n",
            "Sample   9: MLP= 5, VAR_GT=4, Error= 1, Accuracy= 75.0%, R2= 0.000\n",
            "\n",
            "Sample 10: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[2.1740463]]\n",
            "VAR ground truth: 2\n",
            "After inverse transform: 4.174046516418457\n",
            "Final predicted window (after clamping): 5\n",
            "Sample  10: MLP= 5, VAR_GT=2, Error= 3, Accuracy=-50.0%, R2= 0.000\n",
            "\n",
            "======================================================================\n",
            "FINAL PERFORMANCE SUMMARY\n",
            "======================================================================\n",
            "Total predictions: 10\n",
            "Average accuracy: 0.0750\n",
            "Average R2 score: 0.0000\n",
            "Average MSE: 0.0000\n",
            "Drift events: 0\n",
            "Retraining events: 0\n",
            "Transformer fitted: True\n",
            "\n",
            "Test results saved to: real_data_test_results.json\n"
          ]
        }
      ],
      "source": [
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque, defaultdict, Counter\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Agent A: Adaptive Window Management with Enhanced MLP\n",
        "\n",
        "    Capabilities:\n",
        "    1. Invoke new data and score using trained MLP\n",
        "    2. Calculate actual performance using VAR forecast (real-time only)\n",
        "    3. Track accuracy and performance statistics\n",
        "    4. Monitor for drift in prediction performance\n",
        "    5. Communicate with sensor agents to verify drift\n",
        "    6. Retrain MLP when drift is confirmed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str = \"adaptive_window_agent\",\n",
        "                 model_path: str = None,\n",
        "                 checkpoint_path: str = None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "        self.checkpoint_path = checkpoint_path or \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/ckp2.weights.h5\"\n",
        "\n",
        "        # Core MLP components\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Performance tracking\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.accuracy_history = deque(maxlen=200)\n",
        "        self.r2_history = deque(maxlen=200)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "\n",
        "        # Drift detection parameters\n",
        "        self.drift_detection_window = 50\n",
        "        self.drift_threshold_r2 = 0.1\n",
        "        self.drift_threshold_mse = 0.2\n",
        "        self.consecutive_poor_predictions = 0\n",
        "        self.drift_confirmed = False\n",
        "\n",
        "        # Statistics storage\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_r2': 0.0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        # Retraining data storage\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        # Sensor agents for drift confirmation\n",
        "        self.sensor_agents = {}\n",
        "\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "        print(f\"Model loaded: {self.is_model_loaded}\")\n",
        "        print(f\"Transformer fitted: {self.transformer_fitted}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load trained MLP model and recreate transformer using original training data\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                print(f\"Loaded MLP model from {self.model_path}\")\n",
        "                self.is_model_loaded = True\n",
        "\n",
        "                # Try to load saved transformer first\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                    print(\"Loaded saved transformer\")\n",
        "                else:\n",
        "                    # Recreate transformer from original training data\n",
        "                    print(\"No saved transformer found, recreating from original training data...\")\n",
        "\n",
        "                    try:\n",
        "                        # Load your original y training data\n",
        "                        y_original = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "\n",
        "                        # Fit transformer on original training data (same as your training code)\n",
        "                        self.transformer = StandardScaler()\n",
        "                        self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                        self.transformer_fitted = True\n",
        "\n",
        "                        # Save it for future use\n",
        "                        with open(transformer_path, 'wb') as f:\n",
        "                            pickle.dump(self.transformer, f)\n",
        "\n",
        "                        print(f\"Fitted transformer on {len(y_original)} original training samples and saved\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not load original training data: {e}\")\n",
        "                        self.transformer = StandardScaler()\n",
        "                        self.transformer_fitted = False\n",
        "\n",
        "            else:\n",
        "                print(f\"Model file not found at {self.model_path}\")\n",
        "                self.is_model_loaded = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            self.is_model_loaded = False\n",
        "\n",
        "    def calculate_var_ground_truth(self, sequence_3d: np.ndarray, n_future: int = 1) -> int:\n",
        "        \"\"\"\n",
        "        Calculate ground truth window size using your EXACT VAR analysis logic\n",
        "        \"\"\"\n",
        "        # Your exact VAR analysis logic\n",
        "        rmse_list = []\n",
        "        K = sequence_3d.shape[0]  # K is the number of timesteps (50 in your case)\n",
        "\n",
        "        for k in range(2, round(K)):\n",
        "            cur_seq = sequence_3d\n",
        "            df = pd.DataFrame(cur_seq, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12'])\n",
        "            df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "            model = VAR(df_train)\n",
        "            try:\n",
        "                model_fitted1 = model.fit(k)\n",
        "                forecast_input1 = df_train.values[-k:]\n",
        "                fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                rmse_list.append(mse)\n",
        "            except:\n",
        "                rmse_list.append(99999)\n",
        "\n",
        "        if rmse_list:\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            return min_sw\n",
        "        else:\n",
        "            # Return the most frequently occurring window from training data\n",
        "            return self._get_most_frequent_window()\n",
        "\n",
        "    def _get_most_frequent_window(self) -> int:\n",
        "        \"\"\"Get the most frequently occurring window size from original training data\"\"\"\n",
        "        try:\n",
        "            # Load your original training ground truth data\n",
        "            y_training_data = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "\n",
        "            # Find most frequent window size from training data\n",
        "            window_counts = Counter(y_training_data.astype(int))\n",
        "            most_frequent_window = window_counts.most_common(1)[0][0]\n",
        "\n",
        "            print(f\"Using most frequent window from training data: {most_frequent_window}\")\n",
        "            return most_frequent_window\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load training data for fallback: {e}\")\n",
        "            return 20  # Only if training data unavailable\n",
        "\n",
        "    def predict_window_size(self, feature_vector: np.ndarray, sequence_3d: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Predict window size using MLP and calculate VAR ground truth in real-time\n",
        "        \"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\n",
        "                'predicted_window': 20,\n",
        "                'confidence': 0.0,\n",
        "                'error': \"Model not loaded\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Ensure feature vector is 2D\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            print(f\"Input feature vector shape: {feature_vector.shape}\")\n",
        "\n",
        "            # 1. INVOKE NEW DATA AND SCORE USING MLP\n",
        "            # Handle fixed batch size requirement - pad to batch size 32\n",
        "            try:\n",
        "                prediction_raw = self.model.predict(feature_vector, verbose=0)\n",
        "            except Exception as e:\n",
        "                if \"input shape\" in str(e) and \"32\" in str(e):\n",
        "                    print(\"Padding input to batch size 32\")\n",
        "                    # Model expects batch size 32, pad the input\n",
        "                    feature_batch = np.repeat(feature_vector, 32, axis=0)  # Shape: (32, 600)\n",
        "                    prediction_batch = self.model.predict(feature_batch, verbose=0)\n",
        "                    prediction_raw = prediction_batch[0:1]  # Take first prediction only\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "            print(f\"Raw prediction: {prediction_raw}\")\n",
        "\n",
        "            # 2. CALCULATE VAR-BASED GROUND TRUTH from sequence\n",
        "            var_ground_truth = self.calculate_var_ground_truth(sequence_3d)\n",
        "            print(f\"VAR ground truth: {var_ground_truth}\")\n",
        "\n",
        "            # 3. TRANSFORM PREDICTION BACK TO ORIGINAL SCALE\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = self.transformer.inverse_transform(prediction_raw)[0, 0]\n",
        "                print(f\"After inverse transform: {predicted_window}\")\n",
        "            else:\n",
        "                # Use raw prediction if transformer not fitted yet\n",
        "                predicted_window = prediction_raw[0, 0]\n",
        "                logger.warning(\"Transformer not fitted yet, using raw prediction\")\n",
        "                print(f\"Using raw prediction: {predicted_window}\")\n",
        "\n",
        "            predicted_window = max(1, min(50, int(predicted_window)))\n",
        "            print(f\"Final predicted window (after clamping): {predicted_window}\")\n",
        "\n",
        "            # Create prediction record\n",
        "            prediction_record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'feature_vector': feature_vector.flatten(),\n",
        "                'raw_prediction': prediction_raw[0, 0],\n",
        "                'var_ground_truth': var_ground_truth,\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "\n",
        "            # 4. CALCULATE ACTUAL PERFORMANCE AGAINST VAR GROUND TRUTH\n",
        "            absolute_error = abs(predicted_window - var_ground_truth)\n",
        "            relative_error = absolute_error / max(var_ground_truth, 1)\n",
        "            accuracy = max(0, 1 - relative_error)\n",
        "\n",
        "            self.accuracy_history.append(accuracy)\n",
        "\n",
        "            # Calculate metrics for recent predictions\n",
        "            if len(self.prediction_history) >= 10:\n",
        "                recent_predictions = [p['predicted_window'] for p in list(self.prediction_history)[-10:]\n",
        "                                    if p['var_ground_truth'] is not None]\n",
        "                recent_ground_truths = [p['var_ground_truth'] for p in list(self.prediction_history)[-10:]\n",
        "                                      if p['var_ground_truth'] is not None]\n",
        "\n",
        "                if len(recent_predictions) >= 5:\n",
        "                    r2 = r2_score(recent_ground_truths, recent_predictions)\n",
        "                    mse = mean_squared_error(recent_ground_truths, recent_predictions)\n",
        "                    mae = mean_absolute_error(recent_ground_truths, recent_predictions)\n",
        "\n",
        "                    self.r2_history.append(r2)\n",
        "                    self.mse_history.append(mse)\n",
        "\n",
        "                    self.performance_stats.update({\n",
        "                        'total_predictions': self.performance_stats['total_predictions'] + 1,\n",
        "                        'avg_r2': np.mean(self.r2_history),\n",
        "                        'avg_mse': np.mean(self.mse_history),\n",
        "                        'avg_mae': mae\n",
        "                    })\n",
        "\n",
        "                    prediction_record.update({\n",
        "                        'absolute_error': absolute_error,\n",
        "                        'relative_error': relative_error,\n",
        "                        'accuracy': accuracy,\n",
        "                        'recent_r2': r2,\n",
        "                        'recent_mse': mse,\n",
        "                        'recent_mae': mae\n",
        "                    })\n",
        "\n",
        "                    # 5. CHECK FOR DRIFT\n",
        "                    drift_detected = self._check_for_drift()\n",
        "                    prediction_record['drift_detected'] = drift_detected\n",
        "\n",
        "                    if drift_detected:\n",
        "                        prediction_record['drift_action'] = self._handle_drift_detection(feature_vector, var_ground_truth)\n",
        "\n",
        "            # Store prediction\n",
        "            self.prediction_history.append(prediction_record)\n",
        "\n",
        "            # Add to retraining buffer\n",
        "            self.retraining_data['x_buffer'].append(feature_vector.flatten())\n",
        "            self.retraining_data['y_buffer'].append(var_ground_truth)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'var_ground_truth': var_ground_truth,\n",
        "                'confidence': self._calculate_confidence(prediction_record),\n",
        "                'performance_stats': self.get_recent_performance(),\n",
        "                'drift_detected': prediction_record.get('drift_detected', False),\n",
        "                'prediction_id': len(self.prediction_history),\n",
        "                'transformer_status': 'fitted' if self.transformer_fitted else 'not_fitted'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction error: {e}\")\n",
        "            return {\n",
        "                'predicted_window': 20,\n",
        "                'confidence': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _check_for_drift(self) -> bool:\n",
        "        \"\"\"Monitor and identify drift in prediction performance\"\"\"\n",
        "        if len(self.r2_history) < self.drift_detection_window:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Get recent and historical performance\n",
        "            recent_r2 = np.mean(list(self.r2_history)[-20:])\n",
        "            historical_r2 = np.mean(list(self.r2_history)[-self.drift_detection_window:-20])\n",
        "\n",
        "            recent_mse = np.mean(list(self.mse_history)[-20:])\n",
        "            historical_mse = np.mean(list(self.mse_history)[-self.drift_detection_window:-20])\n",
        "\n",
        "            # Check for significant performance degradation\n",
        "            r2_drop = historical_r2 - recent_r2\n",
        "            mse_increase = recent_mse / max(historical_mse, 0.001) - 1\n",
        "\n",
        "            # Drift conditions\n",
        "            r2_drift = r2_drop > self.drift_threshold_r2\n",
        "            mse_drift = mse_increase > self.drift_threshold_mse\n",
        "\n",
        "            # Track consecutive poor predictions\n",
        "            recent_accuracy = np.mean(list(self.accuracy_history)[-10:]) if len(self.accuracy_history) >= 10 else 1.0\n",
        "            if recent_accuracy < 0.7:\n",
        "                self.consecutive_poor_predictions += 1\n",
        "            else:\n",
        "                self.consecutive_poor_predictions = 0\n",
        "\n",
        "            consecutive_drift = self.consecutive_poor_predictions > 10\n",
        "\n",
        "            # Drift detected if multiple conditions met\n",
        "            drift_detected = (r2_drift and mse_drift) or consecutive_drift\n",
        "\n",
        "            if drift_detected:\n",
        "                logger.warning(f\"Drift detected: R2 drop={r2_drop:.3f}, MSE increase={mse_increase:.3f}, \"\n",
        "                             f\"Consecutive poor predictions={self.consecutive_poor_predictions}\")\n",
        "                self.performance_stats['drift_events'] += 1\n",
        "\n",
        "            return drift_detected\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Drift detection error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _handle_drift_detection(self, current_features: np.ndarray, var_ground_truth: int) -> str:\n",
        "        \"\"\"Check with sensor agents if really drifting\"\"\"\n",
        "        if self.drift_confirmed:\n",
        "            return \"Already handling drift\"\n",
        "\n",
        "        # Query sensor agents for their drift status\n",
        "        sensor_drift_confirmations = self._query_sensor_agents_for_drift()\n",
        "\n",
        "        # If majority of sensors also detect drift, confirm and retrain\n",
        "        if sensor_drift_confirmations >= len(self.sensor_agents) * 0.6:\n",
        "            self.drift_confirmed = True\n",
        "            logger.info(\"Drift confirmed by sensor agents. Initiating retraining...\")\n",
        "\n",
        "            # Retrain MLP\n",
        "            retrain_success = self._retrain_model()\n",
        "\n",
        "            if retrain_success:\n",
        "                self.drift_confirmed = False\n",
        "                self.consecutive_poor_predictions = 0\n",
        "                self.performance_stats['retraining_events'] += 1\n",
        "                self.performance_stats['last_retrain_time'] = dt.datetime.now()\n",
        "                return \"Retraining completed successfully\"\n",
        "            else:\n",
        "                return \"Retraining failed\"\n",
        "        else:\n",
        "            return f\"Drift suspected but not confirmed by sensors ({sensor_drift_confirmations}/{len(self.sensor_agents)})\"\n",
        "\n",
        "    def _query_sensor_agents_for_drift(self) -> int:\n",
        "        \"\"\"Query sensor agents to confirm drift\"\"\"\n",
        "        confirmations = 0\n",
        "\n",
        "        for sensor_id, sensor_agent in self.sensor_agents.items():\n",
        "            try:\n",
        "                # This would be actual message passing in full implementation\n",
        "                sensor_drift = np.random.random() > 0.7  # Simulate sensor response\n",
        "                if sensor_drift:\n",
        "                    confirmations += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error querying sensor {sensor_id}: {e}\")\n",
        "\n",
        "        return confirmations\n",
        "\n",
        "    def _retrain_model(self) -> bool:\n",
        "        \"\"\"Retrain MLP and reinstate new model\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Starting MLP retraining...\")\n",
        "\n",
        "            if len(self.retraining_data['x_buffer']) < 100:\n",
        "                logger.warning(\"Insufficient data for retraining\")\n",
        "                return False\n",
        "\n",
        "            # Prepare retraining data\n",
        "            X_retrain = np.array(list(self.retraining_data['x_buffer']))\n",
        "            y_raw = np.array(list(self.retraining_data['y_buffer']))\n",
        "\n",
        "            # Transform y data for training if transformer is fitted\n",
        "            if self.transformer_fitted:\n",
        "                y_retrain = self.transformer.transform(y_raw.reshape(-1, 1)).flatten()\n",
        "            else:\n",
        "                y_retrain = y_raw\n",
        "\n",
        "            # Create new model with same architecture\n",
        "            new_model = Sequential()\n",
        "            new_model.add(Dense(64, activation='relu', input_shape=(X_retrain.shape[1],)))\n",
        "            new_model.add(Dense(32, activation='relu'))\n",
        "            new_model.add(Dense(16, activation='relu'))\n",
        "            new_model.add(Dense(8, activation='relu'))\n",
        "            new_model.add(Dense(1))\n",
        "\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=0.0003, clipnorm=1)\n",
        "            new_model.compile(loss='mean_squared_error', optimizer=optimizer,\n",
        "                            metrics=['mean_squared_error'])\n",
        "\n",
        "            es = keras.callbacks.EarlyStopping(\n",
        "                patience=10, verbose=0, min_delta=0.0001,\n",
        "                monitor='loss', mode='min', restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            # Train the new model\n",
        "            history = new_model.fit(\n",
        "                X_retrain, y_retrain,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[es],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate new model performance\n",
        "            val_loss = min(history.history['val_loss'])\n",
        "\n",
        "            # Only replace model if new one is better\n",
        "            current_recent_mse = np.mean(list(self.mse_history)[-10:]) if self.mse_history else float('inf')\n",
        "            if val_loss < current_recent_mse * 1.1:\n",
        "                # Replace the model\n",
        "                self.model = new_model\n",
        "\n",
        "                # Save the retrained model\n",
        "                retrain_path = self.model_path.replace('.keras', '_retrained.keras')\n",
        "                self.model.save(retrain_path)\n",
        "\n",
        "                # Clear history to start fresh\n",
        "                self.r2_history.clear()\n",
        "                self.mse_history.clear()\n",
        "                self.accuracy_history.clear()\n",
        "\n",
        "                logger.info(f\"Model successfully retrained. New validation loss: {val_loss:.4f}\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(f\"New model performance worse ({val_loss:.4f} vs {current_recent_mse:.4f})\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Retraining failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _calculate_confidence(self, prediction_record: Dict) -> float:\n",
        "        \"\"\"Calculate confidence based on recent performance\"\"\"\n",
        "        if len(self.accuracy_history) < 10:\n",
        "            return 0.5\n",
        "\n",
        "        recent_accuracy = np.mean(list(self.accuracy_history)[-10:])\n",
        "        recent_r2 = self.performance_stats.get('avg_r2', 0.0)\n",
        "\n",
        "        confidence = (recent_accuracy + max(0, recent_r2)) / 2\n",
        "        return min(1.0, max(0.1, confidence))\n",
        "\n",
        "    def get_recent_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get recent performance statistics\"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'recent_accuracy': np.mean(self.accuracy_history) if self.accuracy_history else 0.0,\n",
        "            'recent_r2': np.mean(self.r2_history) if self.r2_history else 0.0,\n",
        "            'recent_mse': np.mean(self.mse_history) if self.mse_history else 0.0,\n",
        "            'total_predictions': len(self.prediction_history),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'last_retrain': self.performance_stats['last_retrain_time'],\n",
        "            'consecutive_poor': self.consecutive_poor_predictions,\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "    def connect_sensor_agents(self, sensor_agents: Dict):\n",
        "        \"\"\"Connect to sensor agents for drift confirmation\"\"\"\n",
        "        self.sensor_agents = sensor_agents\n",
        "        logger.info(f\"Connected to {len(sensor_agents)} sensor agents\")\n",
        "\n",
        "    def get_performance_plot_data(self) -> Dict[str, List]:\n",
        "        \"\"\"Get data for performance visualization\"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return {}\n",
        "\n",
        "        recent_records = [p for p in self.prediction_history if p.get('var_ground_truth') is not None]\n",
        "\n",
        "        return {\n",
        "            'timestamps': [r['timestamp'] for r in recent_records],\n",
        "            'predicted': [r['predicted_window'] for r in recent_records],\n",
        "            'actual': [r['var_ground_truth'] for r in recent_records],\n",
        "            'accuracy': [r.get('accuracy', 0) for r in recent_records],\n",
        "            'r2_scores': list(self.r2_history),\n",
        "            'mse_scores': list(self.mse_history)\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save current performance state\"\"\"\n",
        "        state = {\n",
        "            'performance_stats': self.performance_stats.copy(),\n",
        "            'prediction_history': list(self.prediction_history)[-100:],\n",
        "            'accuracy_history': list(self.accuracy_history),\n",
        "            'r2_history': list(self.r2_history),\n",
        "            'mse_history': list(self.mse_history),\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Convert datetime objects to strings for JSON serialization\n",
        "            for record in state['prediction_history']:\n",
        "                if 'timestamp' in record:\n",
        "                    # Check if timestamp is already a string\n",
        "                    if hasattr(record['timestamp'], 'isoformat'):\n",
        "                        record['timestamp'] = record['timestamp'].isoformat()\n",
        "                # Convert numpy arrays to lists for JSON serialization\n",
        "                if 'feature_vector' in record and hasattr(record['feature_vector'], 'tolist'):\n",
        "                    record['feature_vector'] = record['feature_vector'].tolist()\n",
        "\n",
        "            # Handle last_retrain_time\n",
        "            if state['performance_stats']['last_retrain_time']:\n",
        "                if hasattr(state['performance_stats']['last_retrain_time'], 'isoformat'):\n",
        "                    state['performance_stats']['last_retrain_time'] = state['performance_stats']['last_retrain_time'].isoformat()\n",
        "\n",
        "            # Save to file\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, default=str, indent=2)\n",
        "\n",
        "            logger.info(f\"Performance state saved to {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save performance state: {e}\")\n",
        "            # Try simpler save without complex objects\n",
        "            simple_state = {\n",
        "                'total_predictions': len(self.prediction_history),\n",
        "                'avg_accuracy': np.mean(self.accuracy_history) if self.accuracy_history else 0.0,\n",
        "                'avg_r2': np.mean(self.r2_history) if self.r2_history else 0.0,\n",
        "                'avg_mse': np.mean(self.mse_history) if self.mse_history else 0.0,\n",
        "                'drift_events': self.performance_stats['drift_events'],\n",
        "                'retraining_events': self.performance_stats['retraining_events']\n",
        "            }\n",
        "\n",
        "            with open(filepath.replace('.json', '_simple.json'), 'w') as f:\n",
        "                json.dump(simple_state, f, indent=2)\n",
        "            print(f\"Saved simplified performance state to {filepath.replace('.json', '_simple.json')}\")\n",
        "\n",
        "\n",
        "# Test with YOUR actual data - Real-time VAR calculation only\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the agent with your actual model\n",
        "    agent = AdaptiveWindowAgent(\n",
        "        model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "    )\n",
        "\n",
        "    print(\"Loading your actual dataset...\")\n",
        "\n",
        "    # Load your actual saved dataset\n",
        "    Long_train = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "    print(f\"Loaded Long_train shape: {Long_train.shape}\")\n",
        "\n",
        "    # Take last 10 entries as test module for debugging\n",
        "    test_sequences = Long_train[-10:]\n",
        "    print(f\"Testing with last 10 sequences: {test_sequences.shape}\")\n",
        "\n",
        "    print(\"\\nStarting real-time VAR testing...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for i in range(len(test_sequences)):\n",
        "        sequence_3d = test_sequences[i]  # Shape: (50, 12)\n",
        "\n",
        "        # Simply flatten the sequence to get 600 features (50 * 12 = 600)\n",
        "        features = sequence_3d.flatten()  # This gives you exactly 600 features\n",
        "\n",
        "        print(f\"\\nSample {i+1}: Using flattened sequence of size {len(features)}\")\n",
        "\n",
        "        # Test the agent with real-time VAR calculation\n",
        "        result = agent.predict_window_size(features, sequence_3d=sequence_3d)\n",
        "\n",
        "        # Handle potential missing keys safely\n",
        "        mlp_pred = result.get('predicted_window', 0)\n",
        "        var_gt = result.get('var_ground_truth', None)\n",
        "        error_msg = result.get('error', None)\n",
        "\n",
        "        if error_msg:\n",
        "            print(f\"Sample {i+1}: ERROR - {error_msg}\")\n",
        "            continue\n",
        "\n",
        "        # Print results\n",
        "        performance = agent.get_recent_performance()\n",
        "        error = abs(mlp_pred - var_gt) if var_gt else 0\n",
        "        accuracy_pct = ((1 - error/max(var_gt, 1)) * 100) if var_gt else 0\n",
        "\n",
        "        print(f\"Sample {i+1:3d}: MLP={mlp_pred:2d}, VAR_GT={var_gt if var_gt else 'N/A'}, \"\n",
        "              f\"Error={error:2d}, Accuracy={accuracy_pct:5.1f}%, \"\n",
        "              f\"R2={performance.get('recent_r2', 0):6.3f}\")\n",
        "\n",
        "        # Check for drift detection\n",
        "        if result.get('drift_detected', False):\n",
        "            print(f\"*** DRIFT DETECTED at sample {i+1} ***\")\n",
        "            drift_action = result.get('drift_action')\n",
        "            if drift_action:\n",
        "                print(f\"Drift action: {drift_action}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    final_performance = agent.get_recent_performance()\n",
        "\n",
        "    print(f\"Total predictions: {final_performance.get('total_predictions', 0)}\")\n",
        "    print(f\"Average accuracy: {final_performance.get('recent_accuracy', 0):.4f}\")\n",
        "    print(f\"Average R2 score: {final_performance.get('recent_r2', 0):.4f}\")\n",
        "    print(f\"Average MSE: {final_performance.get('recent_mse', 0):.4f}\")\n",
        "    print(f\"Drift events: {final_performance.get('drift_events', 0)}\")\n",
        "    print(f\"Retraining events: {agent.performance_stats['retraining_events']}\")\n",
        "    print(f\"Transformer fitted: {final_performance.get('transformer_fitted', False)}\")\n",
        "\n",
        "    # Save test results\n",
        "    agent.save_performance_state(\"real_data_test_results.json\")\n",
        "    print(f\"\\nTest results saved to: real_data_test_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyNa9fsJYtrkoYpWix/7q+8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}