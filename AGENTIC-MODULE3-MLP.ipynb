{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "9580252f-e2e8-48ae-fdee-236dd617ff1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Ultra-Simple MLP with Line Charts (Huber + Tail Weights)\n",
            "üìÅ Ultra-Simple MLP directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/ultra_simple_mlp/\n",
            "============================================================\n",
            "‚ö° ULTRA-SIMPLE MLP PIPELINE (Huber + Tail Weights)\n",
            "============================================================\n",
            "üìä Loading data...\n",
            "‚úÖ Data loaded: X=(350000, 650), y=(350000,)\n",
            "üîß Adding key interaction to 650 features...\n",
            "   ‚úÖ Added feat_7 √ó feat_8 interaction\n",
            "   Features: 650 ‚Üí 651\n",
            "üéØ Using ALL original features: 651 features...\n",
            "   Using all 651 original features\n",
            "   Feature range: 0 to 650\n",
            "üìä Splitting data...\n",
            "‚öñÔ∏è Building tail sample weights...\n",
            "‚öñÔ∏è Scaling data...\n"
          ]
        }
      ],
      "source": [
        "# Ultra-Simple MLP - Minimal Architecture\n",
        "# Clean implementation with line chart for predictions vs actual\n",
        "# Now with Huber loss + tail sample weights for [2‚Äì4] and [20‚Äì24]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class UltraSimpleMLP:\n",
        "    \"\"\"\n",
        "    Ultra-simple MLP with minimal architecture, Huber loss, and tail weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.y_scaler = StandardScaler()\n",
        "        self.x_scaler = StandardScaler()\n",
        "        self.model = None\n",
        "\n",
        "        # Create ultra-simple model directory\n",
        "        self.ultra_dir = f\"{output_dir}ultra_simple_mlp/\"\n",
        "        os.makedirs(self.ultra_dir, exist_ok=True)\n",
        "        print(f\"üìÅ Ultra-Simple MLP directory: {self.ultra_dir}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Data I/O\n",
        "    # -------------------------\n",
        "    def load_data(self, data_filename, windows_filename):\n",
        "        \"\"\"Load data\"\"\"\n",
        "        print(\"üìä Loading data...\")\n",
        "\n",
        "        data_path = os.path.join(self.output_dir, data_filename)\n",
        "        windows_path = os.path.join(self.output_dir, windows_filename)\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        y = np.load(windows_path)\n",
        "\n",
        "        print(f\"‚úÖ Data loaded: X={x.shape}, y={y.shape}\")\n",
        "        return x, y\n",
        "\n",
        "    # -------------------------\n",
        "    # Tail Weights\n",
        "    # -------------------------\n",
        "    def make_tail_sample_weights(self, y_unscaled, low=(2, 4), high=(20, 24),\n",
        "                                 tail_w=3.0, mid_w=1.0, dtype=np.float32):\n",
        "        \"\"\"\n",
        "        Build per-sample weights to up-weight extremes (in ORIGINAL units).\n",
        "        \"\"\"\n",
        "        y = y_unscaled.reshape(-1)\n",
        "        w = np.full_like(y, fill_value=mid_w, dtype=dtype)\n",
        "        low_mask  = (y >= low[0])  & (y <= low[1])\n",
        "        high_mask = (y >= high[0]) & (y <= high[1])\n",
        "        w[low_mask | high_mask] = tail_w\n",
        "        return w\n",
        "\n",
        "    # -------------------------\n",
        "    # Features\n",
        "    # -------------------------\n",
        "    def add_key_interaction(self, x):\n",
        "        \"\"\"\n",
        "        Add the best interaction feature (feat_7 √ó feat_8) that was found in original analysis\n",
        "        \"\"\"\n",
        "        print(f\"üîß Adding key interaction to {x.shape[1]} features...\")\n",
        "\n",
        "        if x.shape[1] > 8:\n",
        "            best_interaction = (x[:, 7] * x[:, 8]).reshape(-1, 1)\n",
        "            x_enhanced = np.hstack([x, best_interaction])\n",
        "            print(f\"   ‚úÖ Added feat_7 √ó feat_8 interaction\")\n",
        "            print(f\"   Features: {x.shape[1]} ‚Üí {x_enhanced.shape[1]}\")\n",
        "            return x_enhanced\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Not enough features for interaction\")\n",
        "            return x\n",
        "\n",
        "    def no_feature_selection(self, x, y):\n",
        "        \"\"\"\n",
        "        Use ALL original features - no selection at all\n",
        "        \"\"\"\n",
        "        print(f\"üéØ Using ALL original features: {x.shape[1]} features...\")\n",
        "        X_selected = x.copy()\n",
        "        selected_indices = np.arange(x.shape[1])\n",
        "        print(f\"   Using all {X_selected.shape[1]} original features\")\n",
        "        print(f\"   Feature range: 0 to {x.shape[1]-1}\")\n",
        "        return X_selected, selected_indices\n",
        "\n",
        "    # -------------------------\n",
        "    # Model\n",
        "    # -------------------------\n",
        "    def build_ultra_simple_model(self, input_dim, huber_delta=1.0):\n",
        "        \"\"\"\n",
        "        Ultra-minimal architecture\n",
        "        \"\"\"\n",
        "        print(f\"üèóÔ∏è Building ULTRA-SIMPLE model for {input_dim} features...\")\n",
        "\n",
        "        first_layer_size = max(32, min(64, input_dim))  # Scale with input size\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(first_layer_size, input_dim=input_dim, activation='relu'),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(8, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=0.0003,clipnorm=1)\n",
        "\n",
        "        model.compile(\n",
        "            loss='mean_squared_error'\n",
        "            #loss=tf.keras.losses.Huber(delta=huber_delta),  # Huber loss\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mean_squared_error']\n",
        "        )\n",
        "\n",
        "        print(f\"   Architecture: {input_dim} ‚Üí {first_layer_size} ‚Üí 32 ‚Üí 16 ‚Üí 1\")\n",
        "        print(f\"   Ultra-simple model parameters: {model.count_params():,}\")\n",
        "        print(f\"   Huber delta: {huber_delta}\")\n",
        "        return model\n",
        "\n",
        "    def train_ultra_simple_model(self, x_train, x_val, y_train, y_val,\n",
        "                                 huber_delta=1.0,\n",
        "                                 sample_weight=None, val_sample_weight=None):\n",
        "        \"\"\"\n",
        "        Quick training with optional sample weights (tail up-weighting)\n",
        "        \"\"\"\n",
        "        print(f\"üöÄ Training ultra-simple model...\")\n",
        "\n",
        "        self.model = self.build_ultra_simple_model(x_train.shape[1], huber_delta=huber_delta)\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=30,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                f\"{self.ultra_dir}ultra_simple_model.weights.h5\",\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val, val_sample_weight) if val_sample_weight is not None else (x_val, y_val),\n",
        "            epochs=200,\n",
        "            batch_size=128,  # slightly smaller than 256 to reduce over-smoothing\n",
        "            sample_weight=sample_weight,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        self.model.save(f\"{self.ultra_dir}ultra_simple_model.keras\")\n",
        "        print(\"‚úÖ Ultra-simple model training complete!\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    # -------------------------\n",
        "    # Evaluation & Plots\n",
        "    # -------------------------\n",
        "    def evaluate_and_plot(self, x_test, y_test, history):\n",
        "        \"\"\"Evaluate and create line chart\"\"\"\n",
        "        print(\"üìä Evaluating ultra-simple model...\")\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_scaled = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = self.y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "        y_true = self.y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Accuracy metrics\n",
        "        acc_05 = np.mean(np.abs(y_true - y_pred) <= 0.5) * 100\n",
        "        acc_1 = np.mean(np.abs(y_true - y_pred) <= 1) * 100\n",
        "        acc_2 = np.mean(np.abs(y_true - y_pred) <= 2) * 100\n",
        "\n",
        "        print(f\"\\nüìä ULTRA-SIMPLE MODEL RESULTS:\")\n",
        "        print(f\"=\"*40)\n",
        "        print(f\"R¬≤:           {r2:.6f}\")\n",
        "        print(f\"MAE:          {mae:.4f}\")\n",
        "        print(f\"RMSE:         {rmse:.4f}\")\n",
        "        print(f\"Accuracy ¬±2:  {acc_2:.1f}%\")\n",
        "        print(f\"=\"*40)\n",
        "\n",
        "        # Create comprehensive plots\n",
        "        self.create_result_plots(y_true, y_pred, history, r2, mae)\n",
        "\n",
        "        return {\n",
        "            'r2': r2, 'mae': mae, 'rmse': rmse,\n",
        "            'acc_2': acc_2, 'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "    def create_result_plots(self, y_true, y_pred, history, r2, mae):\n",
        "        \"\"\"Create comprehensive result plots including line chart\"\"\"\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Training History\n",
        "        plt.subplot(3, 3, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "        plt.title('Training History - Loss', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (Huber)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. MAE History\n",
        "        plt.subplot(3, 3, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "        plt.title('Training History - MAE', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Predictions Scatter Plot\n",
        "        plt.subplot(3, 3, 3)\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6, s=30, edgecolors='white', linewidth=0.5)\n",
        "        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "        plt.xlabel('True Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title(f'Predictions vs True\\nR¬≤ = {r2:.4f}', fontsize=14, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. LINE CHART - First 100 test samples\n",
        "        plt.subplot(3, 3, 4)\n",
        "        n_samples = min(100, len(y_true))\n",
        "        sample_indices = np.arange(n_samples)\n",
        "        plt.plot(sample_indices, y_true[:n_samples], '-', linewidth=2, label='True Values', marker='o', markersize=4)\n",
        "        plt.plot(sample_indices, y_pred[:n_samples], '-', linewidth=2, label='Predictions', marker='s', markersize=4)\n",
        "        plt.xlabel('Test Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'Predictions vs True (First {n_samples} samples)', fontsize=14, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. LINE CHART - Last 100 test samples\n",
        "        plt.subplot(3, 3, 5)\n",
        "        if len(y_true) > 100:\n",
        "            start_idx = len(y_true) - 100\n",
        "            sample_indices = np.arange(start_idx, len(y_true))\n",
        "            plt.plot(sample_indices, y_true[start_idx:], '-', linewidth=2, label='True Values', marker='o', markersize=4)\n",
        "            plt.plot(sample_indices, y_pred[start_idx:], '-', linewidth=2, label='Predictions', marker='s', markersize=4)\n",
        "            plt.xlabel('Test Sample Index')\n",
        "            plt.ylabel('Value')\n",
        "            plt.title('Predictions vs True (Last 100 samples)', fontsize=14, fontweight='bold')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'Not enough samples\\nfor separate last 100',\n",
        "                     ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
        "            plt.title('Last 100 Samples (N/A)', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 6. Residuals Plot\n",
        "        plt.subplot(3, 3, 6)\n",
        "        residuals = y_true - y_pred\n",
        "        plt.scatter(y_pred, residuals, alpha=0.6, s=30, edgecolors='white', linewidth=0.5)\n",
        "        plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "        plt.xlabel('Predicted Values')\n",
        "        plt.ylabel('Residuals (True - Predicted)')\n",
        "        plt.title('Residuals Plot', fontsize=14, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Error Distribution\n",
        "        plt.subplot(3, 3, 7)\n",
        "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Residuals Distribution', fontsize=14, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Prediction Error Over Range\n",
        "        plt.subplot(3, 3, 8)\n",
        "        abs_errors = np.abs(residuals)\n",
        "        plt.scatter(y_true, abs_errors, alpha=0.6, s=30, edgecolors='white', linewidth=0.5)\n",
        "        plt.xlabel('True Values')\n",
        "        plt.ylabel('Absolute Error')\n",
        "        plt.title('Prediction Error vs True Value', fontsize=14, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 9. Model Summary Text\n",
        "        plt.subplot(3, 3, 9)\n",
        "        plt.axis('off')\n",
        "        summary_text = f\"\"\"\n",
        "        ULTRA-SIMPLE MODEL SUMMARY\n",
        "\n",
        "        Architecture: dynamic ‚Üí 32 ‚Üí 16 ‚Üí 1\n",
        "        Parameters: {self.model.count_params():,}\n",
        "\n",
        "        Performance Metrics:\n",
        "        R¬≤ Score: {r2:.6f}\n",
        "        MAE: {mae:.4f}\n",
        "        RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}\n",
        "\n",
        "        Accuracy Metrics:\n",
        "        ¬±0.5: {np.mean(np.abs(y_true - y_pred) <= 0.5)*100:.1f}%\n",
        "        ¬±1.0: {np.mean(np.abs(y_true - y_pred) <= 1)*100:.1f}%\n",
        "        ¬±2.0: {np.mean(np.abs(y_true - y_pred) <= 2)*100:.1f}%\n",
        "\n",
        "        Training Info:\n",
        "        Epochs: {len(history.history['loss'])}\n",
        "        Final Train Loss: {history.history['loss'][-1]:.4f}\n",
        "        Final Val Loss: {history.history['val_loss'][-1]:.4f}\n",
        "        \"\"\"\n",
        "        plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes,\n",
        "                 fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
        "                 bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ultra_dir}ultra_simple_comprehensive_results.png\",\n",
        "                    dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Additional detailed line chart for all samples\n",
        "        self.create_detailed_line_chart(y_true, y_pred)\n",
        "\n",
        "    def create_detailed_line_chart(self, y_true, y_pred):\n",
        "        \"\"\"Create detailed line chart for predictions vs actual\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plot 1: All or sample of 500\n",
        "        plt.subplot(2, 1, 1)\n",
        "        if len(y_true) <= 500:\n",
        "            sample_indices = np.arange(len(y_true))\n",
        "            plt.plot(sample_indices, y_true, '-', linewidth=1.5, label='True Values', alpha=0.8)\n",
        "            plt.plot(sample_indices, y_pred, '-', linewidth=1.5, label='Predictions', alpha=0.8)\n",
        "            plt.title(f'All Test Samples: Predictions vs True Values (n={len(y_true)})',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "        else:\n",
        "            random_indices = np.sort(np.random.choice(len(y_true), 500, replace=False))\n",
        "            plt.plot(random_indices, y_true[random_indices], '-', linewidth=1.5,\n",
        "                     label='True Values', alpha=0.8, marker='o', markersize=2)\n",
        "            plt.plot(random_indices, y_pred[random_indices], '-', linewidth=1.5,\n",
        "                     label='Predictions', alpha=0.8, marker='s', markersize=2)\n",
        "            plt.title(f'Random Sample: Predictions vs True Values (500 of {len(y_true)} samples)',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Zoomed view of interesting region\n",
        "        plt.subplot(2, 1, 2)\n",
        "        if len(y_true) > 50:\n",
        "            window_size = min(50, len(y_true) // 4)\n",
        "            rolling_var = pd.Series(y_true).rolling(window=window_size).var()\n",
        "            max_var_idx = int(rolling_var.idxmax())\n",
        "\n",
        "            start_idx = max(0, max_var_idx - window_size)\n",
        "            end_idx = min(len(y_true), max_var_idx + window_size)\n",
        "\n",
        "            zoom_indices = np.arange(start_idx, end_idx)\n",
        "            plt.plot(zoom_indices, y_true[start_idx:end_idx], '-', linewidth=2,\n",
        "                     label='True Values', marker='o', markersize=4)\n",
        "            plt.plot(zoom_indices, y_pred[start_idx:end_idx], '-', linewidth=2,\n",
        "                     label='Predictions', marker='s', markersize=4)\n",
        "            plt.title(f'Zoomed View: Most Variable Region (samples {start_idx}-{end_idx})',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "        else:\n",
        "            plt.plot(y_true, '-', linewidth=2, label='True Values', marker='o', markersize=4)\n",
        "            plt.plot(y_pred, '-', linewidth=2, label='Predictions', marker='s', markersize=4)\n",
        "            plt.title('Zoomed View: All Samples', fontsize=14, fontweight='bold')\n",
        "\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ultra_dir}detailed_line_chart.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # -------------------------\n",
        "    # Pipeline\n",
        "    # -------------------------\n",
        "    def run_ultra_simple_pipeline(self, data_filename, windows_filename,\n",
        "                                  huber_delta=1.0, tail_weight=3.0):\n",
        "        \"\"\"Run the complete ultra-simple pipeline\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚ö° ULTRA-SIMPLE MLP PIPELINE (Huber + Tail Weights)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. Load data\n",
        "        x, y = self.load_data(data_filename, windows_filename)\n",
        "\n",
        "        # 2. Add key interaction feature\n",
        "        x_enhanced = self.add_key_interaction(x)\n",
        "\n",
        "        # 3. No feature selection - use all enhanced features\n",
        "        X_selected, selected_indices = self.no_feature_selection(x_enhanced, y)\n",
        "\n",
        "        # 3. Split data\n",
        "        print(\"üìä Splitting data...\")\n",
        "        x_temp, x_test, y_temp, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "        x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "        # >>> Build weights from UN-SCALED y <<<\n",
        "        print(\"‚öñÔ∏è Building tail sample weights...\")\n",
        "        train_weights = self.make_tail_sample_weights(y_train, low=(2,4), high=(20,24), tail_w=tail_weight)\n",
        "        val_weights   = self.make_tail_sample_weights(y_val,   low=(2,4), high=(20,24), tail_w=tail_weight)\n",
        "\n",
        "        # 4. Scale data\n",
        "        print(\"‚öñÔ∏è Scaling data...\")\n",
        "        x_train_scaled = self.x_scaler.fit_transform(x_train)\n",
        "        x_val_scaled   = self.x_scaler.transform(x_val)\n",
        "        x_test_scaled  = self.x_scaler.transform(x_test)\n",
        "\n",
        "        y_train_scaled = self.y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "        y_val_scaled   = self.y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
        "        y_test_scaled  = self.y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        print(f\"   Train: {x_train_scaled.shape[0]} samples, {x_train_scaled.shape[1]} features\")\n",
        "        print(f\"   Val:   {x_val_scaled.shape[0]} samples\")\n",
        "        print(f\"   Test:  {x_test_scaled.shape[0]} samples\")\n",
        "\n",
        "        # 5. Train ultra-simple model (with weights)\n",
        "        history = self.train_ultra_simple_model(\n",
        "            x_train_scaled, x_val_scaled, y_train_scaled, y_val_scaled,\n",
        "            huber_delta=huber_delta,\n",
        "            sample_weight=train_weights,\n",
        "            val_sample_weight=val_weights\n",
        "        )\n",
        "\n",
        "        # 6. Evaluate with detailed plots\n",
        "        results = self.evaluate_and_plot(x_test_scaled, y_test_scaled, history)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚ö° ULTRA-SIMPLE MODEL COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'model': self.model,\n",
        "            'results': results,\n",
        "            'history': history,\n",
        "            'ultra_dir': self.ultra_dir\n",
        "        }\n",
        "\n",
        "\n",
        "# Quick execution function\n",
        "def run_ultra_simple_mlp(data_filename, windows_filename,\n",
        "                         output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/',\n",
        "                         huber_delta=1.0, tail_weight=3.0):\n",
        "    \"\"\"\n",
        "    Run ultra-simple MLP with line charts, Huber loss, and tail weights\n",
        "    \"\"\"\n",
        "    ultra_mlp = UltraSimpleMLP(output_dir)\n",
        "    return ultra_mlp.run_ultra_simple_pipeline(\n",
        "        data_filename, windows_filename,\n",
        "        huber_delta=huber_delta, tail_weight=tail_weight\n",
        "    )\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚ö° Ultra-Simple MLP with Line Charts (Huber + Tail Weights)\")\n",
        "\n",
        "    # Use your data files\n",
        "    data_file = 'generated-data-OPTIMIZED.npy'\n",
        "    windows_file = 'generated-data-true-window-OPTIMIZED.npy'\n",
        "\n",
        "    # You can tune these two knobs:\n",
        "    HUBER_DELTA = 1.0   # try {0.5, 1.0, 2.0}\n",
        "    TAIL_WEIGHT = 3.0   # try {2.0, 3.0, 4.0}\n",
        "\n",
        "    results = run_ultra_simple_mlp(\n",
        "        data_file, windows_file,\n",
        "        huber_delta=HUBER_DELTA, tail_weight=TAIL_WEIGHT\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        r2 = results['results']['r2']\n",
        "        mae = results['results']['mae']\n",
        "        acc_2 = results['results']['acc_2']\n",
        "\n",
        "        print(f\"\\n‚ö° ULTRA-SIMPLE MODEL RESULTS:\")\n",
        "        print(f\"=\"*40)\n",
        "        print(f\"R¬≤:           {r2:.6f}\")\n",
        "        print(f\"MAE:          {mae:.4f}\")\n",
        "        print(f\"Accuracy ¬±2:  {acc_2:.1f}%\")\n",
        "        print(f\"Parameters:   {results['model'].count_params():,}\")\n",
        "        print(f\"=\"*40)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyO57fBNhTqvevkdBrukC90G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}