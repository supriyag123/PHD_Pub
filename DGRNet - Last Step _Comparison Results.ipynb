{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20-%20Last%20Step%20_Comparison%20Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoP7OuWNxlsJ",
        "outputId": "ff9f7b2e-5914-4245-d4a6-ae4e0219533b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "2290/2290 [==============================] - 11s 4ms/step - loss: 0.9923 - mean_squared_error: 0.9923 - val_loss: 0.9885 - val_mean_squared_error: 0.9885\n",
            "Epoch 2/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 0.9854 - val_mean_squared_error: 0.9854\n",
            "Epoch 3/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9782 - mean_squared_error: 0.9782 - val_loss: 0.9706 - val_mean_squared_error: 0.9706\n",
            "Epoch 4/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9678 - mean_squared_error: 0.9678 - val_loss: 0.9666 - val_mean_squared_error: 0.9666\n",
            "Epoch 5/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9638 - mean_squared_error: 0.9638 - val_loss: 0.9681 - val_mean_squared_error: 0.9681\n",
            "Epoch 6/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9614 - mean_squared_error: 0.9614 - val_loss: 0.9622 - val_mean_squared_error: 0.9622\n",
            "Epoch 7/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9584 - mean_squared_error: 0.9584 - val_loss: 0.9586 - val_mean_squared_error: 0.9586\n",
            "Epoch 8/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9563 - mean_squared_error: 0.9563 - val_loss: 0.9563 - val_mean_squared_error: 0.9563\n",
            "Epoch 9/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9546 - mean_squared_error: 0.9546 - val_loss: 0.9576 - val_mean_squared_error: 0.9576\n",
            "Epoch 10/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9527 - mean_squared_error: 0.9527 - val_loss: 0.9576 - val_mean_squared_error: 0.9576\n",
            "Epoch 11/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9515 - mean_squared_error: 0.9515 - val_loss: 0.9617 - val_mean_squared_error: 0.9617\n",
            "Epoch 12/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 0.9591 - val_mean_squared_error: 0.9591\n",
            "Epoch 13/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9478 - mean_squared_error: 0.9478 - val_loss: 0.9490 - val_mean_squared_error: 0.9490\n",
            "Epoch 14/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9447 - mean_squared_error: 0.9447 - val_loss: 0.9526 - val_mean_squared_error: 0.9526\n",
            "Epoch 15/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9414 - mean_squared_error: 0.9414 - val_loss: 0.9447 - val_mean_squared_error: 0.9447\n",
            "Epoch 16/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9388 - mean_squared_error: 0.9388 - val_loss: 0.9383 - val_mean_squared_error: 0.9383\n",
            "Epoch 17/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9359 - mean_squared_error: 0.9359 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
            "Epoch 18/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9334 - mean_squared_error: 0.9334 - val_loss: 0.9370 - val_mean_squared_error: 0.9370\n",
            "Epoch 19/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9311 - mean_squared_error: 0.9311 - val_loss: 0.9352 - val_mean_squared_error: 0.9352\n",
            "Epoch 20/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9290 - mean_squared_error: 0.9290 - val_loss: 0.9250 - val_mean_squared_error: 0.9250\n",
            "Epoch 21/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9275 - mean_squared_error: 0.9275 - val_loss: 0.9232 - val_mean_squared_error: 0.9232\n",
            "Epoch 22/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9263 - mean_squared_error: 0.9263 - val_loss: 0.9291 - val_mean_squared_error: 0.9291\n",
            "Epoch 23/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9250 - mean_squared_error: 0.9250 - val_loss: 0.9182 - val_mean_squared_error: 0.9182\n",
            "Epoch 24/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9234 - mean_squared_error: 0.9234 - val_loss: 0.9247 - val_mean_squared_error: 0.9247\n",
            "Epoch 25/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9229 - mean_squared_error: 0.9229 - val_loss: 0.9233 - val_mean_squared_error: 0.9233\n",
            "Epoch 26/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9222 - mean_squared_error: 0.9222 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
            "Epoch 27/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9204 - mean_squared_error: 0.9204 - val_loss: 0.9153 - val_mean_squared_error: 0.9153\n",
            "Epoch 28/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9192 - mean_squared_error: 0.9192 - val_loss: 0.9117 - val_mean_squared_error: 0.9117\n",
            "Epoch 29/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9182 - mean_squared_error: 0.9182 - val_loss: 0.9126 - val_mean_squared_error: 0.9126\n",
            "Epoch 30/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9166 - mean_squared_error: 0.9166 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
            "Epoch 31/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 0.9145 - val_mean_squared_error: 0.9145\n",
            "Epoch 32/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9147 - mean_squared_error: 0.9147 - val_loss: 0.9167 - val_mean_squared_error: 0.9167\n",
            "Epoch 33/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9135 - mean_squared_error: 0.9135 - val_loss: 0.9176 - val_mean_squared_error: 0.9176\n",
            "Epoch 34/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9123 - mean_squared_error: 0.9123 - val_loss: 0.9047 - val_mean_squared_error: 0.9047\n",
            "Epoch 35/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9100 - mean_squared_error: 0.9100 - val_loss: 0.9176 - val_mean_squared_error: 0.9176\n",
            "Epoch 36/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9106 - mean_squared_error: 0.9106 - val_loss: 0.9031 - val_mean_squared_error: 0.9031\n",
            "Epoch 37/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9097 - mean_squared_error: 0.9097 - val_loss: 0.9215 - val_mean_squared_error: 0.9215\n",
            "Epoch 38/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9078 - mean_squared_error: 0.9078 - val_loss: 0.9031 - val_mean_squared_error: 0.9031\n",
            "Epoch 39/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9072 - mean_squared_error: 0.9072 - val_loss: 0.9137 - val_mean_squared_error: 0.9137\n",
            "Epoch 40/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9051 - mean_squared_error: 0.9051 - val_loss: 0.9016 - val_mean_squared_error: 0.9016\n",
            "Epoch 41/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9053 - mean_squared_error: 0.9053 - val_loss: 0.9083 - val_mean_squared_error: 0.9083\n",
            "Epoch 42/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9040 - mean_squared_error: 0.9040 - val_loss: 0.9140 - val_mean_squared_error: 0.9140\n",
            "Epoch 43/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9028 - mean_squared_error: 0.9028 - val_loss: 0.9196 - val_mean_squared_error: 0.9196\n",
            "Epoch 44/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9026 - mean_squared_error: 0.9026 - val_loss: 0.8991 - val_mean_squared_error: 0.8991\n",
            "Epoch 45/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9009 - mean_squared_error: 0.9009 - val_loss: 0.9302 - val_mean_squared_error: 0.9302\n",
            "Epoch 46/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9001 - mean_squared_error: 0.9001 - val_loss: 0.8970 - val_mean_squared_error: 0.8970\n",
            "Epoch 47/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.9004 - mean_squared_error: 0.9004 - val_loss: 0.8997 - val_mean_squared_error: 0.8997\n",
            "Epoch 48/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8987 - mean_squared_error: 0.8987 - val_loss: 0.9009 - val_mean_squared_error: 0.9009\n",
            "Epoch 49/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8982 - mean_squared_error: 0.8982 - val_loss: 0.8944 - val_mean_squared_error: 0.8944\n",
            "Epoch 50/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8972 - mean_squared_error: 0.8972 - val_loss: 0.8933 - val_mean_squared_error: 0.8933\n",
            "Epoch 51/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8967 - mean_squared_error: 0.8967 - val_loss: 0.8981 - val_mean_squared_error: 0.8981\n",
            "Epoch 52/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8958 - mean_squared_error: 0.8958 - val_loss: 0.8965 - val_mean_squared_error: 0.8965\n",
            "Epoch 53/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8960 - mean_squared_error: 0.8960 - val_loss: 0.8996 - val_mean_squared_error: 0.8996\n",
            "Epoch 54/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8937 - mean_squared_error: 0.8937 - val_loss: 0.8875 - val_mean_squared_error: 0.8875\n",
            "Epoch 55/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8931 - mean_squared_error: 0.8931 - val_loss: 0.8893 - val_mean_squared_error: 0.8893\n",
            "Epoch 56/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8920 - mean_squared_error: 0.8920 - val_loss: 0.8879 - val_mean_squared_error: 0.8879\n",
            "Epoch 57/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8916 - mean_squared_error: 0.8916 - val_loss: 0.8888 - val_mean_squared_error: 0.8888\n",
            "Epoch 58/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8921 - mean_squared_error: 0.8921 - val_loss: 0.9218 - val_mean_squared_error: 0.9218\n",
            "Epoch 59/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8903 - mean_squared_error: 0.8903 - val_loss: 0.8822 - val_mean_squared_error: 0.8822\n",
            "Epoch 60/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8893 - mean_squared_error: 0.8893 - val_loss: 0.9224 - val_mean_squared_error: 0.9224\n",
            "Epoch 61/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8895 - mean_squared_error: 0.8895 - val_loss: 0.8797 - val_mean_squared_error: 0.8797\n",
            "Epoch 62/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8887 - mean_squared_error: 0.8887 - val_loss: 0.8838 - val_mean_squared_error: 0.8838\n",
            "Epoch 63/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8885 - mean_squared_error: 0.8885 - val_loss: 0.8942 - val_mean_squared_error: 0.8942\n",
            "Epoch 64/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 0.8796 - val_mean_squared_error: 0.8796\n",
            "Epoch 65/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8861 - mean_squared_error: 0.8861 - val_loss: 0.8817 - val_mean_squared_error: 0.8817\n",
            "Epoch 66/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8850 - mean_squared_error: 0.8850 - val_loss: 0.8790 - val_mean_squared_error: 0.8790\n",
            "Epoch 67/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8858 - mean_squared_error: 0.8858 - val_loss: 0.8924 - val_mean_squared_error: 0.8924\n",
            "Epoch 68/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8841 - mean_squared_error: 0.8841 - val_loss: 0.8885 - val_mean_squared_error: 0.8885\n",
            "Epoch 69/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8828 - mean_squared_error: 0.8828 - val_loss: 0.8883 - val_mean_squared_error: 0.8883\n",
            "Epoch 70/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8831 - mean_squared_error: 0.8831 - val_loss: 0.8822 - val_mean_squared_error: 0.8822\n",
            "Epoch 71/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8831 - mean_squared_error: 0.8831 - val_loss: 0.8794 - val_mean_squared_error: 0.8794\n",
            "Epoch 72/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8822 - mean_squared_error: 0.8822 - val_loss: 0.8816 - val_mean_squared_error: 0.8816\n",
            "Epoch 73/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8826 - mean_squared_error: 0.8826 - val_loss: 0.8887 - val_mean_squared_error: 0.8887\n",
            "Epoch 74/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8815 - mean_squared_error: 0.8815 - val_loss: 0.8739 - val_mean_squared_error: 0.8739\n",
            "Epoch 75/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8809 - mean_squared_error: 0.8809 - val_loss: 0.8873 - val_mean_squared_error: 0.8873\n",
            "Epoch 76/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8806 - mean_squared_error: 0.8806 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
            "Epoch 77/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8793 - mean_squared_error: 0.8793 - val_loss: 0.8867 - val_mean_squared_error: 0.8867\n",
            "Epoch 78/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8793 - mean_squared_error: 0.8793 - val_loss: 0.8804 - val_mean_squared_error: 0.8804\n",
            "Epoch 79/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8782 - mean_squared_error: 0.8782 - val_loss: 0.8837 - val_mean_squared_error: 0.8837\n",
            "Epoch 80/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8779 - mean_squared_error: 0.8779 - val_loss: 0.8748 - val_mean_squared_error: 0.8748\n",
            "Epoch 81/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8783 - mean_squared_error: 0.8783 - val_loss: 0.8810 - val_mean_squared_error: 0.8810\n",
            "Epoch 82/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8777 - mean_squared_error: 0.8777 - val_loss: 0.8971 - val_mean_squared_error: 0.8971\n",
            "Epoch 83/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8758 - mean_squared_error: 0.8758 - val_loss: 0.8785 - val_mean_squared_error: 0.8785\n",
            "Epoch 84/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8757 - mean_squared_error: 0.8757 - val_loss: 0.8950 - val_mean_squared_error: 0.8950\n",
            "Epoch 85/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8756 - mean_squared_error: 0.8756 - val_loss: 0.8777 - val_mean_squared_error: 0.8777\n",
            "Epoch 86/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8751 - mean_squared_error: 0.8751 - val_loss: 0.8691 - val_mean_squared_error: 0.8691\n",
            "Epoch 87/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8749 - mean_squared_error: 0.8749 - val_loss: 0.8671 - val_mean_squared_error: 0.8671\n",
            "Epoch 88/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8738 - mean_squared_error: 0.8738 - val_loss: 0.8681 - val_mean_squared_error: 0.8681\n",
            "Epoch 89/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8745 - mean_squared_error: 0.8745 - val_loss: 0.8788 - val_mean_squared_error: 0.8788\n",
            "Epoch 90/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8747 - mean_squared_error: 0.8747 - val_loss: 0.8986 - val_mean_squared_error: 0.8986\n",
            "Epoch 91/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8730 - mean_squared_error: 0.8730 - val_loss: 0.8670 - val_mean_squared_error: 0.8670\n",
            "Epoch 92/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8725 - mean_squared_error: 0.8725 - val_loss: 0.8701 - val_mean_squared_error: 0.8701\n",
            "Epoch 93/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8738 - mean_squared_error: 0.8738 - val_loss: 0.8712 - val_mean_squared_error: 0.8712\n",
            "Epoch 94/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8723 - mean_squared_error: 0.8723 - val_loss: 0.8668 - val_mean_squared_error: 0.8668\n",
            "Epoch 95/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8712 - mean_squared_error: 0.8712 - val_loss: 0.8650 - val_mean_squared_error: 0.8650\n",
            "Epoch 96/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8713 - mean_squared_error: 0.8713 - val_loss: 0.8700 - val_mean_squared_error: 0.8700\n",
            "Epoch 97/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8706 - mean_squared_error: 0.8706 - val_loss: 0.9109 - val_mean_squared_error: 0.9109\n",
            "Epoch 98/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8714 - mean_squared_error: 0.8714 - val_loss: 0.8701 - val_mean_squared_error: 0.8701\n",
            "Epoch 99/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8698 - mean_squared_error: 0.8698 - val_loss: 0.8746 - val_mean_squared_error: 0.8746\n",
            "Epoch 100/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8684 - mean_squared_error: 0.8684 - val_loss: 0.8676 - val_mean_squared_error: 0.8676\n",
            "Epoch 101/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8690 - mean_squared_error: 0.8690 - val_loss: 0.8748 - val_mean_squared_error: 0.8748\n",
            "Epoch 102/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8683 - mean_squared_error: 0.8683 - val_loss: 0.8645 - val_mean_squared_error: 0.8645\n",
            "Epoch 103/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8678 - mean_squared_error: 0.8678 - val_loss: 0.8700 - val_mean_squared_error: 0.8700\n",
            "Epoch 104/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8687 - mean_squared_error: 0.8687 - val_loss: 0.8890 - val_mean_squared_error: 0.8890\n",
            "Epoch 105/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8663 - mean_squared_error: 0.8663 - val_loss: 0.8857 - val_mean_squared_error: 0.8857\n",
            "Epoch 106/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8661 - mean_squared_error: 0.8661 - val_loss: 0.9031 - val_mean_squared_error: 0.9031\n",
            "Epoch 107/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8671 - mean_squared_error: 0.8671 - val_loss: 0.8810 - val_mean_squared_error: 0.8810\n",
            "Epoch 108/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8660 - mean_squared_error: 0.8660 - val_loss: 0.8798 - val_mean_squared_error: 0.8798\n",
            "Epoch 109/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8655 - mean_squared_error: 0.8655 - val_loss: 0.8678 - val_mean_squared_error: 0.8678\n",
            "Epoch 110/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8627 - mean_squared_error: 0.8627 - val_loss: 0.8920 - val_mean_squared_error: 0.8920\n",
            "Epoch 111/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8643 - mean_squared_error: 0.8643 - val_loss: 0.8597 - val_mean_squared_error: 0.8597\n",
            "Epoch 112/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8639 - mean_squared_error: 0.8639 - val_loss: 0.8754 - val_mean_squared_error: 0.8754\n",
            "Epoch 113/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8632 - mean_squared_error: 0.8632 - val_loss: 0.8787 - val_mean_squared_error: 0.8787\n",
            "Epoch 114/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8622 - mean_squared_error: 0.8622 - val_loss: 0.8698 - val_mean_squared_error: 0.8698\n",
            "Epoch 115/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 0.8547 - val_mean_squared_error: 0.8547\n",
            "Epoch 116/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8612 - mean_squared_error: 0.8612 - val_loss: 0.8585 - val_mean_squared_error: 0.8585\n",
            "Epoch 117/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8616 - mean_squared_error: 0.8616 - val_loss: 0.8658 - val_mean_squared_error: 0.8658\n",
            "Epoch 118/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8611 - mean_squared_error: 0.8611 - val_loss: 0.8581 - val_mean_squared_error: 0.8581\n",
            "Epoch 119/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8610 - mean_squared_error: 0.8610 - val_loss: 0.8547 - val_mean_squared_error: 0.8547\n",
            "Epoch 120/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8609 - mean_squared_error: 0.8609 - val_loss: 0.8522 - val_mean_squared_error: 0.8522\n",
            "Epoch 121/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8600 - mean_squared_error: 0.8600 - val_loss: 0.9286 - val_mean_squared_error: 0.9286\n",
            "Epoch 122/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8604 - mean_squared_error: 0.8604 - val_loss: 0.8517 - val_mean_squared_error: 0.8517\n",
            "Epoch 123/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8589 - mean_squared_error: 0.8589 - val_loss: 0.8527 - val_mean_squared_error: 0.8527\n",
            "Epoch 124/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8570 - mean_squared_error: 0.8570 - val_loss: 0.8594 - val_mean_squared_error: 0.8594\n",
            "Epoch 125/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8578 - mean_squared_error: 0.8578 - val_loss: 0.8640 - val_mean_squared_error: 0.8640\n",
            "Epoch 126/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8573 - mean_squared_error: 0.8573 - val_loss: 0.8512 - val_mean_squared_error: 0.8512\n",
            "Epoch 127/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8577 - mean_squared_error: 0.8577 - val_loss: 0.8666 - val_mean_squared_error: 0.8666\n",
            "Epoch 128/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8550 - mean_squared_error: 0.8550 - val_loss: 0.8450 - val_mean_squared_error: 0.8450\n",
            "Epoch 129/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8567 - mean_squared_error: 0.8567 - val_loss: 0.8808 - val_mean_squared_error: 0.8808\n",
            "Epoch 130/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8550 - mean_squared_error: 0.8550 - val_loss: 0.8486 - val_mean_squared_error: 0.8486\n",
            "Epoch 131/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 0.8771 - val_mean_squared_error: 0.8771\n",
            "Epoch 132/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8547 - mean_squared_error: 0.8547 - val_loss: 0.8742 - val_mean_squared_error: 0.8742\n",
            "Epoch 133/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8536 - mean_squared_error: 0.8536 - val_loss: 0.8516 - val_mean_squared_error: 0.8516\n",
            "Epoch 134/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8534 - mean_squared_error: 0.8534 - val_loss: 0.8897 - val_mean_squared_error: 0.8897\n",
            "Epoch 135/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8527 - mean_squared_error: 0.8527 - val_loss: 0.8548 - val_mean_squared_error: 0.8548\n",
            "Epoch 136/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8520 - mean_squared_error: 0.8520 - val_loss: 0.8685 - val_mean_squared_error: 0.8685\n",
            "Epoch 137/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 0.8752 - val_mean_squared_error: 0.8752\n",
            "Epoch 138/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 0.8835 - val_mean_squared_error: 0.8835\n",
            "Epoch 139/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8502 - mean_squared_error: 0.8502 - val_loss: 0.8527 - val_mean_squared_error: 0.8527\n",
            "Epoch 140/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8520 - mean_squared_error: 0.8520 - val_loss: 0.8674 - val_mean_squared_error: 0.8674\n",
            "Epoch 141/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 0.8495 - val_mean_squared_error: 0.8495\n",
            "Epoch 142/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 0.8401 - val_mean_squared_error: 0.8401\n",
            "Epoch 143/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9303 - val_mean_squared_error: 0.9303\n",
            "Epoch 144/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.8461 - val_mean_squared_error: 0.8461\n",
            "Epoch 145/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8489 - mean_squared_error: 0.8489 - val_loss: 0.8687 - val_mean_squared_error: 0.8687\n",
            "Epoch 146/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.8513 - val_mean_squared_error: 0.8513\n",
            "Epoch 147/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8472 - mean_squared_error: 0.8472 - val_loss: 0.8575 - val_mean_squared_error: 0.8575\n",
            "Epoch 148/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8481 - mean_squared_error: 0.8481 - val_loss: 0.8713 - val_mean_squared_error: 0.8713\n",
            "Epoch 149/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8463 - mean_squared_error: 0.8463 - val_loss: 0.8440 - val_mean_squared_error: 0.8440\n",
            "Epoch 150/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8462 - mean_squared_error: 0.8462 - val_loss: 0.8519 - val_mean_squared_error: 0.8519\n",
            "Epoch 151/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.8501 - val_mean_squared_error: 0.8501\n",
            "Epoch 152/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.8488 - val_mean_squared_error: 0.8488\n",
            "Epoch 153/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.8428 - val_mean_squared_error: 0.8428\n",
            "Epoch 154/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 0.8572 - val_mean_squared_error: 0.8572\n",
            "Epoch 155/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.8334 - val_mean_squared_error: 0.8334\n",
            "Epoch 156/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.8386 - val_mean_squared_error: 0.8386\n",
            "Epoch 157/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.8476 - val_mean_squared_error: 0.8476\n",
            "Epoch 158/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.8326 - val_mean_squared_error: 0.8326\n",
            "Epoch 159/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.8470 - val_mean_squared_error: 0.8470\n",
            "Epoch 160/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.8378 - val_mean_squared_error: 0.8378\n",
            "Epoch 161/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.8337 - val_mean_squared_error: 0.8337\n",
            "Epoch 162/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.8441 - val_mean_squared_error: 0.8441\n",
            "Epoch 163/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.8491 - val_mean_squared_error: 0.8491\n",
            "Epoch 164/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.8460 - val_mean_squared_error: 0.8460\n",
            "Epoch 165/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.8612 - val_mean_squared_error: 0.8612\n",
            "Epoch 166/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.8562 - val_mean_squared_error: 0.8562\n",
            "Epoch 167/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.8354 - val_mean_squared_error: 0.8354\n",
            "Epoch 168/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.8403 - val_mean_squared_error: 0.8403\n",
            "Epoch 169/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.8454 - val_mean_squared_error: 0.8454\n",
            "Epoch 170/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.8378 - val_mean_squared_error: 0.8378\n",
            "Epoch 171/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.8446 - val_mean_squared_error: 0.8446\n",
            "Epoch 172/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.8449 - val_mean_squared_error: 0.8449\n",
            "Epoch 173/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.8480 - val_mean_squared_error: 0.8480\n",
            "Epoch 174/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.8515 - val_mean_squared_error: 0.8515\n",
            "Epoch 175/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.8251 - val_mean_squared_error: 0.8251\n",
            "Epoch 176/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8406 - mean_squared_error: 0.8406 - val_loss: 0.8360 - val_mean_squared_error: 0.8360\n",
            "Epoch 177/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.8362 - val_mean_squared_error: 0.8362\n",
            "Epoch 178/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8403 - mean_squared_error: 0.8403 - val_loss: 0.8486 - val_mean_squared_error: 0.8486\n",
            "Epoch 179/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8391 - mean_squared_error: 0.8391 - val_loss: 0.8351 - val_mean_squared_error: 0.8351\n",
            "Epoch 180/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.8693 - val_mean_squared_error: 0.8693\n",
            "Epoch 181/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.8418 - val_mean_squared_error: 0.8418\n",
            "Epoch 182/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 0.8407 - val_mean_squared_error: 0.8407\n",
            "Epoch 183/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.8544 - val_mean_squared_error: 0.8544\n",
            "Epoch 184/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.8452 - val_mean_squared_error: 0.8452\n",
            "Epoch 185/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 0.8426 - val_mean_squared_error: 0.8426\n",
            "Epoch 186/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 0.8437 - val_mean_squared_error: 0.8437\n",
            "Epoch 187/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.8366 - val_mean_squared_error: 0.8366\n",
            "Epoch 188/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.8690 - val_mean_squared_error: 0.8690\n",
            "Epoch 189/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.8303 - val_mean_squared_error: 0.8303\n",
            "Epoch 190/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8406 - mean_squared_error: 0.8406 - val_loss: 0.8341 - val_mean_squared_error: 0.8341\n",
            "Epoch 191/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.8392 - val_mean_squared_error: 0.8392\n",
            "Epoch 192/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.8554 - val_mean_squared_error: 0.8554\n",
            "Epoch 193/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.8327 - val_mean_squared_error: 0.8327\n",
            "Epoch 194/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.8575 - val_mean_squared_error: 0.8575\n",
            "Epoch 195/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.8421 - val_mean_squared_error: 0.8421\n",
            "Epoch 196/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8368 - mean_squared_error: 0.8368 - val_loss: 0.8379 - val_mean_squared_error: 0.8379\n",
            "Epoch 197/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.8311 - val_mean_squared_error: 0.8311\n",
            "Epoch 198/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.8398 - val_mean_squared_error: 0.8398\n",
            "Epoch 199/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8386 - mean_squared_error: 0.8386 - val_loss: 0.8412 - val_mean_squared_error: 0.8412\n",
            "Epoch 200/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.8395 - val_mean_squared_error: 0.8395\n",
            "Epoch 201/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8386 - mean_squared_error: 0.8386 - val_loss: 0.8467 - val_mean_squared_error: 0.8467\n",
            "Epoch 202/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.8336 - val_mean_squared_error: 0.8336\n",
            "Epoch 203/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.8612 - val_mean_squared_error: 0.8612\n",
            "Epoch 204/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.8505 - val_mean_squared_error: 0.8505\n",
            "Epoch 205/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8380 - mean_squared_error: 0.8380 - val_loss: 0.8555 - val_mean_squared_error: 0.8555\n",
            "Epoch 206/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.8480 - val_mean_squared_error: 0.8480\n",
            "Epoch 207/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.8949 - val_mean_squared_error: 0.8949\n",
            "Epoch 208/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.8276 - val_mean_squared_error: 0.8276\n",
            "Epoch 209/2000\n",
            "2290/2290 [==============================] - 9s 4ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.8385 - val_mean_squared_error: 0.8385\n",
            "Epoch 210/2000\n",
            "1982/2290 [========================>.....] - ETA: 1s - loss: 0.8365 - mean_squared_error: 0.8365"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "\n",
        "generator_multiply = 100 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "#get data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN-Daily-May2024.npy')\n",
        "\n",
        "\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TEST_hourly.npy')\n",
        "#all_data = np.concatenate((train_data,test_data),axis=0)\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-Daily-May2024.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "#----------------check window distribution - we see upto 20 has very high fequency. so we remove that and take the rest to generate more samples, to create overall uniform distribution...\n",
        "#First we test window 20 to 30 and see if this method can fit the model well\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(window_label, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "\n",
        "##---------------------------IGNORE THIS IF NOT GENERATING FRESH VAE DATASET--------------------------------------------------------------------------------------------\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-round4-latent5-dim256.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-round4-latent5-dim256.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_daily_x100',results1)\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_daily_x100.npy')\n",
        "\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "#--------------------------------------IF REQUIRED REMOVE outlier....however we are not doing this now.--------------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x, y = x[mask, :], y[mask]\n",
        "\n",
        "\n",
        "###############Scale the target and then split the data into train test----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#Looking at the dist, we remove al y less than 20\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "y_transformed = transformer.fit_transform(y.reshape(-1,1)).flatten()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_transformed, test_size = 0.1, random_state = 42)\n",
        "\n",
        "#--------------------------------------------------------------------------------------CONSTRUCT, COMPILE, AND TRAIN THE MODEL------------------------------------------------------------------------------------------------------------------\n",
        "#------------MLP------------------------------------------------------\n",
        "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(LSTM(1024, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(512,return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(units = 64))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 32))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "#--------------------------------------------------------------LSTM--------------------------\n",
        "\n",
        "a =  x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "b =  x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(a.shape[1],a.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 16))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "#model.add(Dense(units = 4))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 1, activation = 'linear'))\n",
        "model.summary()\n",
        "x_train = a\n",
        "x_test = b\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.0002,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.0001, monitor='loss', mode='min', restore_best_weights=True)\n",
        "n_epochs = 2000\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=50,\n",
        "                   validation_split=0.1)\n",
        "                 #callbacks=[es])\n",
        "\n",
        "\n",
        "#-------------------------LOSS-------------------------------------\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "#-----------------------------------------------------TRAIN EVALUATION----------------------------------------------------------------\n",
        "y_train_pred_raw = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred_raw)\n",
        "y_train_true = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "\n",
        "score_train= r2_score(y_train_true,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "plt.plot(y_train_true[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------------------------TEST EVALUATION----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[100:150], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[100:150], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------SAVE MODEL AND RESULTS-----------------------------------------------------------------\n",
        "\n",
        "model.save(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily_X100.keras')\n",
        "# It can be used to reconstruct the model identically.\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "reconstructed_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily.keras')\n",
        "\n",
        "# Let's check:\n",
        "np.testing.assert_allclose(\n",
        "    model.predict(x_test), reconstructed_model.predict(x_test)\n",
        ")\n",
        "\n",
        "y_pred_raw = reconstructed_model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true, color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred, color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/preduber_2.csv',y_test_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/realuber_2.csv',y_test_true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Daily%20Data.ipynb",
      "authorship_tag": "ABX9TyOoYahrJVgD9B/8/GDdVVL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}