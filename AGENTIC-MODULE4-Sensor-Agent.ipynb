{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9e551293-7602-47c3-d5c2-f84b8fdc0e23"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3308771868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load your hold-out data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# shape: [timesteps, num_sensors]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 50,\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1):\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {'mean': 0.0, 'std': 1.0, 'q95': 0.0, 'q99': 0.0}\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path)\n",
        "\n",
        "                # Metadata sidecar file\n",
        "                metadata_path = model_path.replace('.h5', '_metadata.pkl')\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "                    self.scaler = metadata.get('scaler', StandardScaler())\n",
        "                    self.rolling_stats = metadata.get('rolling_stats', self.rolling_stats)\n",
        "                    if 'error_history' in metadata:\n",
        "                        self.baseline_errors = np.array(metadata['error_history'])\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format ‚Äì expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"‚úÖ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3. Update rolling stats periodically\n",
        "        if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "            self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model.\"\"\"\n",
        "        try:\n",
        "            data_scaled = self.scaler.transform(subsequence.reshape(-1, 1))\n",
        "            X = data_scaled.reshape(1, self.window_length, 1)  # [batch, timesteps, features]\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "            error = mean_squared_error(data_scaled.flatten(), reconstruction.flatten())\n",
        "            return max(0.0, error)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            return np.var(subsequence)\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "        self.rolling_stats['mean'] = np.mean(errors_array)\n",
        "        self.rolling_stats['std'] = np.std(errors_array) + 1e-8\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        return score > threshold\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            return js_divergence > self.drift_threshold\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        if self.rolling_stats['std'] == 0: return 0.5\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        distance_from_threshold = abs(score - threshold) / self.rolling_stats['std']\n",
        "        return min(1.0, distance_from_threshold / 3.0)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"üöÄ Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=50,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1)\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"‚úÖ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "    return sensor_agents, master\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# DEMO ENTRYPOINT\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "if __name__ == \"__main__\":\n",
        "    models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "\n",
        "    # Load precomputed subsequences\n",
        "    data_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\"\n",
        "    subsequences = np.load(data_path)   # shape: [num_samples, window_length, num_sensors]\n",
        "\n",
        "    print(f\"‚úÖ Loaded subsequences: {subsequences.shape}\")\n",
        "    num_samples, window_length, num_sensors = subsequences.shape\n",
        "\n",
        "    # Take last 1000 samples as hold-out\n",
        "    holdout = subsequences[-1000:]\n",
        "\n",
        "    # Create agents + master\n",
        "    sensor_agents, master = create_robust_system(num_sensors=num_sensors, models_dir=models_dir)\n",
        "\n",
        "    # Streaming test: pick 10‚Äì100 samples\n",
        "    test_samples = 20  # you can set 10, 20, 100, etc.\n",
        "\n",
        "    print(f\"‚ñ∂Ô∏è Running streaming test on {test_samples} hold-out subsequences\")\n",
        "\n",
        "    for i in range(test_samples):\n",
        "        live_seq = holdout[i]   # shape [window_length, num_sensors]\n",
        "\n",
        "        result = master.process_system_input(live_seq)\n",
        "\n",
        "        # Print system-level decision\n",
        "        print(f\"\\nStep {i+1}/{test_samples}\")\n",
        "        print(\"System decision:\", result[\"system_decisions\"])\n",
        "\n",
        "        # Print sensor-level results\n",
        "        for r in result[\"sensor_results\"]:\n",
        "            print(f\"  Sensor {r['sensor_id']}: \"\n",
        "                  f\"anomaly={r['is_anomaly']} \"\n",
        "                  f\"drift={r['drift_flag']} \"\n",
        "                  f\"score={r['anomaly_score']:.4f} \"\n",
        "                  f\"conf={r['confidence']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb",
      "authorship_tag": "ABX9TyM9OgY1eJXDCQCn6l3PezEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}