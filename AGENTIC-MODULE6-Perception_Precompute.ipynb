{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-Perception_Precompute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "outputId": "5c1756da-b3e8-4dbb-c346-fe31c7c5d55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Creating robust system with 12 sensors\n",
            "‚úÖ AE model loaded for sensor 0\n",
            "‚úÖ AE model loaded for sensor 1\n",
            "‚úÖ AE model loaded for sensor 2\n",
            "‚úÖ AE model loaded for sensor 3\n",
            "‚úÖ AE model loaded for sensor 4\n",
            "‚úÖ AE model loaded for sensor 5\n",
            "‚úÖ AE model loaded for sensor 6\n",
            "‚úÖ AE model loaded for sensor 7\n",
            "‚úÖ AE model loaded for sensor 8\n",
            "‚úÖ AE model loaded for sensor 9\n",
            "‚úÖ AE model loaded for sensor 10\n",
            "‚úÖ AE model loaded for sensor 11\n",
            "‚úÖ Created system: 12/12 models loaded\n",
            "‚úÖ Loaded baseline error + window distribution.\n",
            "   Error median=0.001153, MAD=0.000440\n",
            "   Window mean=67.258, std=22.841\n",
            "‚úÖ Loaded MLP model from /content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\n",
            "AdaptiveWindowAgent adaptive_window_agent initialized\n",
            "‚úÖ Loaded NSP LSTM next-step predictor\n",
            "üÜï Starting fresh holdout evaluation\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1993703726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mmaster_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_system_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mwindow_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_window_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3788495037.py\u001b[0m in \u001b[0;36mpredict_window_size\u001b[0;34m(self, feature_vector, sequence_3d)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mpred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Fault Classification Pipeline\n",
        "# ============================================================\n",
        "\n",
        "############PASTE ADAPTIVE WINDOW HERE - so everything is in one file - later, we can import as a package#####################\n",
        "\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "# =====================================================\n",
        "# AdaptiveWindowAgent (improved version)\n",
        "# =====================================================\n",
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor  # ‚úÖ MOVED TO TOP\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "class EventStore:\n",
        "    def __init__(self, db_path=\"event_store.db\"):\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self._init_tables()\n",
        "\n",
        "    def _init_tables(self):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS events (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT,\n",
        "                event_type TEXT,\n",
        "                packet_json TEXT,\n",
        "                expert_json TEXT,\n",
        "                human_json TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.conn.commit()\n",
        "\n",
        "    def save_event(self, event_type, packet, expert=None, human=None):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\n",
        "            \"INSERT INTO events(timestamp,event_type,packet_json,expert_json,human_json) VALUES (?,?,?,?,?)\",\n",
        "            (\n",
        "                datetime.now().isoformat(),\n",
        "                event_type,\n",
        "                json.dumps(packet, default=str),\n",
        "                json.dumps(expert, default=str) if expert else None,\n",
        "                json.dumps(human, default=str) if human else None,\n",
        "            )\n",
        "        )\n",
        "        self.conn.commit()\n",
        "\n",
        "    def fetch_recent(self, limit=100):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"SELECT packet_json, expert_json, human_json FROM events ORDER BY id DESC LIMIT ?\", (limit,))\n",
        "        return [json.loads(row[0]) for row in c.fetchall()]\n",
        "\n",
        "######################################################################################\n",
        "# DECISION POLICY CONFIGS-----------------------------------------------------------\n",
        "######################################################################################'\n",
        "\n",
        "# =====================================================\n",
        "# DECISION POLICY (SEPARATED FROM LOGIC)\n",
        "# =====================================================\n",
        "\n",
        "BASE_POLICY = {\n",
        "    # --- detection fusion ---\n",
        "    \"w_sensor\": 0.60,\n",
        "    \"w_window\": 0.40,\n",
        "\n",
        "    # --- drift fusion ---\n",
        "    \"w_fdi\": 0.50,\n",
        "    \"w_wdi\": 0.25,\n",
        "    \"w_sensor_drift\": 0.25,\n",
        "\n",
        "    # --- failure prediction ---\n",
        "    \"w_fault\": 0.70,\n",
        "    \"w_warn\": 0.15,\n",
        "    \"w_det_context\": 0.15,\n",
        "\n",
        "    # --- thresholds ---\n",
        "    \"detection_threshold\": 0.50,\n",
        "    \"drift_threshold\": 0.35,\n",
        "    \"failure_threshold\": 0.50,\n",
        "    \"failure_critical_threshold\": 0.80,\n",
        "\n",
        "    # --- alert mapping ---\n",
        "    \"alert_low\": 0.35,\n",
        "    \"alert_med\": 0.55,\n",
        "    \"alert_high\": 0.75,\n",
        "}\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Window Agent - Global Context or Global Predictive Context\n",
        "#########################################################################\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Adaptive Window Agent:\n",
        "    - Predicts window size using MLP\n",
        "    - Evaluates forecast with RF/persistence\n",
        "    - Computes:\n",
        "        * FDS: Forecast Deviation Score (normalized error)\n",
        "        * FDI: Forecast Drift Index (JSD over FDS distribution)\n",
        "        * WSS: Window Shift Score (normalized window size)\n",
        "        * WDI: Window Drift Index (JSD over window size distribution)\n",
        "    - Detects anomaly (local) + drift (regime) events.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id=\"adaptive_window_agent\",\n",
        "                 model_path=None, checkpoint_path=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        "\n",
        "        self.baseline_path = \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_baseline.pkl\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Core model\n",
        "        # --------------------------------------------------\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Baseline error stats (median / MAD) ‚Äì filled from baseline file\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Metrics memory\n",
        "        # --------------------------------------------------\n",
        "        # Raw error\n",
        "        self.error_memory = deque(maxlen=300)     # long-term errors\n",
        "        self.recent_errors = deque(maxlen=50)     # kept for backward compat (not central now)\n",
        "\n",
        "        # FDS (Forecast Deviation Score) history\n",
        "        self.fds_memory = deque(maxlen=300)\n",
        "        self.recent_fds = deque(maxlen=50)\n",
        "\n",
        "        # Window history\n",
        "        self.window_memory = deque(maxlen=300)\n",
        "        self.recent_windows = deque(maxlen=50)\n",
        "\n",
        "        # Last-step metrics (for returning)\n",
        "        self.last_fds = 0.0\n",
        "        self.last_fdi = 0.0\n",
        "        self.last_wss = 0.0\n",
        "        self.last_wdi = 0.0\n",
        "\n",
        "        # Baseline distributions\n",
        "        self.baseline_errors = None\n",
        "        self.baseline_fds = None\n",
        "        self.baseline_windows = None\n",
        "        self.window_mean = 50.0    # a reasonable mid value\n",
        "        self.window_std = 10.0     # non-zero, avoids div-by-zero\n",
        "\n",
        "        # Optional histogram bins stored in baseline\n",
        "        self.window_hist_bins = None\n",
        "        self.window_hist_counts = None\n",
        "\n",
        "        # Debug flag (OFF by default)\n",
        "        self.debug = False\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Anomaly / drift settings\n",
        "        # --------------------------------------------------\n",
        "        self.threshold_k = 3.0\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.anomaly_cooldown_steps = 5\n",
        "\n",
        "        # Drift detection\n",
        "        self.drift_threshold = 0.25          # for FDI (JSD over FDS)\n",
        "        self.window_drift_threshold = 0.20   # for WDI (JSD over window sizes)\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.drift_cooldown = 0\n",
        "        self.drift_votes_required = 10\n",
        "        self.drift_cooldown_steps = 100\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Retraining buffer (unchanged)\n",
        "        # --------------------------------------------------\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Prediction history (for reporting)\n",
        "        # --------------------------------------------------\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load baseline (errors + windows)\n",
        "        # --------------------------------------------------\n",
        "        self._load_baseline()\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load model last\n",
        "        # --------------------------------------------------\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load NSP (Next-Step Predictor)\n",
        "        # --------------------------------------------------\n",
        "        self.nsp_model_path = \"/content/drive/MyDrive/PHD/2025/NSP_LSTM_next_step.keras\"\n",
        "        self.nsp_model = keras.models.load_model(self.nsp_model_path)\n",
        "        print(\"‚úÖ Loaded NSP LSTM next-step predictor\")\n",
        "\n",
        "    # =================== BASELINE LOADING ===================\n",
        "\n",
        "    def _load_baseline(self):\n",
        "        \"\"\"\n",
        "        Load baseline stats:\n",
        "          - baseline_errors, median, mad\n",
        "          - baseline_windows, window_mean, window_std\n",
        "          - optional histogram bins/counts for windows\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.baseline_path):\n",
        "            with open(self.baseline_path, \"rb\") as f:\n",
        "                base = pickle.load(f)\n",
        "\n",
        "            # Error baseline\n",
        "            self.baseline_errors = np.array(base[\"baseline_errors\"])\n",
        "            self.rolling_stats[\"median\"] = base[\"median\"]\n",
        "            self.rolling_stats[\"mad\"] = base[\"mad\"]\n",
        "\n",
        "            # Precompute baseline FDS distribution\n",
        "            med = self.rolling_stats[\"median\"]\n",
        "            mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "            self.baseline_fds = (self.baseline_errors - med) / (mad + 1e-8)\n",
        "\n",
        "            # Window baseline (may or may not exist)\n",
        "            if \"baseline_windows\" in base:\n",
        "                self.baseline_windows = np.array(base[\"baseline_windows\"])\n",
        "                self.window_mean = float(base.get(\"window_mean\", np.mean(self.baseline_windows)))\n",
        "                self.window_std = float(base.get(\"window_std\", np.std(self.baseline_windows) + 1e-8))\n",
        "                self.window_hist_bins = np.array(base.get(\"window_hist_bins\", [])) if \"window_hist_bins\" in base else None\n",
        "                self.window_hist_counts = np.array(base.get(\"window_hist_counts\", [])) if \"window_hist_counts\" in base else None\n",
        "            else:\n",
        "                self.baseline_windows = None\n",
        "                self.window_mean = 0.0\n",
        "                self.window_std = 1.0\n",
        "                self.window_hist_bins = None\n",
        "                self.window_hist_counts = None\n",
        "\n",
        "            print(\"‚úÖ Loaded baseline error + window distribution.\")\n",
        "            print(f\"   Error median={self.rolling_stats['median']:.6f}, MAD={self.rolling_stats['mad']:.6f}\")\n",
        "            if self.baseline_windows is not None:\n",
        "                print(f\"   Window mean={self.window_mean:.3f}, std={self.window_std:.3f}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No baseline found. Using live history only.\")\n",
        "            self.baseline_errors = None\n",
        "            self.baseline_fds = None\n",
        "            self.baseline_windows = None\n",
        "\n",
        "    # =================== MODEL LOADING ===================\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"‚úÖ Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                # Try to load transformer\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                else:\n",
        "                    # Fit transformer from true window labels\n",
        "                    y_original = np.load(\n",
        "                        \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy\"\n",
        "                    )\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(\"‚ö†Ô∏è No transformer found, fitted a new one.\")\n",
        "            else:\n",
        "                print(f\"‚ùå Model not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {e}\")\n",
        "\n",
        "    # =================== FORECAST EVALUATION ===================\n",
        "\n",
        "    def evaluate_forecast_performance(self, sequence_3d, predicted_window, n_future=1):\n",
        "        try:\n",
        "            seq = np.asarray(sequence_3d)\n",
        "            T, F = seq.shape\n",
        "\n",
        "            W = int(predicted_window)\n",
        "            if W < 2:\n",
        "                W = 2\n",
        "            if W > T - n_future - 1:\n",
        "                W = max(2, T - n_future - 1)\n",
        "\n",
        "            # --- Prepare NSP input ---\n",
        "            window = seq[-W:-n_future, :]   # shape (W-1, F)\n",
        "            x = window[np.newaxis, ...]      # shape (1, W-1, F)\n",
        "\n",
        "            # --- NSP prediction ---\n",
        "            y_pred = self.nsp_model.predict(x, verbose=0)[0]  # (F,)\n",
        "            y_true = seq[-n_future, :]                        # (F,)\n",
        "\n",
        "            mse = float(np.mean((y_true - y_pred) ** 2))\n",
        "            mae = float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "            return {\n",
        "                \"mse\": mse,\n",
        "                \"mae\": mae,\n",
        "                \"forecast_success\": True,\n",
        "                \"actual_values\": y_true.tolist(),\n",
        "                \"predicted_values\": y_pred.tolist(),\n",
        "                \"window_size_used\": W,\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"mse\": 9999.0,\n",
        "                \"mae\": 9999.0,\n",
        "                \"forecast_success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "    # =================== PERSISTENCE FALLBACK ===================\n",
        "\n",
        "    def _persistence_forecast(self, seq, target_sensor_index, n_future):\n",
        "        \"\"\"\n",
        "        Persistence fallback for RF evaluation.\n",
        "        Last-value-carried-forward for target sensor.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            seq = np.asarray(seq)\n",
        "            if len(seq) < 2:\n",
        "                return {\n",
        "                    'mse': 9999,\n",
        "                    'mae': 9999,\n",
        "                    'forecast_success': False,\n",
        "                    'error': 'Sequence too short',\n",
        "                    'method': 'Persistence'\n",
        "                }\n",
        "\n",
        "            last_value = seq[-1, target_sensor_index]\n",
        "            predicted_vals = [last_value]\n",
        "            actual = [seq[-1, target_sensor_index]]\n",
        "\n",
        "            mse = 0.0\n",
        "            mae = 0.0\n",
        "\n",
        "            return {\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual,\n",
        "                'predicted_values': predicted_vals,\n",
        "                'target_sensor_index': target_sensor_index,\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'mse': 9999,\n",
        "                'mae': 9999,\n",
        "                'forecast_success': False,\n",
        "                'error': str(e),\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback_failed'\n",
        "            }\n",
        "\n",
        "    # =================== PREDICTION PIPELINE ===================\n",
        "\n",
        "    def predict_window_size(self, feature_vector, sequence_3d):\n",
        "        \"\"\"\n",
        "        Main entrypoint:\n",
        "          - Predict window W_t\n",
        "          - Evaluate forecast error e_t\n",
        "          - Compute FDS (S_t) and WSS (Z_t)\n",
        "          - Update drift/anomaly logic (FDI, WDI, events)\n",
        "          - Return full metrics packet\n",
        "        \"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'error': \"Model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            pred_raw = self.model.predict(feature_vector, verbose=0)\n",
        "\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = int(round(self.transformer.inverse_transform(pred_raw)[0, 0]))\n",
        "            else:\n",
        "                predicted_window = int(round(pred_raw[0, 0]))\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # WINDOW CLAMP ‚Äî HARD SAFETY FIX\n",
        "            # ----------------------------------------\n",
        "            # Prevent negative, zero, or extreme window sizes\n",
        "            predicted_window = max(2, predicted_window)        # lower bound\n",
        "\n",
        "            # Forecast evaluation\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            fds = None\n",
        "            wss = None\n",
        "\n",
        "            if forecast_metrics.get(\"forecast_success\", False):\n",
        "                mse = forecast_metrics[\"mse\"]\n",
        "                mae = forecast_metrics[\"mae\"]\n",
        "\n",
        "                # Basic stats\n",
        "                self.mse_history.append(mse)\n",
        "                self.mae_history.append(mae)\n",
        "                self.error_memory.append(mse)\n",
        "\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = float(np.mean(self.mse_history))\n",
        "                self.performance_stats['avg_mae'] = float(np.mean(self.mae_history))\n",
        "\n",
        "                # ---------- Forecast Deviation Score (FDS) ----------\n",
        "                baseline_median = self.rolling_stats[\"median\"]\n",
        "                baseline_mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "                fds = (mse - baseline_median) / (baseline_mad + 1e-8)\n",
        "                fds = float(fds) if fds is not None and not np.isnan(fds) else 0.0\n",
        "\n",
        "\n",
        "                self.last_fds = fds\n",
        "                self.fds_memory.append(fds)\n",
        "                self.recent_fds.append(fds)\n",
        "\n",
        "                # ---------- Window Shift Score (WSS) ----------\n",
        "                if self.baseline_windows is not None and self.window_std > 0:\n",
        "                    wss = (predicted_window - self.window_mean) / (self.window_std + 1e-8)\n",
        "                else:\n",
        "                    wss = 0.0\n",
        "\n",
        "                wss = float(wss)\n",
        "                self.last_wss = wss\n",
        "                self.window_memory.append(predicted_window)\n",
        "                self.recent_windows.append(predicted_window)\n",
        "\n",
        "            # Event (ANOMALY / DRIFT) + Drift indices\n",
        "            event, sev, fdi, wdi = self._check_for_event()\n",
        "            self.last_fdi = fdi\n",
        "            self.last_wdi = wdi\n",
        "\n",
        "            # Save history for reporting\n",
        "            record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'wss': self.last_wss,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev\n",
        "            }\n",
        "            self.prediction_history.append(record)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wss': self.last_wss,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev,\n",
        "                'performance_stats': self.get_recent_performance()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'error': str(e)}\n",
        "\n",
        "    # =================== EVENT LOGIC (ANOMALY + DRIFT) ===================\n",
        "\n",
        "    def _check_for_event(self):\n",
        "        \"\"\"\n",
        "        Event detection for the Adaptive Window Agent.\n",
        "\n",
        "        - ANOMALY: deviation of last MSE from baseline (median + k * MAD)\n",
        "        - DRIFT:\n",
        "            * FDI: JSD between recent FDS distribution and baseline FDS\n",
        "            * WDI: JSD between recent window distribution and baseline window distribution\n",
        "        \"\"\"\n",
        "        # Require enough history\n",
        "        if len(self.error_memory) < 30:\n",
        "            return None, 0.0, None, None\n",
        "\n",
        "        last_mse = float(self.error_memory[-1])\n",
        "        live_errors = np.array(self.error_memory)\n",
        "\n",
        "        # ---------- BASELINE STATS ----------\n",
        "        if self.baseline_errors is not None and len(self.baseline_errors) > 10:\n",
        "            base_errors = np.array(self.baseline_errors)\n",
        "            baseline_median = np.median(base_errors)\n",
        "            baseline_mad = np.median(np.abs(base_errors - baseline_median)) + 1e-8\n",
        "        else:\n",
        "            baseline_median = np.median(live_errors)\n",
        "            baseline_mad = np.median(np.abs(live_errors - baseline_median)) + 1e-8\n",
        "\n",
        "        # Update rolling_stats so other components can see latest baseline-ish values\n",
        "        self.rolling_stats[\"median\"] = baseline_median\n",
        "        self.rolling_stats[\"mad\"] = baseline_mad\n",
        "\n",
        "        # ---------- LIVE STATS ----------\n",
        "        live_median = np.median(live_errors)\n",
        "        live_mad = np.median(np.abs(live_errors - live_median)) + 1e-8\n",
        "\n",
        "        # ---------- ANOMALY THRESHOLD ----------\n",
        "        baseline_threshold = baseline_median + self.threshold_k * baseline_mad\n",
        "        live_threshold = live_median + self.threshold_k * live_mad\n",
        "\n",
        "        anomaly_threshold = 0.8 * baseline_threshold + 0.2 * live_threshold\n",
        "\n",
        "        is_anomaly = last_mse > anomaly_threshold\n",
        "\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            is_anomaly = False\n",
        "        elif is_anomaly:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "\n",
        "        if is_anomaly:\n",
        "            severity = (last_mse - anomaly_threshold) / (baseline_mad + 1e-6)\n",
        "            severity = float(severity)\n",
        "            self.performance_stats[\"anomaly_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[ANOMALY] mse={last_mse:.6f}, thr={anomaly_threshold:.6f}, sev={severity:.3f}\")\n",
        "            return \"ANOMALY\", severity, 0.0, 0.0\n",
        "\n",
        "        # ---------- DRIFT (FDI + WDI) ----------\n",
        "        fdi = None\n",
        "        wdi = None\n",
        "\n",
        "        # FDI: JSD over FDS distribution\n",
        "        if self.baseline_fds is not None and len(self.recent_fds) >= 30:\n",
        "            base_fds = np.asarray(self.baseline_fds)\n",
        "            recent_fds = np.asarray(self.recent_fds)\n",
        "\n",
        "            hist_base, bins = np.histogram(base_fds, bins=25, density=True)\n",
        "            hist_recent, _ = np.histogram(recent_fds, bins=bins, density=True)\n",
        "\n",
        "            hist_base = hist_base / (hist_base.sum() + 1e-12)\n",
        "            hist_recent = hist_recent / (hist_recent.sum() + 1e-12)\n",
        "\n",
        "            fdi = float(jensenshannon(hist_base + 1e-12, hist_recent + 1e-12))\n",
        "\n",
        "        # WDI: JSD over window-size distribution\n",
        "        if self.baseline_windows is not None and len(self.recent_windows) >= 30:\n",
        "            base_win = np.asarray(self.baseline_windows)\n",
        "            recent_win = np.asarray(self.recent_windows)\n",
        "\n",
        "            hist_w_base, bins_w = np.histogram(base_win, bins=20, density=True)\n",
        "            hist_w_recent, _ = np.histogram(recent_win, bins=bins_w, density=True)\n",
        "\n",
        "            hist_w_base = hist_w_base / (hist_w_base.sum() + 1e-12)\n",
        "            hist_w_recent = hist_w_recent / (hist_w_recent.sum() + 1e-12)\n",
        "\n",
        "            wdi = float(jensenshannon(hist_w_base + 1e-12, hist_w_recent + 1e-12))\n",
        "\n",
        "        # Decide drift if either index is high\n",
        "        is_drift_fdi = fdi is not None and fdi > self.drift_threshold\n",
        "        is_drift_wdi = wdi is not None and wdi > self.window_drift_threshold\n",
        "\n",
        "        is_drift = is_drift_fdi or is_drift_wdi\n",
        "\n",
        "        if self.drift_cooldown > 0:\n",
        "            self.drift_cooldown -= 1\n",
        "            is_drift = False\n",
        "        else:\n",
        "            if is_drift:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "        if self.consecutive_drift_votes >= self.drift_votes_required:\n",
        "            self.consecutive_drift_votes = 0\n",
        "            self.drift_cooldown = self.drift_cooldown_steps\n",
        "            self.performance_stats[\"drift_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[DRIFT] FDI={fdi:.4f} WDI={wdi:.4f}\")\n",
        "            fdi = float(fdi) if fdi is not None else 0.0\n",
        "            wdi = float(wdi) if wdi is not None else 0.0\n",
        "            return \"DRIFT\", fdi, fdi, wdi\n",
        "\n",
        "        # Make safe for printing\n",
        "        fdi = float(fdi) if fdi is not None else 0.0\n",
        "        wdi = float(wdi) if wdi is not None else 0.0\n",
        "\n",
        "        return None, 0.0, fdi, wdi\n",
        "\n",
        "\n",
        "    # =================== HELPERS ===================\n",
        "\n",
        "    def get_recent_performance(self):\n",
        "        all_preds = list(self.prediction_history)\n",
        "\n",
        "        successful_predictions = [\n",
        "            p for p in all_preds\n",
        "            if p.get('forecast_metrics', {}).get('forecast_success', False)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'total_predictions': len(all_preds),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(all_preds), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': float(np.mean(list(self.mse_history)[-10:])) if self.mse_history else 0,\n",
        "            'avg_mse': float(np.mean(self.mse_history)) if self.mse_history else 0,\n",
        "            'recent_mae': float(np.mean(list(self.mae_history)[-10:])) if self.mae_history else 0,\n",
        "            'avg_mae': float(np.mean(self.mae_history)) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted,\n",
        "            'last_fdi': self.last_fdi,\n",
        "            'last_wdi': self.last_wdi,\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save performance statistics + prediction history to JSON\"\"\"\n",
        "        try:\n",
        "            state = {\n",
        "                'performance_stats': self.performance_stats.copy(),\n",
        "                'prediction_history': list(self.prediction_history)[-100:],\n",
        "                'mse_history': list(self.mse_history),\n",
        "                'mae_history': list(self.mae_history),\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, indent=2, default=str)\n",
        "            print(f\"‚úÖ Performance state saved to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to save performance state: {e}\")\n",
        "\n",
        "\n",
        "#===============================================================================================================================================\n",
        "###  SENSOR AGENTS - INDIVIDUAL AND MASTER\n",
        "#--------------------------------------==========================================================================================================\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 10, #K\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1,\n",
        "                warmup_steps: int = 100):    # <‚îÄ‚îÄ NEW PARAM\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "            'mean': 0.0,   # backward compatibility\n",
        "            'std': 1.0,    # backward compatibility\n",
        "            'q95': 0.0,\n",
        "            'q99': 0.0\n",
        "        }\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.drift_cooldown = 0\n",
        "\n",
        "        self.anomaly_cooldown_steps = 5    # you can tune\n",
        "        self.drift_cooldown_steps = 10     # you can tune\n",
        "\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.consecutive_anomaly_votes = 0\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Correct metadata file\n",
        "                metadata_path = model_path.replace('_model.h5', '_metadata.pkl')\n",
        "\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "\n",
        "                    baseline = metadata.get('baseline_stats', None)\n",
        "\n",
        "                    if baseline is not None:\n",
        "                        # Initialize rolling stats from training\n",
        "                        # Load robust baseline stats\n",
        "                        self.rolling_stats['median'] = baseline.get('median')\n",
        "                        self.rolling_stats['mad']    = baseline.get('mad')\n",
        "\n",
        "                        # Backward compatibility for other parts of system\n",
        "                        self.rolling_stats['mean'] = self.rolling_stats['median']\n",
        "                        self.rolling_stats['std']  = self.rolling_stats['mad']\n",
        "\n",
        "                        self.rolling_stats['q95']  = baseline['q95']\n",
        "                        self.rolling_stats['q99']  = baseline['q99']\n",
        "\n",
        "                        # Save baseline distribution for drift detection\n",
        "                        self.baseline_errors = np.array(baseline['baseline_errors'])\n",
        "\n",
        "                # AE was trained on raw, NOT scaled\n",
        "                self.scaler = None\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format ‚Äì expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"‚úÖ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3\n",
        "\n",
        "        # --------------- WARM-UP PHASE -----------------\n",
        "        # During warm-up, rolling stats ignore live data and stay fixed\n",
        "        if self.total_processed < self.warmup_steps:\n",
        "            med = np.median(self.baseline_errors)\n",
        "            mad = np.median(np.abs(self.baseline_errors - med)) + 1e-8\n",
        "\n",
        "            self.rolling_stats['median'] = med\n",
        "            self.rolling_stats['mad'] = mad\n",
        "            self.rolling_stats['mean'] = med     # backward compatibility\n",
        "            self.rolling_stats['std'] = mad\n",
        "        else:\n",
        "            # After warm-up, rolling stats evolve normally\n",
        "            if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "                self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['median'] + self.threshold_k * self.rolling_stats['mad']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model on RAW values.\"\"\"\n",
        "        try:\n",
        "            # Ensure shape: [1, window_length, 1]\n",
        "            X = subsequence.reshape(1, self.window_length, 1)\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "\n",
        "            error = mean_squared_error(\n",
        "                subsequence.flatten(),\n",
        "                reconstruction.flatten()\n",
        "            )\n",
        "            return max(0.0, error)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            # Fallback: variance of raw subsequence\n",
        "            return float(np.var(subsequence))\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "\n",
        "        median = np.median(errors_array)\n",
        "        mad = np.median(np.abs(errors_array - median)) + 1e-8  # avoid zero\n",
        "\n",
        "        # Store\n",
        "        self.rolling_stats['median'] = median\n",
        "        self.rolling_stats['mad'] = mad\n",
        "\n",
        "        # Backward compatibility fields (for plotting)\n",
        "        self.rolling_stats['mean'] = median\n",
        "        self.rolling_stats['std'] = mad\n",
        "\n",
        "        # Percentile bands (unchanged; good for drift & visualization)\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        median = self.rolling_stats.get('median', self.rolling_stats['mean'])\n",
        "        mad = self.rolling_stats.get('mad', self.rolling_stats['std'])\n",
        "        threshold = median + self.threshold_k * mad\n",
        "        is_anomaly_now = score > threshold\n",
        "\n",
        "        # Cooldown active ‚Üí suppress anomaly\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            return False\n",
        "\n",
        "        # No cooldown and anomaly happened ‚Üí activate cooldown\n",
        "        if is_anomaly_now:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            is_drift_now = js_divergence > self.drift_threshold\n",
        "\n",
        "            # Cooldown active ‚Üí suppress\n",
        "            if self.drift_cooldown > 0:\n",
        "                self.drift_cooldown -= 1\n",
        "                return False\n",
        "\n",
        "            # Multi-step confirmation: require 3 drift votes in last few steps\n",
        "            if is_drift_now:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "            if self.consecutive_drift_votes >= 3:\n",
        "                self.drift_cooldown = self.drift_cooldown_steps\n",
        "                self.consecutive_drift_votes = 0\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        median = self.rolling_stats.get('median')\n",
        "        mad = self.rolling_stats.get('mad')\n",
        "\n",
        "        if mad == 0:\n",
        "            return 0.5\n",
        "\n",
        "        threshold = median + self.threshold_k * mad\n",
        "\n",
        "        z = (score - threshold) / mad  # how far beyond threshold?\n",
        "\n",
        "        # Smooth probability-like mapping\n",
        "        confidence = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        return float(np.clip(confidence, 0.0, 1.0))\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str, win_length: int, warmup_steps: int = 100) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"üöÄ Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=win_length,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1,\n",
        "                                  warmup_steps=warmup_steps)       # <‚îÄ‚îÄ NEW\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"‚úÖ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "\n",
        "    return sensor_agents, master\n",
        "\n",
        "\n",
        "\n",
        "#=========================================================================================================================================\n",
        "#=================  MAIN PERCEPTION LAYER PRECOMPUTATION CODE========================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#############################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 1. LOAD DATA + MASK + LABELS\n",
        "    # -------------------------------------------------\n",
        "data_path       = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy\"\n",
        "label_path      = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/window_labels_3class.npy\"\n",
        "train_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/train_mask.npy\"\n",
        "test_mask_path  = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/test_mask.npy\"\n",
        "holdout_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/holdout_mask.npy\"\n",
        "\n",
        "X = np.load(data_path)        # (N, W, S)\n",
        "y = np.load(label_path)       # (N,) ‚àà {0,1,2}\n",
        "\n",
        "train_mask   = np.load(train_mask_path).astype(bool)\n",
        "test_mask    = np.load(test_mask_path).astype(bool)\n",
        "holdout_mask = np.load(holdout_mask_path).astype(bool)\n",
        "\n",
        "# Sanity check (VERY important) -- This assertion enforces strict mutual exclusivity between evaluation and holdout subsets, preventing data leakage and ensuring unbiased performance estimation.\n",
        "assert not np.any(train_mask & test_mask)\n",
        "assert not np.any(train_mask & holdout_mask)\n",
        "assert not np.any(test_mask & holdout_mask)\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "X_hold,  y_hold  = X[holdout_mask], y[holdout_mask]\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "print(\"Hold :\", X_hold.shape)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# -------------TRansformer for 3-class classification - NO CONTRASTIVE LEARNING\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "MODEL_DIR  = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class\"\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, \"transformer_classifier.keras\")\n",
        "\n",
        "\n",
        "def transformer_encoder(x, head_size, num_heads, ff_dim, dropout=0.2):\n",
        "    # --- Self-attention block ---\n",
        "    attn_output = layers.MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout\n",
        "    )(x, x)\n",
        "\n",
        "    attn_output = layers.Dropout(dropout)(attn_output)\n",
        "    x = layers.Add()([x, attn_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # --- Feed-forward block ---\n",
        "    ff_output = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff_output = layers.Dropout(dropout)(ff_output)\n",
        "    ff_output = layers.Dense(x.shape[-1])(ff_output)\n",
        "\n",
        "    x = layers.Add()([x, ff_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_transformer(window_len, n_features, n_classes=3):\n",
        "    inputs = keras.Input(shape=(window_len, n_features))\n",
        "\n",
        "    # Project features to model dimension\n",
        "    x = layers.Dense(64)(inputs)\n",
        "\n",
        "    # Transformer blocks\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "\n",
        "    # Pooling + classifier\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "n_classes = 3\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(\"‚úÖ Found pretrained Transformer. Loading...\")\n",
        "\n",
        "    classifier = load_model(MODEL_PATH)\n",
        "    classifier.trainable = False\n",
        "\n",
        "else:\n",
        "    print(\"üöÄ No pretrained model found. Training Transformer...\")\n",
        "\n",
        "    window_len  = X_train.shape[1]\n",
        "    n_features = X_train.shape[2]\n",
        "\n",
        "    classifier = build_transformer(window_len, n_features, n_classes)\n",
        "\n",
        "    classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(k=2, name=\"top2_acc\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Class weights (IMBALANCE HANDLING)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    class_weight = {\n",
        "        i: w for i, w in zip(np.unique(y_train), class_weights)\n",
        "    }\n",
        "\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=50,\n",
        "        batch_size=256,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "    classifier.save(MODEL_PATH, include_optimizer=False)\n",
        "\n",
        "    print(\"‚úÖ Transformer trained and saved.\")\n",
        "\n",
        "\n",
        "\n",
        "####-----------------------------Load and test-----------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        modulating_factor = K.pow(1.0 - p_t, gamma)\n",
        "\n",
        "        return K.mean(alpha_factor * modulating_factor * bce)\n",
        "    return loss\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class/transformer_classifier.keras\"\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "classifier = load_model(\n",
        "    model_path,\n",
        "    custom_objects={\"focal_loss\": focal_loss}  # only if you actually used it\n",
        ")\n",
        "\n",
        "classifier.trainable = False  # safety\n",
        "\n",
        "\n",
        "print(\"‚úÖ Transformer loaded for TEST\")\n",
        "probs_test = classifier.predict(X_test)\n",
        "y_pred_test = np.argmax(probs_test, axis=1)\n",
        "\n",
        "print(\"\\n=== TEST SET PERFORMANCE ===\")\n",
        "print(classification_report(y_test, y_pred_test, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "###-------------------------HOLDOUT-------------------\n",
        "############################################################\n",
        "probs_hold = classifier.predict(X_hold)\n",
        "y_pred_hold = np.argmax(probs_hold, axis=1)\n",
        "\n",
        "print(\"\\n=== HOLDOUT SET PERFORMANCE (FINAL) ===\")\n",
        "print(classification_report(y_hold, y_pred_hold, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_hold, y_pred_hold))\n",
        "\n",
        "\n",
        "\n",
        "def transformer_signals(classifier, X_window):\n",
        "    probs = classifier.predict(X_window, verbose=0)[0]  # shape (3,)\n",
        "\n",
        "    p0, p1, p2 = probs\n",
        "\n",
        "    return {\n",
        "        \"p_normal\": float(p0),\n",
        "        \"early_fault_score\": float(p1),\n",
        "        \"severe_fault_score\": float(p2),\n",
        "        \"anomaly_score\": float(1.0 - p0)\n",
        "    }\n",
        "\n",
        "signals = transformer_signals(classifier, X_hold) #Get actual prob distribution now\n",
        "print(signals)\n",
        "\n",
        "# ============================================================\n",
        "# HOLDOUT PERCEPTION PRECOMPUTE (CHECKPOINTED + RESUMABLE)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def json_safe(obj):\n",
        "    import numpy as np\n",
        "    from collections import deque\n",
        "\n",
        "    if isinstance(obj, (np.integer,)):\n",
        "        return int(obj)\n",
        "    if isinstance(obj, (np.floating,)):\n",
        "        return float(obj)\n",
        "    if isinstance(obj, (np.ndarray,)):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, deque):\n",
        "        return list(obj)\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: json_safe(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list):\n",
        "        return [json_safe(v) for v in obj]\n",
        "    if isinstance(obj, tuple):\n",
        "        return [json_safe(v) for v in obj]\n",
        "    if obj is None or isinstance(obj, (str, int, float, bool)):\n",
        "        return obj\n",
        "\n",
        "    # last resort (custom objects)\n",
        "    return str(obj)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CONFIG\n",
        "# --------------------------------------------------\n",
        "CACHE_PATH = \"/content/drive/MyDrive/PHD/2025/cache/holdout_precomp.json\"\n",
        "TMP_PATH   = CACHE_PATH + \".tmp\"\n",
        "\n",
        "SAVE_EVERY   = 250          # checkpoint frequency\n",
        "MAX_SAMPLES  = 2000         # None = full holdout, or set to 5000, 10000, etc.\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ASSUMED IN MEMORY\n",
        "# --------------------------------------------------\n",
        "# X_hold, y_hold\n",
        "# classifier\n",
        "# create_robust_system\n",
        "# AdaptiveWindowAgent\n",
        "\n",
        "models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# RESUME OR INIT\n",
        "# --------------------------------------------------\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    print(\"‚ôªÔ∏è Resuming existing HOLDOUT precompute...\")\n",
        "    with open(CACHE_PATH, \"r\") as f:\n",
        "        PRECOMP = json.load(f)\n",
        "\n",
        "    done_indices = {item[\"index\"] for item in PRECOMP}\n",
        "    start_idx = max(done_indices) + 1 if done_indices else 0\n",
        "else:\n",
        "    print(\"üÜï Starting fresh HOLDOUT precompute...\")\n",
        "    PRECOMP = []\n",
        "    done_indices = set()\n",
        "    start_idx = 0\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# LIMIT\n",
        "# --------------------------------------------------\n",
        "N_TOTAL = len(X_hold)\n",
        "END_IDX = N_TOTAL if MAX_SAMPLES is None else min(N_TOTAL, MAX_SAMPLES)\n",
        "\n",
        "print(f\"‚û°Ô∏è Starting at index {start_idx}, ending at {END_IDX}\")\n",
        "print(f\"üì¶ Already cached: {len(PRECOMP)} samples\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# BUILD PERCEPTION AGENTS ONCE\n",
        "# --------------------------------------------------\n",
        "num_sensors = X_hold.shape[2]\n",
        "win_length  = X_hold.shape[1]\n",
        "\n",
        "sensor_agents, master = create_robust_system(\n",
        "    num_sensors=num_sensors,\n",
        "    models_dir=models_dir,\n",
        "    win_length=win_length,\n",
        "    warmup_steps=100,\n",
        ")\n",
        "\n",
        "window_agent = AdaptiveWindowAgent(\n",
        "    model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        ")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MAIN LOOP\n",
        "# --------------------------------------------------\n",
        "print(\"\\n‚è≥ Running HOLDOUT perception precompute...\\n\")\n",
        "\n",
        "for i in tqdm(range(start_idx, END_IDX)):\n",
        "    if i in done_indices:\n",
        "        continue\n",
        "\n",
        "    seq = X_hold[i]\n",
        "\n",
        "    try:\n",
        "        # -------------------------\n",
        "        # Perception\n",
        "        # -------------------------\n",
        "        master_out = master.process_system_input(seq)\n",
        "        window_out = window_agent.predict_window_size(seq.flatten(), seq)\n",
        "\n",
        "        probs = classifier.predict(seq[np.newaxis, ...], verbose=0)[0]\n",
        "        p0, p1, p2 = probs\n",
        "\n",
        "        PRECOMP.append({\n",
        "            \"index\": i,\n",
        "            \"y_true\": int(y_hold[i]),\n",
        "            \"master\": master_out,\n",
        "            \"window\": window_out,\n",
        "            \"model_outputs\": {\n",
        "                \"p_normal\": float(p0),\n",
        "                \"p_warn\": float(p1),\n",
        "                \"p_fault\": float(p2),\n",
        "                \"argmax\": int(np.argmax(probs)),\n",
        "            }\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed at index {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # -------------------------\n",
        "    # CHECKPOINT\n",
        "    # -------------------------\n",
        "    if (i + 1) % SAVE_EVERY == 0:\n",
        "        with open(TMP_PATH, \"w\") as f:\n",
        "            json.dump(json_safe(PRECOMP), f)\n",
        "\n",
        "\n",
        "        os.replace(TMP_PATH, CACHE_PATH)\n",
        "        print(f\"üíæ Checkpoint saved at {i+1}/{END_IDX}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# FINAL SAVE\n",
        "# --------------------------------------------------\n",
        "with open(TMP_PATH, \"w\") as f:\n",
        "    json.dump(PRECOMP, f)\n",
        "os.replace(TMP_PATH, CACHE_PATH)\n",
        "\n",
        "print(\"\\n‚úÖ HOLDOUT precompute complete\")\n",
        "print(f\"üì¶ Total cached samples: {len(PRECOMP)}\")\n",
        "print(f\"üìÅ Saved to: {CACHE_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-REVISED-DecisionExpertLLM.ipynb",
      "authorship_tag": "ABX9TyNp47gampNO/u+tRXTaqEE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}