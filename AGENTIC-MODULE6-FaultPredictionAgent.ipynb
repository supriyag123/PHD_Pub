{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-FaultPredictionAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed846ae4-f259-42fe-8754-119d77bc8008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2901, 50, 12) (2901,)\n",
            "Val shape: (726, 50, 12) (726,)\n",
            "Class weights: {np.int64(0): np.float64(0.5125441696113074), np.int64(1): np.float64(20.429577464788732)}\n",
            "Epoch 1/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 669ms/step - accuracy: 0.8704 - loss: 0.0622 - precision: 0.0794 - recall: 0.4089 - val_accuracy: 0.9325 - val_loss: 0.0408 - val_precision: 0.1702 - val_recall: 0.4444 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9442 - loss: 0.0451 - precision: 0.2169 - recall: 0.5220 - val_accuracy: 0.9284 - val_loss: 0.0339 - val_precision: 0.1852 - val_recall: 0.5556 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9403 - loss: 0.0428 - precision: 0.2579 - recall: 0.6432 - val_accuracy: 0.9063 - val_loss: 0.0334 - val_precision: 0.1711 - val_recall: 0.7222 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9348 - loss: 0.0331 - precision: 0.2942 - recall: 0.8725 - val_accuracy: 0.9118 - val_loss: 0.0262 - val_precision: 0.2125 - val_recall: 0.9444 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9340 - loss: 0.0234 - precision: 0.2414 - recall: 0.8633 - val_accuracy: 0.8967 - val_loss: 0.0365 - val_precision: 0.1935 - val_recall: 1.0000 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9215 - loss: 0.0231 - precision: 0.2192 - recall: 0.9206 - val_accuracy: 0.8926 - val_loss: 0.0393 - val_precision: 0.1875 - val_recall: 1.0000 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9143 - loss: 0.0236 - precision: 0.2185 - recall: 0.9598 - val_accuracy: 0.9146 - val_loss: 0.0272 - val_precision: 0.2179 - val_recall: 0.9444 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9219 - loss: 0.0216 - precision: 0.2401 - recall: 0.9604 - val_accuracy: 0.9242 - val_loss: 0.0235 - val_precision: 0.2394 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9269 - loss: 0.0196 - precision: 0.2557 - recall: 0.9515 - val_accuracy: 0.9146 - val_loss: 0.0245 - val_precision: 0.2179 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9285 - loss: 0.0188 - precision: 0.2831 - recall: 0.9934 - val_accuracy: 0.9160 - val_loss: 0.0232 - val_precision: 0.2208 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9307 - loss: 0.0188 - precision: 0.2436 - recall: 0.9531 - val_accuracy: 0.9146 - val_loss: 0.0255 - val_precision: 0.2179 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9312 - loss: 0.0171 - precision: 0.2712 - recall: 0.9577 - val_accuracy: 0.9229 - val_loss: 0.0219 - val_precision: 0.2361 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9345 - loss: 0.0158 - precision: 0.2344 - recall: 0.9843 - val_accuracy: 0.9215 - val_loss: 0.0239 - val_precision: 0.2329 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9392 - loss: 0.0150 - precision: 0.2715 - recall: 0.9800 - val_accuracy: 0.9284 - val_loss: 0.0302 - val_precision: 0.2500 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9343 - loss: 0.0178 - precision: 0.2641 - recall: 0.9476 - val_accuracy: 0.9215 - val_loss: 0.0238 - val_precision: 0.2329 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9360 - loss: 0.0167 - precision: 0.2818 - recall: 0.9930 - val_accuracy: 0.9366 - val_loss: 0.0174 - val_precision: 0.2742 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9440 - loss: 0.0151 - precision: 0.3054 - recall: 0.9825 - val_accuracy: 0.9504 - val_loss: 0.0162 - val_precision: 0.3269 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9521 - loss: 0.0152 - precision: 0.3171 - recall: 0.9104 - val_accuracy: 0.9284 - val_loss: 0.0232 - val_precision: 0.2500 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9507 - loss: 0.0140 - precision: 0.3482 - recall: 0.9763 - val_accuracy: 0.9545 - val_loss: 0.0161 - val_precision: 0.3469 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9515 - loss: 0.0157 - precision: 0.3209 - recall: 0.9274 - val_accuracy: 0.9421 - val_loss: 0.0201 - val_precision: 0.2931 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.0141 - precision: 0.3753 - recall: 0.9738 - val_accuracy: 0.9559 - val_loss: 0.0153 - val_precision: 0.3542 - val_recall: 0.9444 - learning_rate: 1.2500e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9559 - loss: 0.0130 - precision: 0.3213 - recall: 0.9388 - val_accuracy: 0.9421 - val_loss: 0.0211 - val_precision: 0.2931 - val_recall: 0.9444 - learning_rate: 1.2500e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9460 - loss: 0.0157 - precision: 0.3023 - recall: 0.9684 - val_accuracy: 0.9435 - val_loss: 0.0192 - val_precision: 0.2982 - val_recall: 0.9444 - learning_rate: 1.2500e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.0153 - precision: 0.3733 - recall: 0.9754 - val_accuracy: 0.9518 - val_loss: 0.0167 - val_precision: 0.3333 - val_recall: 0.9444 - learning_rate: 1.2500e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9556 - loss: 0.0170 - precision: 0.3587 - recall: 0.9654 - val_accuracy: 0.9518 - val_loss: 0.0179 - val_precision: 0.3333 - val_recall: 0.9444 - learning_rate: 6.2500e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9585 - loss: 0.0142 - precision: 0.3641 - recall: 0.9622 - val_accuracy: 0.9518 - val_loss: 0.0181 - val_precision: 0.3333 - val_recall: 0.9444 - learning_rate: 6.2500e-05\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step\n",
            "\n",
            "=== Transformer Evaluation ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9985    0.9562    0.9769       708\n",
            "           1     0.3542    0.9444    0.5152        18\n",
            "\n",
            "    accuracy                         0.9559       726\n",
            "   macro avg     0.6763    0.9503    0.7460       726\n",
            "weighted avg     0.9825    0.9559    0.9655       726\n",
            "\n",
            "[[677  31]\n",
            " [  1  17]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# Fault Classification Pipeline\n",
        "# ============================================================\n",
        "\n",
        "############PASTE ADAPTIVE WINDOW HERE - so everything is in one file - later, we can import as a package#####################\n",
        "\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "# =====================================================\n",
        "# AdaptiveWindowAgent (improved version)\n",
        "# =====================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, os, logging, datetime as dt\n",
        "from collections import deque\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Adaptive Window Agent:\n",
        "    - Predicts window size using MLP\n",
        "    - Evaluates forecast with VAR\n",
        "    - Monitors anomalies & drift with adaptive thresholds\n",
        "    - Outputs severity scores + suppresses redundant events\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id=\"adaptive_window_agent\",\n",
        "                 model_path=None, checkpoint_path=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # Core model\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Histories\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # Event detection params\n",
        "        self.drift_detection_window = 20\n",
        "        self.drift_threshold_mse = 1.5   # stricter\n",
        "        self.drift_threshold_mae = 1.5\n",
        "        self.consecutive_poor_predictions = 0\n",
        "        self.cooldown_counter = 0\n",
        "\n",
        "        # Stats\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        # Retraining buffers\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "\n",
        "    # ------------------- Model -------------------\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"✅ Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                # Try to load transformer\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                else:\n",
        "                    # Fit transformer from true window labels\n",
        "                    y_original = np.load(\n",
        "                        \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy\"\n",
        "                    )\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(\"⚠️ No transformer found, fitted a new one.\")\n",
        "            else:\n",
        "                print(f\"❌ Model not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {e}\")\n",
        "\n",
        "    # ------------------- Forecast Eval -------------------\n",
        "    def evaluate_forecast_performance(self, sequence_3d, predicted_window, n_future=1):\n",
        "        try:\n",
        "            df = pd.DataFrame(sequence_3d, columns=[f'V{i+1}' for i in range(sequence_3d.shape[1])])\n",
        "            df_train, df_test = df[:-n_future], df[-n_future:]\n",
        "\n",
        "            # Drop constant cols\n",
        "            constant_cols = [c for c in df_train.columns if df_train[c].nunique() <= 1]\n",
        "            df_train = df_train.drop(columns=constant_cols, errors=\"ignore\")\n",
        "            df_test = df_test.drop(columns=constant_cols, errors=\"ignore\")\n",
        "\n",
        "            # If too few variables, fall back immediately\n",
        "            if len(df_train.columns) < 1:\n",
        "                return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            k = min(predicted_window, len(df_train) - 2)\n",
        "            if k < 1: k = 1\n",
        "\n",
        "            # Try VAR\n",
        "            try:\n",
        "                model = VAR(df_train)\n",
        "                model_fitted = model.fit(maxlags=k, trend=\"c\")\n",
        "                forecast_input = df_train.values[-model_fitted.k_ar:]\n",
        "                fc = model_fitted.forecast(y=forecast_input, steps=n_future)\n",
        "                df_forecast = pd.DataFrame(fc, index=df.index[-n_future:], columns=df_train.columns)\n",
        "\n",
        "                actual = df_test[df_forecast.columns].values.flatten()\n",
        "                predicted = df_forecast.values.flatten()\n",
        "\n",
        "            except Exception:\n",
        "                # Try AutoReg\n",
        "                try:\n",
        "                    from statsmodels.tsa.ar_model import AutoReg\n",
        "                    col = df_train.columns[0]\n",
        "                    model = AutoReg(df_train[col], lags=min(k, len(df_train)//2)).fit()\n",
        "                    predicted = model.predict(start=len(df_train), end=len(df_train)+n_future-1).values\n",
        "                    actual = df_test[col].values\n",
        "                except Exception:\n",
        "                    # Fallback to persistence\n",
        "                    return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            mse = np.mean((actual - predicted) ** 2)\n",
        "            mae = np.mean(np.abs(actual - predicted))\n",
        "\n",
        "            # If forecast is unstable, fallback\n",
        "            if np.isnan(mse) or np.isnan(mae) or mse > 10 or mae > 10:\n",
        "                return self._persistence_forecast(df, df_test)\n",
        "\n",
        "            return {\n",
        "                'mse': float(mse),\n",
        "                'mae': float(mae),\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual.tolist(),\n",
        "                'predicted_values': predicted.tolist(),\n",
        "                'used_columns': list(df_test.columns)\n",
        "            }\n",
        "\n",
        "        except Exception:\n",
        "            return self._persistence_forecast(df, df_test)\n",
        "\n",
        "# ------------------- Persistence fallback -------------------\n",
        "\n",
        "    def _persistence_forecast(self, df, df_test):\n",
        "        \"\"\"Simple last-value-carried-forward forecast.\"\"\"\n",
        "        try:\n",
        "            last_values = df.iloc[-1].values\n",
        "            predicted = np.tile(last_values, (len(df_test), 1))\n",
        "            actual = df_test.values.flatten()\n",
        "\n",
        "            mse = np.mean((actual - predicted.flatten()) ** 2)\n",
        "            mae = np.mean(np.abs(actual - predicted.flatten()))\n",
        "\n",
        "            return {\n",
        "                'mse': float(mse),\n",
        "                'mae': float(mae),\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual.tolist(),\n",
        "                'predicted_values': predicted.flatten().tolist(),\n",
        "                'used_columns': list(df_test.columns),\n",
        "                'note': 'persistence_fallback'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'mse': 9999, 'mae': 9999, 'forecast_success': True, 'error': str(e), 'note': 'persistence_fallback_failed'}\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------- Prediction -------------------\n",
        "\n",
        "    def predict_window_size(self, feature_vector, sequence_3d):\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'error': \"Model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            pred_raw = self.model.predict(feature_vector, verbose=0)\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = int(round(self.transformer.inverse_transform(pred_raw)[0, 0]))\n",
        "            else:\n",
        "                predicted_window = int(round(pred_raw[0, 0]))\n",
        "\n",
        "            # Evaluate\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            if forecast_metrics.get(\"forecast_success\", False):\n",
        "                self.mse_history.append(forecast_metrics[\"mse\"])\n",
        "                self.mae_history.append(forecast_metrics[\"mae\"])\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = np.mean(self.mse_history)\n",
        "                self.performance_stats['avg_mae'] = np.mean(self.mae_history)\n",
        "\n",
        "            # Event check\n",
        "            event, sev = self._check_for_event()\n",
        "\n",
        "            # Save history\n",
        "            record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'event_type': event,\n",
        "                'severity': sev\n",
        "            }\n",
        "            self.prediction_history.append(record)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'event_type': event,\n",
        "                'severity': sev,\n",
        "                'performance_stats': self.get_recent_performance()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'error': str(e)}\n",
        "\n",
        "    # ------------------- Event Logic -------------------\n",
        "\n",
        "    def _check_for_event(self):\n",
        "        if len(self.mse_history) < self.drift_detection_window:\n",
        "            return None, 0.0\n",
        "\n",
        "        mse_vals = np.array(self.mse_history)[-self.drift_detection_window:]\n",
        "        mae_vals = np.array(self.mae_history)[-self.drift_detection_window:]\n",
        "\n",
        "        # Rolling stats\n",
        "        mean_mse, std_mse = np.mean(mse_vals), np.std(mse_vals) + 1e-8\n",
        "        last_mse = mse_vals[-1]\n",
        "\n",
        "        # Normalized error\n",
        "        norm_error = (last_mse - mean_mse) / std_mse\n",
        "\n",
        "        # Severity\n",
        "        anomaly_severity = max(0, norm_error)\n",
        "        drift_severity = max(0, (np.mean(mse_vals) / (np.median(mse_vals)+1e-5)) - 1)\n",
        "\n",
        "        # Check anomaly\n",
        "        if last_mse > mean_mse + 2.0 * std_mse:\n",
        "            self.performance_stats['anomaly_events'] += 1\n",
        "            return \"ANOMALY\", anomaly_severity\n",
        "\n",
        "        # Check drift (with persistence + cooldown)\n",
        "        ema_mse = 0.3*np.mean(mse_vals) + 0.7*np.median(mse_vals)\n",
        "        ema_mae = 0.3*np.mean(mae_vals) + 0.7*np.median(mae_vals)\n",
        "        mse_ratio = ema_mse / max(np.median(mse_vals), 1e-5)\n",
        "        mae_ratio = ema_mae / max(np.median(mae_vals), 1e-5)\n",
        "\n",
        "        if mse_ratio > self.drift_threshold_mse and mae_ratio > self.drift_threshold_mae:\n",
        "            self.consecutive_poor_predictions += 1\n",
        "            if self.consecutive_poor_predictions >= 5 and self.cooldown_counter == 0:\n",
        "                self.performance_stats['drift_events'] += 1\n",
        "                self.cooldown_counter = 10\n",
        "                return \"DRIFT\", drift_severity\n",
        "        else:\n",
        "            self.consecutive_poor_predictions = 0\n",
        "\n",
        "        if self.cooldown_counter > 0:\n",
        "            self.cooldown_counter -= 1\n",
        "\n",
        "        return None, 0.0\n",
        "\n",
        "    # ------------------- Helpers -------------------\n",
        "\n",
        "    def get_recent_performance(self):\n",
        "        successful_predictions = [\n",
        "            p for p in list(self.prediction_history)[-50:]\n",
        "            if p.get('forecast_metrics', {}).get('forecast_success', False)\n",
        "        ]\n",
        "        return {\n",
        "            'total_predictions': len(self.prediction_history),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(self.prediction_history), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': float(np.mean(list(self.mse_history)[-10:])) if self.mse_history else 0,\n",
        "            'avg_mse': float(np.mean(self.mse_history)) if self.mse_history else 0,\n",
        "            'recent_mae': float(np.mean(list(self.mae_history)[-10:])) if self.mae_history else 0,\n",
        "            'avg_mae': float(np.mean(self.mae_history)) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save performance statistics + prediction history to JSON\"\"\"\n",
        "        try:\n",
        "            state = {\n",
        "                'performance_stats': self.performance_stats.copy(),\n",
        "                'prediction_history': list(self.prediction_history)[-100:],  # last 100\n",
        "                'mse_history': list(self.mse_history),\n",
        "                'mae_history': list(self.mae_history),\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "            import json\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, indent=2, default=str)\n",
        "            print(f\"✅ Performance state saved to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save performance state: {e}\")\n",
        "\n",
        "\n",
        "#############################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load Data\n",
        "# ============================================================\n",
        "# Long subsequences (length=50) and labels\n",
        "long_sequences = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\")\n",
        "labels_detection = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_detection.npy\")  # fault labels\n",
        "labels_h1 = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H1.npy\")  # optional predictive labels\n",
        "\n",
        "print(\"Data loaded:\", long_sequences.shape, labels_detection.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Baseline Fixed Window Features (window=50, flattened)\n",
        "# ============================================================\n",
        "X_fixed = long_sequences.reshape(long_sequences.shape[0], -1)\n",
        "y = labels_detection  # detection labels (can switch to h1/h3/h12)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_fixed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# Baseline-1: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Baseline-1: Fixed Window Random Forest ===\")\n",
        "print(classification_report(y_test, y_pred_rf, digits=4))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "joblib.dump(rf, \"rf_fixed_window.pkl\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Dynamic Window Features (using your AdaptiveWindowAgent)\n",
        "# ============================================================\n",
        "#from agents.adaptive_window_agent import AdaptiveWindowAgent\n",
        "\n",
        "window_agent = AdaptiveWindowAgent(\n",
        "    model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        ")\n",
        "\n",
        "# Generate features for each subsequence using predicted window\n",
        "dynamic_features = []\n",
        "for seq in long_sequences:\n",
        "    features = seq.flatten()\n",
        "    result = window_agent.predict_window_size(features, seq)\n",
        "    w = result.get(\"predicted_window\", 50)\n",
        "    # Extract the last w timesteps from the long sequence\n",
        "    seq_dynamic = seq[-w:].flatten()\n",
        "    dynamic_features.append(seq_dynamic)\n",
        "\n",
        "# Pad to same length (use max window=50)\n",
        "X_dynamic = np.array([np.pad(f, (0, 50*seq.shape[1] - len(f))) for f in dynamic_features])\n",
        "\n",
        "X_train_dyn, X_test_dyn, y_train_dyn, y_test_dyn = train_test_split(X_dynamic, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Baseline-12: Random Forest + dynamic window\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_dyn, y_train_dyn)\n",
        "y_pred_rf_dyn = rf.predict(X_test_dyn)\n",
        "\n",
        "print(\"\\n=== Baseline-1: Fixed Window Random Forest ===\")\n",
        "print(classification_report(y_test_dyn, y_pred_rf_dyn, digits=4))\n",
        "print(confusion_matrix(y_test_dyn, y_pred_rf_dyn))\n",
        "joblib.dump(rf, \"rf_dynamic_window.pkl\")\n",
        "\n",
        "# Baseline-3: XGBoost fixed\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300, max_depth=8, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "xgb.fit(X_train_dyn, y_train_dyn)\n",
        "y_pred_xgb = xgb.predict(X_test_dyn)\n",
        "\n",
        "print(\"\\n=== Baseline-2: Dynamic Window XGBoost ===\")\n",
        "print(classification_report(y_test_dyn, y_pred_xgb, digits=4))\n",
        "print(confusion_matrix(y_test_dyn, y_pred_xgb))\n",
        "joblib.dump(xgb, \"xgb_dynamic_window.pkl\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Ready for Next Steps\n",
        "# ============================================================\n",
        "print(\"\\n✅ Baseline models trained and evaluated.\")\n",
        "print(\"Next: add Transformer-based sequence classifier, then coordinator features.\")\n",
        "\n",
        "\n",
        "\n",
        "# ========================================================================================\n",
        "# SOTA: Transformer (PatchTST-style) Classifier\n",
        "# =======================================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ====================\n",
        "# 1. Mixed Precision\n",
        "# ====================\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# ====================\n",
        "# 2. Define Transformer Block\n",
        "# ====================\n",
        "def transformer_block(inputs, head_size=32, num_heads=2, ff_dim=64, dropout=0.2):\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    out = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return out\n",
        "\n",
        "# ====================\n",
        "# 3. Build Model\n",
        "# ====================\n",
        "def build_transformer_classifier(input_shape, num_classes=1):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Smaller Transformer (2 layers)\n",
        "    x = transformer_block(inputs, head_size=32, num_heads=2, ff_dim=64, dropout=0.2)\n",
        "    x = transformer_block(x, head_size=32, num_heads=2, ff_dim=64, dropout=0.2)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    if num_classes == 1:\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# ====================\n",
        "# 4. Data Setup\n",
        "# ====================\n",
        "long_sequences = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\")\n",
        "labels_detection = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_detection.npy\")\n",
        "\n",
        "X = long_sequences\n",
        "y = labels_detection\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Val shape:\", X_val.shape, y_val.shape)\n",
        "\n",
        "# Class weights\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "class_weight_dict = {i: w for i, w in zip(classes, class_weights)}\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# ====================\n",
        "# 5. Compile + Train\n",
        "# ====================\n",
        "\n",
        "# Focal Loss\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        modulating = (1 - p_t) ** gamma\n",
        "        return K.mean(alpha_factor * modulating * bce, axis=-1)\n",
        "    return loss\n",
        "\n",
        "model = build_transformer_classifier(input_shape=X_train.shape[1:], num_classes=1)\n",
        "\n",
        "# ✅ Compile properly\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=focal_loss(gamma=2, alpha=0.25),\n",
        "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\")]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "lr_sched = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stop, lr_sched],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ====================\n",
        "# 6. Evaluate\n",
        "# ====================\n",
        "y_probs = model.predict(X_val).flatten()\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n=== Transformer Evaluation ===\")\n",
        "print(classification_report(y_val, y_pred, digits=4))\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "\n",
        "\n",
        "#######################################################################################################\n",
        "#Validate with coordination agent###############################\n",
        "#######################################################################################################\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def validate_with_coordinator(y_probs, y_true, coordinator_results, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Validate Transformer predictions against coordinator agent decisions.\n",
        "    \"\"\"\n",
        "    # Transformer raw predictions\n",
        "    y_pred_raw = (y_probs >= threshold).astype(int)\n",
        "\n",
        "    # Apply coordinator filter\n",
        "    y_pred_validated = []\n",
        "    for pred, coord in zip(y_pred_raw, coordinator_results):\n",
        "        coord_flag = coord[\"coordinator\"].get(\"final_anomaly\", False)\n",
        "        if pred == 1 and coord_flag:\n",
        "            y_pred_validated.append(1)   # confirmed\n",
        "        else:\n",
        "            y_pred_validated.append(0)   # suppress FP or keep 0\n",
        "\n",
        "    y_pred_validated = np.array(y_pred_validated)\n",
        "\n",
        "    # Reports\n",
        "    report_raw = classification_report(y_true, y_pred_raw, output_dict=True)\n",
        "    report_validated = classification_report(y_true, y_pred_validated, output_dict=True)\n",
        "\n",
        "    cm_raw = confusion_matrix(y_true, y_pred_raw)\n",
        "    cm_validated = confusion_matrix(y_true, y_pred_validated)\n",
        "\n",
        "    return {\n",
        "        \"report_raw\": report_raw,\n",
        "        \"report_validated\": report_validated,\n",
        "        \"confusion_matrix_raw\": cm_raw,\n",
        "        \"confusion_matrix_validated\": cm_validated,\n",
        "        \"raw_preds\": y_pred_raw,\n",
        "        \"validated_preds\": y_pred_validated\n",
        "    }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrices(cm_raw, cm_validated, labels=[0,1]):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    sns.heatmap(cm_raw, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
        "    axes[0].set_title(\"Transformer (Raw)\")\n",
        "    axes[0].set_xlabel(\"Predicted\")\n",
        "    axes[0].set_ylabel(\"True\")\n",
        "\n",
        "    sns.heatmap(cm_validated, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
        "    axes[1].set_title(\"Transformer + Coordinator\")\n",
        "    axes[1].set_xlabel(\"Predicted\")\n",
        "    axes[1].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# ==========================================\n",
        "# Generate Coordinator outputs for test set\n",
        "# ==========================================\n",
        "\n",
        "\n",
        "results = []\n",
        "coordinator_flags = []\n",
        "\n",
        "print(\"Running Coordinator over test set...\")\n",
        "\n",
        "for i, seq in enumerate(X_test_long):\n",
        "    try:\n",
        "        features = seq.flatten()\n",
        "\n",
        "        # run each agent\n",
        "        master_out = master.process_system_input(seq)\n",
        "        window_out = window_agent.predict_window_size(features, seq)\n",
        "        final = coordinator.fuse(master_out, window_out)\n",
        "\n",
        "        results.append({\n",
        "            \"master\": master_out,\n",
        "            \"window\": window_out,\n",
        "            \"coordinator\": final\n",
        "        })\n",
        "\n",
        "        # Store just the binary anomaly flag (1=anomaly, 0=normal)\n",
        "        coordinator_flags.append(1 if final[\"final_anomaly\"] else 0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Sample {i} failed: {e}\")\n",
        "        coordinator_flags.append(0)  # safe default\n",
        "\n",
        "coordinator_flags = np.array(coordinator_flags)\n",
        "print(f\"✅ Coordinator outputs generated: {coordinator_flags.shape}\")\n",
        "print(f\"Unique flags: {np.unique(coordinator_flags, return_counts=True)}\")\n",
        "\n",
        "\n",
        "\n",
        "validation_out = validate_with_coordinator(\n",
        "    y_probs=y_probs,\n",
        "    y_true=y_true,\n",
        "    coordinator_results=results,\n",
        "    threshold=0.5\n",
        ")\n",
        "\n",
        "print(\"=== Transformer (Raw) ===\")\n",
        "print(validation_out[\"report_raw\"])\n",
        "print(validation_out[\"confusion_matrix_raw\"])\n",
        "\n",
        "print(\"\\n=== Transformer + Coordinator (Validated) ===\")\n",
        "print(validation_out[\"report_validated\"])\n",
        "print(validation_out[\"confusion_matrix_validated\"])\n",
        "\n",
        "plot_confusion_matrices(\n",
        "    validation_out[\"confusion_matrix_raw\"],\n",
        "    validation_out[\"confusion_matrix_validated\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-FaultPredictionAgent.ipynb",
      "authorship_tag": "ABX9TyPTleO4BUbbH5ve0mWSJUTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}