{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Simple Sensor Pre-Training System\n",
        "=================================\n",
        "\n",
        "Trains LSTM Autoencoders for each sensor and saves models with baseline statistics.\n",
        "\n",
        "Usage:\n",
        "    python sensor_pretraining.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def build_lstm_autoencoder(window_length: int, latent_dim: int = 32) -> Model:\n",
        "    \"\"\"Build LSTM Autoencoder.\"\"\"\n",
        "    inputs = Input(shape=(window_length, 1))\n",
        "\n",
        "    # Encoder\n",
        "    encoded = LSTM(latent_dim, activation='relu', return_sequences=False)(inputs)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = RepeatVector(window_length)(encoded)\n",
        "    decoded = LSTM(latent_dim, activation='relu', return_sequences=True)(decoded)\n",
        "    outputs = TimeDistributed(Dense(1, activation='linear'))(decoded)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_sensor_model(sensor_data, sensor_id, base_path, window_length):\n",
        "    \"\"\"Train model for one sensor.\"\"\"\n",
        "\n",
        "    print(f\"\\nTraining sensor {sensor_id}...\")\n",
        "    print(f\"Data shape: {sensor_data.shape}\")\n",
        "\n",
        "    # Split data\n",
        "    n_samples = len(sensor_data)\n",
        "    n_train = int(0.8 * n_samples)\n",
        "\n",
        "    X_train = sensor_data[:n_train]\n",
        "    X_val = sensor_data[n_train:]\n",
        "\n",
        "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_lstm_autoencoder(window_length)\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, X_train,\n",
        "        validation_data=(X_val, X_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Compute baseline reconstruction errors on validation set\n",
        "    print(\"Computing baseline errors...\")\n",
        "    val_predictions = model.predict(X_val, verbose=0)\n",
        "    baseline_errors = []\n",
        "\n",
        "    for i in range(len(X_val)):\n",
        "        error = mean_squared_error(X_val[i].flatten(), val_predictions[i].flatten())\n",
        "        baseline_errors.append(error)\n",
        "\n",
        "    baseline_stats = {\n",
        "        'mean': float(np.mean(baseline_errors)),\n",
        "        'std': float(np.std(baseline_errors)) + 1e-8,\n",
        "        'q95': float(np.percentile(baseline_errors, 95)),\n",
        "        'q99': float(np.percentile(baseline_errors, 99)),\n",
        "        'baseline_errors': baseline_errors  # Store for drift detection\n",
        "    }\n",
        "\n",
        "    # Save model\n",
        "    sensor_dir = os.path.join(base_path, 'sensor', 'model')\n",
        "    os.makedirs(sensor_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(sensor_dir, f'sensor_{sensor_id}_model.h5')\n",
        "    metadata_path = os.path.join(sensor_dir, f'sensor_{sensor_id}_metadata.pkl')\n",
        "\n",
        "    model.save(model_path)\n",
        "\n",
        "    metadata = {\n",
        "        'sensor_id': sensor_id,\n",
        "        'window_length': window_length,\n",
        "        'baseline_stats': baseline_stats,\n",
        "        'trained_at': datetime.now(),\n",
        "        'epochs_trained': len(history.history['loss']),\n",
        "        'final_val_loss': float(history.history['val_loss'][-1])\n",
        "    }\n",
        "\n",
        "    with open(metadata_path, 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    print(f\"Saved: {model_path}\")\n",
        "    print(f\"Baseline error: {baseline_stats['mean']:.6f} Â± {baseline_stats['std']:.6f}\")\n",
        "\n",
        "    return baseline_stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy'\n",
        "    base_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    data = np.load(data_path)\n",
        "    print(f\"Original data shape: {data.shape}\")\n",
        "\n",
        "    # Remove last 1000 samples\n",
        "    training_data = data[:-1000]\n",
        "    print(f\"Training data shape: {training_data.shape}\")\n",
        "\n",
        "    batch_size, window_length, num_sensors = training_data.shape\n",
        "\n",
        "    print(f\"Training {num_sensors} sensors...\")\n",
        "\n",
        "    # Train each sensor\n",
        "    results = {}\n",
        "    for sensor_id in range(num_sensors):\n",
        "        # Extract data for this sensor: [batch, timestep, sensor_id]\n",
        "        sensor_data = training_data[:, :, sensor_id:sensor_id+1]  # Keep feature dim\n",
        "\n",
        "        try:\n",
        "            baseline_stats = train_sensor_model(sensor_data, sensor_id, base_path, window_length)\n",
        "            results[sensor_id] = {'success': True, 'baseline_stats': baseline_stats}\n",
        "        except Exception as e:\n",
        "            print(f\"Failed training sensor {sensor_id}: {e}\")\n",
        "            results[sensor_id] = {'success': False, 'error': str(e)}\n",
        "\n",
        "    # Summary\n",
        "    successful = sum(1 for r in results.values() if r['success'])\n",
        "    print(f\"\\nTraining complete: {successful}/{num_sensors} sensors successful\")\n",
        "\n",
        "    # Save summary\n",
        "    summary_path = os.path.join(base_path, 'sensor', 'model', 'training_summary.pkl')\n",
        "    with open(summary_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'results': results,\n",
        "            'data_shape': training_data.shape,\n",
        "            'timestamp': datetime.now()\n",
        "        }, f)\n",
        "\n",
        "    print(f\"Summary saved: {summary_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb",
      "authorship_tag": "ABX9TyPE94pw1tuj+mtt6WkuSuzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}