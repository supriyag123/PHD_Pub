{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Daily%20Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "outputId": "e15cef5a-c4c2-40a6-cc7f-5b9ab36d9e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5745 - mean_squared_error: 0.5745 - val_loss: 0.5982 - val_mean_squared_error: 0.5982\n",
            "Epoch 2/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 0.5599 - val_mean_squared_error: 0.5599\n",
            "Epoch 3/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5779 - mean_squared_error: 0.5779 - val_loss: 0.5611 - val_mean_squared_error: 0.5611\n",
            "Epoch 4/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5805 - mean_squared_error: 0.5805 - val_loss: 0.5794 - val_mean_squared_error: 0.5794\n",
            "Epoch 5/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5808 - mean_squared_error: 0.5808 - val_loss: 0.5818 - val_mean_squared_error: 0.5818\n",
            "Epoch 6/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5515 - val_mean_squared_error: 0.5515\n",
            "Epoch 7/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5786 - mean_squared_error: 0.5786 - val_loss: 0.6056 - val_mean_squared_error: 0.6056\n",
            "Epoch 8/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5792 - mean_squared_error: 0.5792 - val_loss: 0.5580 - val_mean_squared_error: 0.5580\n",
            "Epoch 9/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5725 - val_mean_squared_error: 0.5725\n",
            "Epoch 10/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5791 - mean_squared_error: 0.5791 - val_loss: 0.5526 - val_mean_squared_error: 0.5526\n",
            "Epoch 11/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 0.5508 - val_mean_squared_error: 0.5508\n",
            "Epoch 12/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5787 - mean_squared_error: 0.5787 - val_loss: 0.5641 - val_mean_squared_error: 0.5641\n",
            "Epoch 13/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5777 - mean_squared_error: 0.5777 - val_loss: 0.5804 - val_mean_squared_error: 0.5804\n",
            "Epoch 14/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5788 - mean_squared_error: 0.5788 - val_loss: 0.5582 - val_mean_squared_error: 0.5582\n",
            "Epoch 15/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5751 - mean_squared_error: 0.5751 - val_loss: 0.5710 - val_mean_squared_error: 0.5710\n",
            "Epoch 16/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5835 - mean_squared_error: 0.5835 - val_loss: 0.5708 - val_mean_squared_error: 0.5708\n",
            "Epoch 17/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5618 - val_mean_squared_error: 0.5618\n",
            "Epoch 18/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5783 - mean_squared_error: 0.5783 - val_loss: 0.5510 - val_mean_squared_error: 0.5510\n",
            "Epoch 19/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5747 - mean_squared_error: 0.5747 - val_loss: 0.5782 - val_mean_squared_error: 0.5782\n",
            "Epoch 20/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 0.5636 - val_mean_squared_error: 0.5636\n",
            "Epoch 21/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5786 - mean_squared_error: 0.5786 - val_loss: 0.6012 - val_mean_squared_error: 0.6012\n",
            "Epoch 22/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5757 - mean_squared_error: 0.5757 - val_loss: 0.5746 - val_mean_squared_error: 0.5746\n",
            "Epoch 23/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5811 - mean_squared_error: 0.5811 - val_loss: 0.5687 - val_mean_squared_error: 0.5687\n",
            "Epoch 24/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5829 - mean_squared_error: 0.5829 - val_loss: 0.6083 - val_mean_squared_error: 0.6083\n",
            "Epoch 25/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5764 - mean_squared_error: 0.5764 - val_loss: 0.5793 - val_mean_squared_error: 0.5793\n",
            "Epoch 26/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5770 - mean_squared_error: 0.5770 - val_loss: 0.5644 - val_mean_squared_error: 0.5644\n",
            "Epoch 27/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5441 - val_mean_squared_error: 0.5441\n",
            "Epoch 28/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5819 - mean_squared_error: 0.5819 - val_loss: 0.5786 - val_mean_squared_error: 0.5786\n",
            "Epoch 29/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 0.5574 - val_mean_squared_error: 0.5574\n",
            "Epoch 30/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5757 - mean_squared_error: 0.5757 - val_loss: 0.5779 - val_mean_squared_error: 0.5779\n",
            "Epoch 31/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5785 - mean_squared_error: 0.5785 - val_loss: 0.5870 - val_mean_squared_error: 0.5870\n",
            "Epoch 32/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5714 - val_mean_squared_error: 0.5714\n",
            "Epoch 33/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5767 - mean_squared_error: 0.5767 - val_loss: 0.5727 - val_mean_squared_error: 0.5727\n",
            "Epoch 34/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5790 - mean_squared_error: 0.5790 - val_loss: 0.6040 - val_mean_squared_error: 0.6040\n",
            "Epoch 35/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5748 - mean_squared_error: 0.5748 - val_loss: 0.5587 - val_mean_squared_error: 0.5587\n",
            "Epoch 36/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5792 - mean_squared_error: 0.5792 - val_loss: 0.5702 - val_mean_squared_error: 0.5702\n",
            "Epoch 37/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5691 - val_mean_squared_error: 0.5691\n",
            "Epoch 38/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5758 - mean_squared_error: 0.5758 - val_loss: 0.5650 - val_mean_squared_error: 0.5650\n",
            "Epoch 39/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5770 - mean_squared_error: 0.5770 - val_loss: 0.5649 - val_mean_squared_error: 0.5649\n",
            "Epoch 40/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 0.5860 - val_mean_squared_error: 0.5860\n",
            "Epoch 41/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5790 - mean_squared_error: 0.5790 - val_loss: 0.5755 - val_mean_squared_error: 0.5755\n",
            "Epoch 42/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5664 - val_mean_squared_error: 0.5664\n",
            "Epoch 43/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5780 - mean_squared_error: 0.5780 - val_loss: 0.5887 - val_mean_squared_error: 0.5887\n",
            "Epoch 44/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5741 - mean_squared_error: 0.5741 - val_loss: 0.5630 - val_mean_squared_error: 0.5630\n",
            "Epoch 45/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5782 - mean_squared_error: 0.5782 - val_loss: 0.5599 - val_mean_squared_error: 0.5599\n",
            "Epoch 46/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5782 - mean_squared_error: 0.5782 - val_loss: 0.5750 - val_mean_squared_error: 0.5750\n",
            "Epoch 47/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5815 - mean_squared_error: 0.5815 - val_loss: 0.5843 - val_mean_squared_error: 0.5843\n",
            "Epoch 48/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5777 - mean_squared_error: 0.5777 - val_loss: 0.5606 - val_mean_squared_error: 0.5606\n",
            "Epoch 49/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5790 - mean_squared_error: 0.5790 - val_loss: 0.5609 - val_mean_squared_error: 0.5609\n",
            "Epoch 50/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5841 - mean_squared_error: 0.5841 - val_loss: 0.5620 - val_mean_squared_error: 0.5620\n",
            "Epoch 51/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5764 - mean_squared_error: 0.5764 - val_loss: 0.5584 - val_mean_squared_error: 0.5584\n",
            "Epoch 52/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5782 - mean_squared_error: 0.5782 - val_loss: 0.5563 - val_mean_squared_error: 0.5563\n",
            "Epoch 53/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5769 - mean_squared_error: 0.5769 - val_loss: 0.5698 - val_mean_squared_error: 0.5698\n",
            "Epoch 54/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5785 - mean_squared_error: 0.5785 - val_loss: 0.5663 - val_mean_squared_error: 0.5663\n",
            "Epoch 55/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5771 - mean_squared_error: 0.5771 - val_loss: 0.5445 - val_mean_squared_error: 0.5445\n",
            "Epoch 56/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 0.5857 - val_mean_squared_error: 0.5857\n",
            "Epoch 57/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5774 - mean_squared_error: 0.5774 - val_loss: 0.5747 - val_mean_squared_error: 0.5747\n",
            "Epoch 58/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5751 - mean_squared_error: 0.5751 - val_loss: 0.5724 - val_mean_squared_error: 0.5724\n",
            "Epoch 59/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5752 - mean_squared_error: 0.5752 - val_loss: 0.5823 - val_mean_squared_error: 0.5823\n",
            "Epoch 60/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5719 - val_mean_squared_error: 0.5719\n",
            "Epoch 61/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.5573 - val_mean_squared_error: 0.5573\n",
            "Epoch 62/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5743 - mean_squared_error: 0.5743 - val_loss: 0.5507 - val_mean_squared_error: 0.5507\n",
            "Epoch 63/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5780 - mean_squared_error: 0.5780 - val_loss: 0.5717 - val_mean_squared_error: 0.5717\n",
            "Epoch 64/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 0.5667 - val_mean_squared_error: 0.5667\n",
            "Epoch 65/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5797 - mean_squared_error: 0.5797 - val_loss: 0.5736 - val_mean_squared_error: 0.5736\n",
            "Epoch 66/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5783 - mean_squared_error: 0.5783 - val_loss: 0.6030 - val_mean_squared_error: 0.6030\n",
            "Epoch 67/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5778 - mean_squared_error: 0.5778 - val_loss: 0.5577 - val_mean_squared_error: 0.5577\n",
            "Epoch 68/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5778 - mean_squared_error: 0.5778 - val_loss: 0.5607 - val_mean_squared_error: 0.5607\n",
            "Epoch 69/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5761 - mean_squared_error: 0.5761 - val_loss: 0.5769 - val_mean_squared_error: 0.5769\n",
            "Epoch 70/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5776 - mean_squared_error: 0.5776 - val_loss: 0.5690 - val_mean_squared_error: 0.5690\n",
            "Epoch 71/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5815 - mean_squared_error: 0.5815 - val_loss: 0.5612 - val_mean_squared_error: 0.5612\n",
            "Epoch 72/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5901 - val_mean_squared_error: 0.5901\n",
            "Epoch 73/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5766 - mean_squared_error: 0.5766 - val_loss: 0.5649 - val_mean_squared_error: 0.5649\n",
            "Epoch 74/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5987 - val_mean_squared_error: 0.5987\n",
            "Epoch 75/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5792 - mean_squared_error: 0.5792 - val_loss: 0.5817 - val_mean_squared_error: 0.5817\n",
            "Epoch 76/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5829 - val_mean_squared_error: 0.5829\n",
            "Epoch 77/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5785 - mean_squared_error: 0.5785 - val_loss: 0.5652 - val_mean_squared_error: 0.5652\n",
            "Epoch 78/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5739 - mean_squared_error: 0.5739 - val_loss: 0.5546 - val_mean_squared_error: 0.5546\n",
            "Epoch 79/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5747 - mean_squared_error: 0.5747 - val_loss: 0.5739 - val_mean_squared_error: 0.5739\n",
            "Epoch 80/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.5515 - val_mean_squared_error: 0.5515\n",
            "Epoch 81/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5786 - mean_squared_error: 0.5786 - val_loss: 0.5563 - val_mean_squared_error: 0.5563\n",
            "Epoch 82/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.5724 - val_mean_squared_error: 0.5724\n",
            "Epoch 83/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5846 - val_mean_squared_error: 0.5846\n",
            "Epoch 84/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 0.5734 - val_mean_squared_error: 0.5734\n",
            "Epoch 85/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5785 - mean_squared_error: 0.5785 - val_loss: 0.5789 - val_mean_squared_error: 0.5789\n",
            "Epoch 86/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5643 - val_mean_squared_error: 0.5643\n",
            "Epoch 87/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.5668 - val_mean_squared_error: 0.5668\n",
            "Epoch 88/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 0.5587 - val_mean_squared_error: 0.5587\n",
            "Epoch 89/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5744 - mean_squared_error: 0.5744 - val_loss: 0.6060 - val_mean_squared_error: 0.6060\n",
            "Epoch 90/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5736 - mean_squared_error: 0.5736 - val_loss: 0.6045 - val_mean_squared_error: 0.6045\n",
            "Epoch 91/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 0.5888 - val_mean_squared_error: 0.5888\n",
            "Epoch 92/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.5699 - val_mean_squared_error: 0.5699\n",
            "Epoch 93/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5777 - mean_squared_error: 0.5777 - val_loss: 0.5627 - val_mean_squared_error: 0.5627\n",
            "Epoch 94/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5733 - mean_squared_error: 0.5733 - val_loss: 0.5947 - val_mean_squared_error: 0.5947\n",
            "Epoch 95/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.6143 - val_mean_squared_error: 0.6143\n",
            "Epoch 96/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5751 - mean_squared_error: 0.5751 - val_loss: 0.5817 - val_mean_squared_error: 0.5817\n",
            "Epoch 97/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5751 - mean_squared_error: 0.5751 - val_loss: 0.5522 - val_mean_squared_error: 0.5522\n",
            "Epoch 98/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5732 - mean_squared_error: 0.5732 - val_loss: 0.6111 - val_mean_squared_error: 0.6111\n",
            "Epoch 99/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5799 - mean_squared_error: 0.5799 - val_loss: 0.5898 - val_mean_squared_error: 0.5898\n",
            "Epoch 100/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5775 - mean_squared_error: 0.5775 - val_loss: 0.5687 - val_mean_squared_error: 0.5687\n",
            "Epoch 101/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5739 - mean_squared_error: 0.5739 - val_loss: 0.5774 - val_mean_squared_error: 0.5774\n",
            "Epoch 102/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5908 - val_mean_squared_error: 0.5908\n",
            "Epoch 103/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5655 - val_mean_squared_error: 0.5655\n",
            "Epoch 104/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5766 - val_mean_squared_error: 0.5766\n",
            "Epoch 105/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5776 - mean_squared_error: 0.5776 - val_loss: 0.5624 - val_mean_squared_error: 0.5624\n",
            "Epoch 106/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5783 - mean_squared_error: 0.5783 - val_loss: 0.5579 - val_mean_squared_error: 0.5579\n",
            "Epoch 107/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5790 - mean_squared_error: 0.5790 - val_loss: 0.5829 - val_mean_squared_error: 0.5829\n",
            "Epoch 108/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.5484 - val_mean_squared_error: 0.5484\n",
            "Epoch 109/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5764 - mean_squared_error: 0.5764 - val_loss: 0.5808 - val_mean_squared_error: 0.5808\n",
            "Epoch 110/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5752 - val_mean_squared_error: 0.5752\n",
            "Epoch 111/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5758 - mean_squared_error: 0.5758 - val_loss: 0.5743 - val_mean_squared_error: 0.5743\n",
            "Epoch 112/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5748 - mean_squared_error: 0.5748 - val_loss: 0.5621 - val_mean_squared_error: 0.5621\n",
            "Epoch 113/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.5595 - val_mean_squared_error: 0.5595\n",
            "Epoch 114/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 0.5764 - val_mean_squared_error: 0.5764\n",
            "Epoch 115/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5784 - mean_squared_error: 0.5784 - val_loss: 0.5950 - val_mean_squared_error: 0.5950\n",
            "Epoch 116/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5771 - mean_squared_error: 0.5771 - val_loss: 0.5499 - val_mean_squared_error: 0.5499\n",
            "Epoch 117/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 0.5599 - val_mean_squared_error: 0.5599\n",
            "Epoch 118/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5798 - mean_squared_error: 0.5798 - val_loss: 0.5640 - val_mean_squared_error: 0.5640\n",
            "Epoch 119/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5742 - mean_squared_error: 0.5742 - val_loss: 0.5852 - val_mean_squared_error: 0.5852\n",
            "Epoch 120/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5751 - mean_squared_error: 0.5751 - val_loss: 0.5656 - val_mean_squared_error: 0.5656\n",
            "Epoch 121/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5590 - val_mean_squared_error: 0.5590\n",
            "Epoch 122/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.5635 - val_mean_squared_error: 0.5635\n",
            "Epoch 123/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5779 - mean_squared_error: 0.5779 - val_loss: 0.5723 - val_mean_squared_error: 0.5723\n",
            "Epoch 124/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5770 - mean_squared_error: 0.5770 - val_loss: 0.5810 - val_mean_squared_error: 0.5810\n",
            "Epoch 125/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5610 - val_mean_squared_error: 0.5610\n",
            "Epoch 126/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5737 - mean_squared_error: 0.5737 - val_loss: 0.5737 - val_mean_squared_error: 0.5737\n",
            "Epoch 127/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5758 - mean_squared_error: 0.5758 - val_loss: 0.5517 - val_mean_squared_error: 0.5517\n",
            "Epoch 128/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5758 - mean_squared_error: 0.5758 - val_loss: 0.5663 - val_mean_squared_error: 0.5663\n",
            "Epoch 129/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5767 - mean_squared_error: 0.5767 - val_loss: 0.5827 - val_mean_squared_error: 0.5827\n",
            "Epoch 130/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5783 - mean_squared_error: 0.5783 - val_loss: 0.5434 - val_mean_squared_error: 0.5434\n",
            "Epoch 131/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5777 - mean_squared_error: 0.5777 - val_loss: 0.5409 - val_mean_squared_error: 0.5409\n",
            "Epoch 132/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5748 - mean_squared_error: 0.5748 - val_loss: 0.5512 - val_mean_squared_error: 0.5512\n",
            "Epoch 133/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.6011 - val_mean_squared_error: 0.6011\n",
            "Epoch 134/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 0.5717 - val_mean_squared_error: 0.5717\n",
            "Epoch 135/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5735 - mean_squared_error: 0.5735 - val_loss: 0.5715 - val_mean_squared_error: 0.5715\n",
            "Epoch 136/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5736 - mean_squared_error: 0.5736 - val_loss: 0.5736 - val_mean_squared_error: 0.5736\n",
            "Epoch 137/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5786 - mean_squared_error: 0.5786 - val_loss: 0.6021 - val_mean_squared_error: 0.6021\n",
            "Epoch 138/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5761 - mean_squared_error: 0.5761 - val_loss: 0.5772 - val_mean_squared_error: 0.5772\n",
            "Epoch 139/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5759 - mean_squared_error: 0.5759 - val_loss: 0.5644 - val_mean_squared_error: 0.5644\n",
            "Epoch 140/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5774 - mean_squared_error: 0.5774 - val_loss: 0.5696 - val_mean_squared_error: 0.5696\n",
            "Epoch 141/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 0.5727 - val_mean_squared_error: 0.5727\n",
            "Epoch 142/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5766 - mean_squared_error: 0.5766 - val_loss: 0.5551 - val_mean_squared_error: 0.5551\n",
            "Epoch 143/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 0.5615 - val_mean_squared_error: 0.5615\n",
            "Epoch 144/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5737 - mean_squared_error: 0.5737 - val_loss: 0.5589 - val_mean_squared_error: 0.5589\n",
            "Epoch 145/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5733 - mean_squared_error: 0.5733 - val_loss: 0.5542 - val_mean_squared_error: 0.5542\n",
            "Epoch 146/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5744 - mean_squared_error: 0.5744 - val_loss: 0.5582 - val_mean_squared_error: 0.5582\n",
            "Epoch 147/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5764 - mean_squared_error: 0.5764 - val_loss: 0.5935 - val_mean_squared_error: 0.5935\n",
            "Epoch 148/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 0.5615 - val_mean_squared_error: 0.5615\n",
            "Epoch 149/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5779 - mean_squared_error: 0.5779 - val_loss: 0.5611 - val_mean_squared_error: 0.5611\n",
            "Epoch 150/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5779 - mean_squared_error: 0.5779 - val_loss: 0.5634 - val_mean_squared_error: 0.5634\n",
            "Epoch 151/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5783 - mean_squared_error: 0.5783 - val_loss: 0.5772 - val_mean_squared_error: 0.5772\n",
            "Epoch 152/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.5670 - val_mean_squared_error: 0.5670\n",
            "Epoch 153/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5743 - mean_squared_error: 0.5743 - val_loss: 0.5609 - val_mean_squared_error: 0.5609\n",
            "Epoch 154/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5764 - mean_squared_error: 0.5764 - val_loss: 0.6401 - val_mean_squared_error: 0.6401\n",
            "Epoch 155/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5987 - val_mean_squared_error: 0.5987\n",
            "Epoch 156/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5759 - mean_squared_error: 0.5759 - val_loss: 0.6132 - val_mean_squared_error: 0.6132\n",
            "Epoch 157/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5769 - mean_squared_error: 0.5769 - val_loss: 0.5670 - val_mean_squared_error: 0.5670\n",
            "Epoch 158/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.5476 - val_mean_squared_error: 0.5476\n",
            "Epoch 159/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5769 - mean_squared_error: 0.5769 - val_loss: 0.5772 - val_mean_squared_error: 0.5772\n",
            "Epoch 160/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5768 - mean_squared_error: 0.5768 - val_loss: 0.5655 - val_mean_squared_error: 0.5655\n",
            "Epoch 161/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5727 - mean_squared_error: 0.5727 - val_loss: 0.5745 - val_mean_squared_error: 0.5745\n",
            "Epoch 162/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5748 - mean_squared_error: 0.5748 - val_loss: 0.5719 - val_mean_squared_error: 0.5719\n",
            "Epoch 163/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5750 - mean_squared_error: 0.5750 - val_loss: 0.5839 - val_mean_squared_error: 0.5839\n",
            "Epoch 164/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5735 - mean_squared_error: 0.5735 - val_loss: 0.6099 - val_mean_squared_error: 0.6099\n",
            "Epoch 165/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5790 - mean_squared_error: 0.5790 - val_loss: 0.5780 - val_mean_squared_error: 0.5780\n",
            "Epoch 166/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 0.5748 - val_mean_squared_error: 0.5748\n",
            "Epoch 167/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5744 - mean_squared_error: 0.5744 - val_loss: 0.5565 - val_mean_squared_error: 0.5565\n",
            "Epoch 168/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5744 - mean_squared_error: 0.5744 - val_loss: 0.6075 - val_mean_squared_error: 0.6075\n",
            "Epoch 169/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5723 - mean_squared_error: 0.5723 - val_loss: 0.5578 - val_mean_squared_error: 0.5578\n",
            "Epoch 170/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5777 - mean_squared_error: 0.5777 - val_loss: 0.5596 - val_mean_squared_error: 0.5596\n",
            "Epoch 171/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 0.5929 - val_mean_squared_error: 0.5929\n",
            "Epoch 172/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5779 - mean_squared_error: 0.5779 - val_loss: 0.5520 - val_mean_squared_error: 0.5520\n",
            "Epoch 173/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5713 - mean_squared_error: 0.5713 - val_loss: 0.5631 - val_mean_squared_error: 0.5631\n",
            "Epoch 174/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5742 - mean_squared_error: 0.5742 - val_loss: 0.5698 - val_mean_squared_error: 0.5698\n",
            "Epoch 175/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5734 - mean_squared_error: 0.5734 - val_loss: 0.5750 - val_mean_squared_error: 0.5750\n",
            "Epoch 176/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5742 - mean_squared_error: 0.5742 - val_loss: 0.5482 - val_mean_squared_error: 0.5482\n",
            "Epoch 177/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5752 - mean_squared_error: 0.5752 - val_loss: 0.5609 - val_mean_squared_error: 0.5609\n",
            "Epoch 178/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5727 - mean_squared_error: 0.5727 - val_loss: 0.5553 - val_mean_squared_error: 0.5553\n",
            "Epoch 179/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5973 - val_mean_squared_error: 0.5973\n",
            "Epoch 180/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5737 - mean_squared_error: 0.5737 - val_loss: 0.5780 - val_mean_squared_error: 0.5780\n",
            "Epoch 181/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 0.5821 - val_mean_squared_error: 0.5821\n",
            "Epoch 182/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 0.5635 - val_mean_squared_error: 0.5635\n",
            "Epoch 183/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.5772 - val_mean_squared_error: 0.5772\n",
            "Epoch 184/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5776 - mean_squared_error: 0.5776 - val_loss: 0.5703 - val_mean_squared_error: 0.5703\n",
            "Epoch 185/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5728 - mean_squared_error: 0.5728 - val_loss: 0.5670 - val_mean_squared_error: 0.5670\n",
            "Epoch 186/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.5681 - val_mean_squared_error: 0.5681\n",
            "Epoch 187/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5787 - mean_squared_error: 0.5787 - val_loss: 0.5698 - val_mean_squared_error: 0.5698\n",
            "Epoch 188/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5757 - mean_squared_error: 0.5757 - val_loss: 0.6147 - val_mean_squared_error: 0.6147\n",
            "Epoch 189/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.5686 - val_mean_squared_error: 0.5686\n",
            "Epoch 190/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5721 - mean_squared_error: 0.5721 - val_loss: 0.5508 - val_mean_squared_error: 0.5508\n",
            "Epoch 191/2000\n",
            "3577/3577 [==============================] - 13s 4ms/step - loss: 0.5754 - mean_squared_error: 0.5754 - val_loss: 0.5629 - val_mean_squared_error: 0.5629\n",
            "Epoch 192/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5771 - mean_squared_error: 0.5771 - val_loss: 0.5685 - val_mean_squared_error: 0.5685\n",
            "Epoch 193/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5732 - mean_squared_error: 0.5732 - val_loss: 0.5568 - val_mean_squared_error: 0.5568\n",
            "Epoch 194/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 0.5909 - val_mean_squared_error: 0.5909\n",
            "Epoch 195/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 0.5469 - val_mean_squared_error: 0.5469\n",
            "Epoch 196/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5745 - mean_squared_error: 0.5745 - val_loss: 0.5735 - val_mean_squared_error: 0.5735\n",
            "Epoch 197/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5611 - val_mean_squared_error: 0.5611\n",
            "Epoch 198/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 0.5916 - val_mean_squared_error: 0.5916\n",
            "Epoch 199/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5736 - mean_squared_error: 0.5736 - val_loss: 0.5846 - val_mean_squared_error: 0.5846\n",
            "Epoch 200/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5771 - mean_squared_error: 0.5771 - val_loss: 0.5767 - val_mean_squared_error: 0.5767\n",
            "Epoch 201/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5767 - mean_squared_error: 0.5767 - val_loss: 0.5623 - val_mean_squared_error: 0.5623\n",
            "Epoch 202/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5769 - mean_squared_error: 0.5769 - val_loss: 0.5613 - val_mean_squared_error: 0.5613\n",
            "Epoch 203/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5754 - mean_squared_error: 0.5754 - val_loss: 0.5659 - val_mean_squared_error: 0.5659\n",
            "Epoch 204/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5745 - mean_squared_error: 0.5745 - val_loss: 0.5824 - val_mean_squared_error: 0.5824\n",
            "Epoch 205/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5772 - mean_squared_error: 0.5772 - val_loss: 0.5585 - val_mean_squared_error: 0.5585\n",
            "Epoch 206/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5748 - mean_squared_error: 0.5748 - val_loss: 0.5391 - val_mean_squared_error: 0.5391\n",
            "Epoch 207/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5741 - mean_squared_error: 0.5741 - val_loss: 0.5690 - val_mean_squared_error: 0.5690\n",
            "Epoch 208/2000\n",
            "3577/3577 [==============================] - 14s 4ms/step - loss: 0.5737 - mean_squared_error: 0.5737 - val_loss: 0.5985 - val_mean_squared_error: 0.5985\n",
            "Epoch 209/2000\n",
            "1917/3577 [===============>..............] - ETA: 5s - loss: 0.5719 - mean_squared_error: 0.5719"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "\n",
        "generator_multiply = 100 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "#get data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN-Daily-May2024.npy')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-Daily-DIRECT-VAR.npy')\n",
        "#window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-Daily-May2024.npy')\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "#----------------check window distribution - we see upto 20 has very high fequency. so we remove that and take the rest to generate more samples, to create overall uniform distribution...\n",
        "#First we test window 20 to 30 and see if this method can fit the model well\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(window_label, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "\n",
        "##---------------------------IGNORE THIS IF NOT GENERATING FRESH VAE DATASET--------------------------------------------------------------------------------------------\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-round4-latent5-dim256.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-round4-latent5-dim256.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_daily_x50',results1)\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_daily_x100.npy')\n",
        "\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "#--------------------------------------IF REQUIRED REMOVE outlier....however we are not doing this now.--------------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x, y = x[mask, :], y[mask]\n",
        "\n",
        "\n",
        "###############Scale the target and then split the data into train test----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#Looking at the dist, we remove al y less than 20\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "y_transformed = transformer.fit_transform(y.reshape(-1,1)).flatten()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_transformed, test_size = 0.1, random_state = 42)\n",
        "\n",
        "#--------------------------------------------------------------------------------------CONSTRUCT, COMPILE, AND TRAIN THE MODEL------------------------------------------------------------------------------------------------------------------\n",
        "#------------MLP------------------------------------------------------\n",
        "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(LSTM(1024, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(512,return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(units = 128))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 64))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 32))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "#--------------------------------------------------------------LSTM--------------------------\n",
        "\n",
        "a =  x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "b =  x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(a.shape[1],a.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 16))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "#model.add(Dense(units = 4))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 1, activation = 'linear'))\n",
        "model.summary()\n",
        "x_train = a\n",
        "x_test = b\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.0003,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.0001, monitor='loss', mode='min', restore_best_weights=True)\n",
        "n_epochs = 2000\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=32,\n",
        "                   validation_split=0.1)\n",
        "                 #callbacks=[es])\n",
        "\n",
        "\n",
        "#-------------------------LOSS-------------------------------------\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "#-----------------------------------------------------TRAIN EVALUATION----------------------------------------------------------------\n",
        "y_train_pred_raw = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred_raw)\n",
        "y_train_true = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "\n",
        "score_train= r2_score(y_train_true,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "plt.plot(y_train_true[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------------------------TEST EVALUATION----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[100:200], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[100:200], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------SAVE MODEL AND RESULTS-----------------------------------------------------------------\n",
        "\n",
        "model.save(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily_V6A.keras')\n",
        "# It can be used to reconstruct the model identically.\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------Retrieve Model-----------------------------------------------------------------\n",
        "\n",
        "reconstructed_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_Daily_V6A.keras')\n",
        "\n",
        "#-----------------------------------------------------Test with retrieved model-----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = reconstructed_model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[1000:1050], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[1000:1050], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/predicted_window_V4.csv',y_test_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/derived_window_label_V4.csv',y_test_true)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/test_data_V4.csv',x_test)\n",
        "\n",
        "#--------------------------NOW RUN THE WHOLE DATASET-----------------------------------------------------------------\n",
        "\n",
        "x_whole = x\n",
        "y_whole = y_transformed\n",
        "\n",
        "y_pred_raw_w = reconstructed_model.predict(x_whole)\n",
        "y_whole_pred = transformer.inverse_transform(y_pred_raw_w)\n",
        "y_whole_true = transformer.inverse_transform(y_whole.reshape(-1,1)).flatten()\n",
        "\n",
        "score= r2_score(y_whole_true,y_whole_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_whole_true[5000:10000], color = 'red', label = 'Real data')\n",
        "plt.plot(y_whole_pred[5000:10000], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/predicted_COSW_V4.csv',y_whole_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/Calculated_label_COSW_V4.csv',y_whole_true)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/whole_data_V4.csv',x_whole)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Daily%20Data.ipynb",
      "authorship_tag": "ABX9TyMv7Ed1M4CpTELT/lwVNYeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}