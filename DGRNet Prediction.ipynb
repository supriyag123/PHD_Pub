{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx-5b_puABG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7492f17c-9fd2-4088-e1ce-9a1786364806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "447/447 [==============================] - 38s 81ms/step - loss: 1387.5629 - mean_squared_error: 1387.5629\n",
            "Epoch 2/100\n",
            "447/447 [==============================] - 35s 77ms/step - loss: 1247.8260 - mean_squared_error: 1247.8260\n",
            "Epoch 3/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 898.0354 - mean_squared_error: 898.0354\n",
            "Epoch 4/100\n",
            "447/447 [==============================] - 35s 79ms/step - loss: 800.9507 - mean_squared_error: 800.9507\n",
            "Epoch 5/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 744.1084 - mean_squared_error: 744.1084\n",
            "Epoch 6/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 717.5817 - mean_squared_error: 717.5817\n",
            "Epoch 7/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 700.5239 - mean_squared_error: 700.5239\n",
            "Epoch 8/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 693.1699 - mean_squared_error: 693.1699\n",
            "Epoch 9/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 681.1290 - mean_squared_error: 681.1290\n",
            "Epoch 10/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 673.5122 - mean_squared_error: 673.5122\n",
            "Epoch 11/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 1043.9302 - mean_squared_error: 1043.9302\n",
            "Epoch 12/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 1266.6995 - mean_squared_error: 1266.6995\n",
            "Epoch 13/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 697.0334 - mean_squared_error: 697.0334\n",
            "Epoch 14/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 660.3641 - mean_squared_error: 660.3641\n",
            "Epoch 15/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 652.8807 - mean_squared_error: 652.8806\n",
            "Epoch 16/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 643.2569 - mean_squared_error: 643.2569\n",
            "Epoch 17/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 631.2422 - mean_squared_error: 631.2422\n",
            "Epoch 18/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 621.4828 - mean_squared_error: 621.4828\n",
            "Epoch 19/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 619.9927 - mean_squared_error: 619.9927\n",
            "Epoch 20/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 611.7206 - mean_squared_error: 611.7206\n",
            "Epoch 21/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 607.3719 - mean_squared_error: 607.3719\n",
            "Epoch 22/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 601.8142 - mean_squared_error: 601.8142\n",
            "Epoch 23/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 593.7642 - mean_squared_error: 593.7642\n",
            "Epoch 24/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 587.0152 - mean_squared_error: 587.0152\n",
            "Epoch 25/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 578.6961 - mean_squared_error: 578.6961\n",
            "Epoch 26/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 569.3572 - mean_squared_error: 569.3572\n",
            "Epoch 27/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 558.0148 - mean_squared_error: 558.0148\n",
            "Epoch 28/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 547.6210 - mean_squared_error: 547.6210\n",
            "Epoch 29/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 530.8461 - mean_squared_error: 530.8461\n",
            "Epoch 30/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 524.7037 - mean_squared_error: 524.7037\n",
            "Epoch 31/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 519.2968 - mean_squared_error: 519.2968\n",
            "Epoch 32/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 508.1778 - mean_squared_error: 508.1778\n",
            "Epoch 33/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 491.5347 - mean_squared_error: 491.5347\n",
            "Epoch 34/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 474.6446 - mean_squared_error: 474.6446\n",
            "Epoch 35/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 461.4590 - mean_squared_error: 461.4590\n",
            "Epoch 36/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 452.0912 - mean_squared_error: 452.0912\n",
            "Epoch 37/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 436.5905 - mean_squared_error: 436.5905\n",
            "Epoch 38/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 423.9422 - mean_squared_error: 423.9422\n",
            "Epoch 39/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 408.7127 - mean_squared_error: 408.7127\n",
            "Epoch 40/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 395.4055 - mean_squared_error: 395.4055\n",
            "Epoch 41/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 381.4020 - mean_squared_error: 381.4020\n",
            "Epoch 42/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 368.8209 - mean_squared_error: 368.8209\n",
            "Epoch 43/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 355.8583 - mean_squared_error: 355.8583\n",
            "Epoch 44/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 342.2101 - mean_squared_error: 342.2101\n",
            "Epoch 45/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 334.5859 - mean_squared_error: 334.5859\n",
            "Epoch 46/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 319.6807 - mean_squared_error: 319.6807\n",
            "Epoch 47/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 306.6580 - mean_squared_error: 306.6580\n",
            "Epoch 48/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 292.9978 - mean_squared_error: 292.9978\n",
            "Epoch 49/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 286.4667 - mean_squared_error: 286.4667\n",
            "Epoch 50/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 277.0513 - mean_squared_error: 277.0513\n",
            "Epoch 51/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 263.5936 - mean_squared_error: 263.5936\n",
            "Epoch 52/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 250.2452 - mean_squared_error: 250.2452\n",
            "Epoch 53/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 236.2299 - mean_squared_error: 236.2299\n",
            "Epoch 54/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 229.5189 - mean_squared_error: 229.5189\n",
            "Epoch 55/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 229.4899 - mean_squared_error: 229.4899\n",
            "Epoch 56/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 208.8958 - mean_squared_error: 208.8958\n",
            "Epoch 57/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 206.0532 - mean_squared_error: 206.0532\n",
            "Epoch 58/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 201.4019 - mean_squared_error: 201.4019\n",
            "Epoch 59/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 190.8898 - mean_squared_error: 190.8898\n",
            "Epoch 60/100\n",
            "447/447 [==============================] - 36s 80ms/step - loss: 178.8984 - mean_squared_error: 178.8984\n",
            "Epoch 61/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 175.2472 - mean_squared_error: 175.2472\n",
            "Epoch 62/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 169.4576 - mean_squared_error: 169.4576\n",
            "Epoch 63/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 161.2448 - mean_squared_error: 161.2448\n",
            "Epoch 64/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 150.1612 - mean_squared_error: 150.1612\n",
            "Epoch 65/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 151.4700 - mean_squared_error: 151.4700\n",
            "Epoch 66/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 161.3716 - mean_squared_error: 161.3716\n",
            "Epoch 67/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 135.4873 - mean_squared_error: 135.4873\n",
            "Epoch 68/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 127.0193 - mean_squared_error: 127.0193\n",
            "Epoch 69/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 124.3395 - mean_squared_error: 124.3395\n",
            "Epoch 70/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 120.5247 - mean_squared_error: 120.5247\n",
            "Epoch 71/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 113.4006 - mean_squared_error: 113.4006\n",
            "Epoch 72/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 108.5866 - mean_squared_error: 108.5866\n",
            "Epoch 73/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 105.3130 - mean_squared_error: 105.3130\n",
            "Epoch 74/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 106.6555 - mean_squared_error: 106.6555\n",
            "Epoch 75/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 97.3124 - mean_squared_error: 97.3124\n",
            "Epoch 76/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 94.2239 - mean_squared_error: 94.2239\n",
            "Epoch 77/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 83.5017 - mean_squared_error: 83.5017\n",
            "Epoch 78/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 86.4111 - mean_squared_error: 86.4111\n",
            "Epoch 79/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 99.5129 - mean_squared_error: 99.5129\n",
            "Epoch 80/100\n",
            "447/447 [==============================] - 36s 81ms/step - loss: 74.0039 - mean_squared_error: 74.0039\n",
            "Epoch 81/100\n",
            "406/447 [==========================>...] - ETA: 3s - loss: 71.1962 - mean_squared_error: 71.1962"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "generator_multiply = 5 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "#get data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy')\n",
        "index = 500\n",
        "#We missed i=500 from processing the iosw. So here we are dropping row with index =500\n",
        "train_data= np.delete(train_data, index, axis=0)\n",
        "\n",
        "test_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TEST_hourly.npy')\n",
        "#all_data = np.concatenate((train_data,test_data),axis=0)\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-TRAIN_hourly.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "\n",
        "plt.suptitle('Sub sequence plotting', fontsize='30')\n",
        "plt.xlabel('Time', fontsize ='20')\n",
        "plt.ylabel('Feature 1', fontsize='20')\n",
        "plt.plot(train_data[:,:,1])\n",
        "plt.show()\n",
        "\n",
        "#---------------------------------Without VAE------------------------------------\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, window_label, test_size = 0.2, random_state = 42)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1]*n_features))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1]*n_features))\n",
        "\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "ee = EllipticEnvelope(contamination=0.05)\n",
        "yhat = ee.fit_predict(x_train)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x_train, y_train = x_train[mask, :], y_train[mask]\n",
        "\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], window_size, n_features))\n",
        "x_test = x_test.reshape((x_test.shape[0], window_size, n_features))\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "##!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.12.0\n",
        "#from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(1000, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
        "model.add(Dense(1000, activation = 'relu'))\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "#model.add(Dense(units = 128, activation = 'relu'))\n",
        "#model.add(Dense(units = 128, activation = 'relu'))\n",
        "model.add(Dense(units = 1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "#model.fit(x_train, y_train,epochs=5, batch_size=50, verbose=True)\n",
        "\n",
        "#y_pred = model.predict(x_test)\n",
        "\n",
        "#transform\n",
        "\n",
        "model.fit(x_train, y_train,epochs=100, batch_size=50, verbose=True)\n",
        "y_pred = model.predict(x_test)\n",
        "score= r2_score(y_test,y_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "print(\"mean_sqrd_error is==\",mean_squared_error(y_test,y_pred))\n",
        "print(\"root_mean_squared error of is==\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_test[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/pred_2.csv',y_pred_t)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/real_2.csv',y_test)\n",
        "print(\"MAE is==\",mean_absolute_error(y_test,y_pred))\n",
        "\n",
        "#########Isolation forest-------------------------#\n",
        "iso = IsolationForest(contamination=0.1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, window_label, test_size = 0.2, random_state = 42)\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1]*n_features))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1]*n_features))\n",
        "yhat = iso.fit_predict(x_train)\n",
        "mask = yhat != -1\n",
        "x_train, y_train = x_train[mask, :], y_train[mask]\n",
        "x_train = x_train.reshape((x_train.shape[0], window_size, n_features))\n",
        "x_test = x_test.reshape((x_test.shape[0], window_size, n_features))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(1000, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
        "model.add(Dense(1000, activation = 'relu'))\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "#model.add(Dense(units = 128, activation = 'relu'))\n",
        "#model.add(Dense(units = 128, activation = 'relu'))\n",
        "model.add(Dense(units = 1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "#model.fit(x_train, y_train,epochs=5, batch_size=50, verbose=True)\n",
        "\n",
        "#y_pred = model.predict(x_test)\n",
        "\n",
        "#transform\n",
        "\n",
        "model.fit(x_train, y_train,epochs=100, batch_size=50, verbose=True)\n",
        "y_pred = model.predict(x_test)\n",
        "score= r2_score(y_test,y_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "print(\"mean_sqrd_error is==\",mean_squared_error(y_test,y_pred))\n",
        "print(\"root_mean_squared error of is==\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
        "\n",
        "-----------------------------------------------------------------------------------\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-round5-latent50-dim1000.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-round5-latent50-dim1000.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((all_Z_i,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data',results)\n",
        "#Regression fitting\n",
        "x=results[:,:-1]\n",
        "y=results[:,dim]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "\n",
        "#################  ANN to do regression###########################\n",
        "#######  https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression  #######################\n",
        "###########  Nh = Ns/(α∗ (Ni + No)) ##########################\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "##!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.12.0\n",
        "#from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "\n",
        "transformer = StandardScaler()\n",
        "\n",
        "y_train_transformed = transformer.fit_transform(y_train.reshape(-1,1)).flatten()\n",
        "y_test_transformed = transformer.fit_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation = 'relu', input_dim = dim))\n",
        "model.add(Dense(units = 256, activation = 'relu'))\n",
        "model.add(Dense(units = 256, activation = 'relu'))\n",
        "model.add(Dense(units = 128, activation = 'relu'))\n",
        "model.add(Dense(units = 128, activation = 'relu'))\n",
        "model.add(Dense(units = 1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "#model.fit(x_train, y_train,epochs=5, batch_size=50, verbose=True)\n",
        "\n",
        "#y_pred = model.predict(x_test)\n",
        "\n",
        "#transform\n",
        "\n",
        "model.fit(x_train, y_train_transformed,epochs=500, batch_size=50, verbose=True)\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_t = transformer.inverse_transform(y_pred.reshape(-1,1)).flatten()\n",
        "score= r2_score(y_test,y_pred_t)\n",
        "print(\"r2 score is ==\",score)\n",
        "print(\"mean_sqrd_error is==\",mean_squared_error(y_test,y_pred_t))\n",
        "print(\"root_mean_squared error of is==\",np.sqrt(mean_squared_error(y_test,y_pred_t)))\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_test[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_pred_t[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/pred_2.csv',y_pred_t)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/real_2.csv',y_test)\n",
        "#----------------Random forest ----------------------------------\n",
        "###### Random forrest ergression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.linear_model import HuberRegressor\n",
        " # create regressor object\n",
        "#regressor = RandomForestRegressor(n_estimators = 50, random_state = 0)\n",
        "# prepare the model with input scaling\n",
        "pipeline = Pipeline(steps=[('power', PowerTransformer()), ('model',RandomForestRegressor(n_estimators = 100, random_state = 0))])\n",
        "#pipeline = Pipeline(steps=[('power', PowerTransformer()), ('model',HuberRegressor())])\n",
        "\n",
        "# prepare the model with target scaling\n",
        "model = TransformedTargetRegressor(regressor=pipeline, transformer=PowerTransformer())\n",
        "\n",
        "# fit the regressor with x and y data\n",
        "pipeline.fit(x_train,y_train)\n",
        "y_pred = pipeline.predict(x_test)  # test the output by changing values\n",
        "# creating an object of LinearRegression class\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# predicting the accuracy score\n",
        "\n",
        "score=r2_score(y_test,y_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "print(\"mean_sqrd_error is==\",mean_squared_error(y_test,y_pred))\n",
        "print(\"root_mean_squared error of is==\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
        "plt.scatter(y_test,y_pred);\n",
        "plt.xlabel('Actual');\n",
        "plt.ylabel('Predicted');\n",
        "\n",
        "\n",
        "plt.plot(y_test[5000:5049], color = 'red', label = 'Real data')\n",
        "plt.plot(y_pred_t[5000:5049], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20Prediction.ipynb",
      "authorship_tag": "ABX9TyMfw5Y2HsxdE6DcUixsUj1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}