{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HoP7OuWNxlsJ"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main training pipeline for pre-processed dataset.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Train sensor anomaly detection models on pre-processed dataset\")\n",
        "    parser.add_argument('--data_path', type=str, required=True,\n",
        "                       help='Path to pre-processed dataset [num_samples, window_length, num_sensors]')\n",
        "    parser.add_argument('--models_dir', type=str, default='./trained_models',\n",
        "                       help='Directory to save models')\n",
        "    parser.add_argument('--holdout_dir', type=str, default='./holdout_data',\n",
        "                       help='Directory to save holdout data for streaming simulation')\n",
        "    parser.add_argument('--window_length', type=int, default=50,\n",
        "                       help='Expected sequence window length')\n",
        "    parser.add_argument('--model_type', type=str, default='lstm_autoencoder',\n",
        "                       choices=['lstm_autoencoder', 'vae'], help='Model type')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    parser.add_argument('--sensors', type=str, default=None,\n",
        "                       help='Comma-separated sensor IDs to train (default: all)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"üöÄ SENSOR MODEL PRE-TRAINING SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìÇ Data path: {args.data_path}\")\n",
        "    print(f\"üíæ Models directory: {args.models_dir}\")\n",
        "    print(f\"üì¶ Holdout directory: {args.holdout_dir}\")\n",
        "    print(f\"üìè Window length: {args.window_length}\")\n",
        "    print(f\"üß† Model type: {args.model_type}\")\n",
        "    print(f\"üîÑ Epochs: {args.epochs}\")\n",
        "    print()\n",
        "\n",
        "    # Load and split dataset\n",
        "    training_data, holdout_data = load_your_dataset(args.data_path)\n",
        "\n",
        "    # Validate dataset\n",
        "    training_data = validate_dataset(training_data, args.window_length)\n",
        "\n",
        "    # Save holdout data for streaming simulation\n",
        "    os.makedirs(args.holdout_dir, exist_ok=True)\n",
        "    holdout_path = os.path.join(args.holdout_dir, 'holdout_data.npy')\n",
        "    np.save(holdout_path, holdout_data)\n",
        "    print(f\"üíæ Saved holdout data to: {holdout_path}\")\n",
        "    print(f\"   Holdout shape: {holdout_data.shape}\")\n",
        "\n",
        "    # Get dataset dimensions\n",
        "    num_samples, window_length, num_sensors = training_data.shape\n",
        "\n",
        "    # Filter sensors if specified\n",
        "    sensor_list = list(range(num_sensors))\n",
        "    if args.sensors:\n",
        "        requested_sensors = [int(x.strip()) for x in args.sensors.split(',')]\n",
        "        sensor_list = [sid for sid in requested_sensors if sid < num_sensors]\n",
        "        print(f\"üéØ Training only sensors: {sensor_list}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SensorModelTrainer(\n",
        "        window_length=window_length,\n",
        "        model_type=args.model_type,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    # Train models for each sensor\n",
        "    print(f\"\\nüèãÔ∏è TRAINING {len(sensor_list)} SENSOR MODELS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    training_results = {}\n",
        "    successful_training = 0\n",
        "\n",
        "    for sensor_id in sensor_list:\n",
        "        try:\n",
        "            print(f\"\\nüéØ SENSOR {sensor_id}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Extract data for this sensor: [num_samples, window_length]\n",
        "            sensor_sequences = training_data[:, :, sensor_id]\n",
        "\n",
        "            # Train model\n",
        "            model, training_info = trainer.train_sensor_model(sensor_sequences, sensor_id)\n",
        "\n",
        "            # Save model\n",
        "            model_path, metadata_path = trainer.save_model(model, training_info, args.models_dir)\n",
        "\n",
        "            training_results[sensor_id] = {\n",
        "                'success': True,\n",
        "                'model_path': model_path,\n",
        "                'metadata_path': metadata_path,\n",
        "                'training_info': training_info\n",
        "            }\n",
        "\n",
        "            successful_training += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Training failed: {str(e)}\")\n",
        "            training_results[sensor_id] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nüìä TRAINING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"‚úÖ Successful: {successful_training}/{len(sensor_list)} sensors\")\n",
        "    print(f\"üíæ Models saved to: {args.models_dir}\")\n",
        "    print(f\"üì¶ Holdout data saved to: {holdout_path}\")\n",
        "\n",
        "    if successful_training > 0:\n",
        "        print(f\"\\nüèÜ TRAINED SENSORS:\")\n",
        "        for sensor_id, result in training_results.items():\n",
        "            if result['success']:\n",
        "                info = result['training_info']\n",
        "                print(f\"  Sensor {sensor_id}: {info['epochs_trained']} epochs, \"\n",
        "                      f\"test loss: {info['test_loss']:.6f}, \"\n",
        "                      f\"baseline error: {info['baseline_stats']['mean']:.6f}\")\n",
        "\n",
        "    failed_sensors = [sid for sid, result in training_results.items() if not result['success']]\n",
        "    if failed_sensors:\n",
        "        print(f\"\\n‚ùå FAILED SENSORS: {failed_sensors}\")\n",
        "\n",
        "    # Save comprehensive training summary\n",
        "    summary_path = os.path.join(args.models_dir, 'training_summary.pkl')\n",
        "    training_summary = {\n",
        "        'training_results': training_results,\n",
        "        'config': vars(args),\n",
        "        'dataset_info': {\n",
        "            'original_shape': training_data.shape,\n",
        "            'holdout_shape': holdout_data.shape,\n",
        "            'num_sensors': num_sensors,\n",
        "            'window_length': window_length,\n",
        "            'total_training_samples': num_samples\n",
        "        },\n",
        "        'timestamp': datetime.now(),\n",
        "        'successful_sensors': successful_training,\n",
        "        'failed_sensors': failed_sensors,\n",
        "        'holdout_data_path': holdout_path\n",
        "    }\n",
        "\n",
        "    with open(summary_path, 'wb') as f:\n",
        "        pickle.dump(training_summary, f)\n",
        "\n",
        "    print(f\"\\nüíæ Training summary saved to: {summary_path}\")\n",
        "    print(f\"‚úÖ PRE-TRAINING COMPLETED!\")\n",
        "\n",
        "    # Instructions for next steps\n",
        "    print(f\"\\nüìã NEXT STEPS:\")\n",
        "    print(f\"  1. Use trained models: {args.models_dir}\")\n",
        "    print(f\"  2. Use holdout data for streaming: {holdout_path}\")\n",
        "    print(f\"  3. Run production system with:\")\n",
        "    print(f\"     python production_agent_system.py --models_dir {args.models_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\"\"\"\n",
        "Sensor Model Pre-Training System\n",
        "================================\n",
        "\n",
        "This script handles the initial training of sensor models using your real dataset.\n",
        "Models are trained once and saved to disk for use in the production agent system.\n",
        "\n",
        "Usage:\n",
        "    python sensor_pretraining.py --data_path /path/to/dataset --models_dir ./trained_models\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è TensorFlow not available. Install with: pip install tensorflow\")\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SensorModelTrainer:\n",
        "    \"\"\"\n",
        "    Handles training and saving of individual sensor models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 window_length: int = 50,\n",
        "                 model_type: str = 'lstm_autoencoder',\n",
        "                 latent_dim: int = 32,\n",
        "                 epochs: int = 100,\n",
        "                 batch_size: int = 32,\n",
        "                 validation_split: float = 0.2):\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.model_type = model_type\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise ImportError(\"TensorFlow required for model training\")\n",
        "\n",
        "    def build_lstm_autoencoder(self) -> Model:\n",
        "        \"\"\"Build LSTM Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1), name='encoder_input')\n",
        "        encoded = LSTM(self.latent_dim, activation='relu', return_sequences=False, name='encoder_lstm')(inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = RepeatVector(self.window_length, name='repeat_vector')(encoded)\n",
        "        decoded = LSTM(self.latent_dim, activation='relu', return_sequences=True, name='decoder_lstm')(decoded)\n",
        "        outputs = TimeDistributed(Dense(1, activation='linear'), name='decoder_output')(decoded)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs, outputs, name='sensor_lstm_autoencoder')\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_vae(self) -> Model:\n",
        "        \"\"\"Build Variational Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1))\n",
        "        x = LSTM(self.latent_dim, return_sequences=False)(inputs)\n",
        "\n",
        "        # Latent space\n",
        "        z_mean = Dense(self.latent_dim // 2, name='z_mean')(x)\n",
        "        z_log_var = Dense(self.latent_dim // 2, name='z_log_var')(x)\n",
        "\n",
        "        # Sampling function\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = tf.keras.layers.Lambda(sampling, name='sampling')([z_mean, z_log_var])\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = RepeatVector(self.window_length)(z)\n",
        "        decoded = LSTM(self.latent_dim, return_sequences=True)(decoder_input)\n",
        "        outputs = TimeDistributed(Dense(1))(decoded)\n",
        "\n",
        "        # VAE model\n",
        "        model = Model(inputs, outputs, name='sensor_vae')\n",
        "\n",
        "        # VAE loss\n",
        "        reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        vae_loss = reconstruction_loss + 0.1 * kl_loss\n",
        "        model.add_loss(vae_loss)\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, sensor_data: np.ndarray, overlap_ratio: float = 0.5) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert time series data into sequences for training.\n",
        "\n",
        "        Args:\n",
        "            sensor_data: 1D time series data for one sensor\n",
        "            overlap_ratio: Overlap between consecutive sequences (0.0 = no overlap, 0.9 = high overlap)\n",
        "\n",
        "        Returns:\n",
        "            Array of sequences [n_sequences, window_length]\n",
        "        \"\"\"\n",
        "\n",
        "        if len(sensor_data) < self.window_length:\n",
        "            raise ValueError(f\"Data length ({len(sensor_data)}) < window_length ({self.window_length})\")\n",
        "\n",
        "        # Calculate step size based on overlap\n",
        "        step_size = max(1, int(self.window_length * (1 - overlap_ratio)))\n",
        "\n",
        "        sequences = []\n",
        "        for i in range(0, len(sensor_data) - self.window_length + 1, step_size):\n",
        "            sequences.append(sensor_data[i:i + self.window_length])\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def train_sensor_model(self, sensor_data: np.ndarray, sensor_id: int) -> Tuple[Model, Dict]:\n",
        "        \"\"\"\n",
        "        Train a model for a single sensor using pre-prepared sequences.\n",
        "\n",
        "        Args:\n",
        "            sensor_data: Pre-scaled sequences [num_samples, window_length] for this sensor\n",
        "            sensor_id: Sensor identifier\n",
        "\n",
        "        Returns:\n",
        "            Trained model and training history\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Training model for sensor {sensor_id}...\")\n",
        "        print(f\"  Data shape: {sensor_data.shape}\")\n",
        "        print(f\"  Total sequences: {len(sensor_data)}\")\n",
        "\n",
        "        if len(sensor_data) < 100:\n",
        "            raise ValueError(f\"Insufficient sequences ({len(sensor_data)}) for training. Need at least 100.\")\n",
        "\n",
        "        # Data is already in sequence format [num_samples, window_length]\n",
        "        # Reshape for model input [num_samples, window_length, 1]\n",
        "        X = sensor_data.reshape(len(sensor_data), self.window_length, 1)\n",
        "\n",
        "        # Split data into train/validation/test\n",
        "        # 70% train, 15% validation, 15% test\n",
        "        n_samples = len(X)\n",
        "        n_train = int(0.7 * n_samples)\n",
        "        n_val = int(0.15 * n_samples)\n",
        "\n",
        "        # Shuffle indices to randomize splits\n",
        "        indices = np.random.RandomState(42).permutation(n_samples)\n",
        "\n",
        "        train_idx = indices[:n_train]\n",
        "        val_idx = indices[n_train:n_train + n_val]\n",
        "        test_idx = indices[n_train + n_val:]\n",
        "\n",
        "        X_train = X[train_idx]\n",
        "        X_val = X[val_idx]\n",
        "        X_test = X[test_idx]\n",
        "\n",
        "        print(f\"  Train sequences: {len(X_train)}\")\n",
        "        print(f\"  Validation sequences: {len(X_val)}\")\n",
        "        print(f\"  Test sequences: {len(X_test)}\")\n",
        "\n",
        "        # Build model\n",
        "        if self.model_type == 'lstm_autoencoder':\n",
        "            model = self.build_lstm_autoencoder()\n",
        "        elif self.model_type == 'vae':\n",
        "            model = self.build_vae()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        print(f\"  Model: {self.model_type} with {model.count_params():,} parameters\")\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        if self.model_type == 'vae':\n",
        "            # VAE training (no target needed due to custom loss)\n",
        "            history = model.fit(\n",
        "                X_train, epochs=self.epochs, batch_size=self.batch_size,\n",
        "                validation_data=(X_val,), callbacks=callbacks, verbose=1\n",
        "            )\n",
        "        else:\n",
        "            # Autoencoder training\n",
        "            history = model.fit(\n",
        "                X_train, X_train, epochs=self.epochs, batch_size=self.batch_size,\n",
        "                validation_data=(X_val, X_val), callbacks=callbacks, verbose=1\n",
        "            )\n",
        "\n",
        "        training_time = datetime.now() - start_time\n",
        "        print(f\"  Training completed in {training_time}\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"  Evaluating on test set...\")\n",
        "        if self.model_type == 'vae':\n",
        "            test_loss = model.evaluate(X_test, verbose=0)\n",
        "        else:\n",
        "            test_loss = model.evaluate(X_test, X_test, verbose=0)\n",
        "\n",
        "        print(f\"  Test loss: {test_loss:.6f}\")\n",
        "\n",
        "        # Compute baseline errors for anomaly detection using validation set\n",
        "        print(\"  Computing baseline error statistics...\")\n",
        "        baseline_errors = []\n",
        "\n",
        "        for i in range(0, len(X_val), self.batch_size):\n",
        "            batch = X_val[i:i + self.batch_size]\n",
        "            predictions = model.predict(batch, verbose=0)\n",
        "\n",
        "            for j, pred in enumerate(predictions):\n",
        "                error = mean_squared_error(batch[j].flatten(), pred.flatten())\n",
        "                baseline_errors.append(error)\n",
        "\n",
        "        baseline_stats = {\n",
        "            'mean': float(np.mean(baseline_errors)),\n",
        "            'std': float(np.std(baseline_errors)) + 1e-8,\n",
        "            'q95': float(np.percentile(baseline_errors, 95)),\n",
        "            'q99': float(np.percentile(baseline_errors, 99)),\n",
        "            'min': float(np.min(baseline_errors)),\n",
        "            'max': float(np.max(baseline_errors))\n",
        "        }\n",
        "\n",
        "        training_info = {\n",
        "            'sensor_id': sensor_id,\n",
        "            'model_type': self.model_type,\n",
        "            'window_length': self.window_length,\n",
        "            'training_sequences': len(X_train),\n",
        "            'validation_sequences': len(X_val),\n",
        "            'test_sequences': len(X_test),\n",
        "            'training_time': str(training_time),\n",
        "            'final_loss': float(history.history['loss'][-1]),\n",
        "            'final_val_loss': float(history.history['val_loss'][-1]),\n",
        "            'test_loss': float(test_loss) if isinstance(test_loss, (int, float)) else float(test_loss[0]),\n",
        "            'epochs_trained': len(history.history['loss']),\n",
        "            'baseline_errors': baseline_errors[-100:],  # Store last 100 for drift detection\n",
        "            'baseline_stats': baseline_stats,\n",
        "            'trained_at': datetime.now()\n",
        "        }\n",
        "\n",
        "        print(f\"  ‚úÖ Training successful!\")\n",
        "        print(f\"     Final train loss: {training_info['final_loss']:.6f}\")\n",
        "        print(f\"     Final val loss: {training_info['final_val_loss']:.6f}\")\n",
        "        print(f\"     Test loss: {training_info['test_loss']:.6f}\")\n",
        "        print(f\"     Baseline error: {baseline_stats['mean']:.6f} ¬± {baseline_stats['std']:.6f}\")\n",
        "\n",
        "        return model, training_info\n",
        "\n",
        "    def save_model(self, model: Model, training_info: Dict, models_dir: str):\n",
        "        \"\"\"Save trained model and metadata.\"\"\"\n",
        "\n",
        "        sensor_id = training_info['sensor_id']\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Save model\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        model.save(model_path)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_path = os.path.join(models_dir, f\"sensor_{sensor_id}_metadata.pkl\")\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump(training_info, f)\n",
        "\n",
        "        print(f\"  üíæ Saved model and metadata for sensor {sensor_id}\")\n",
        "\n",
        "        return model_path, metadata_path\n",
        "\n",
        "\n",
        "def load_your_dataset(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load your pre-processed dataset.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to your dataset\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (training_data, holdout_data)\n",
        "        - training_data: [num_samples, window_length, num_sensors] for model training\n",
        "        - holdout_data: [1000, window_length, num_sensors] for streaming simulation\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üìÇ Loading pre-processed dataset from: {data_path}\")\n",
        "\n",
        "    if data_path.endswith('.npy'):\n",
        "        # Numpy array format\n",
        "        print(\"  Format: Numpy array (.npy)\")\n",
        "        data = np.load(data_path)\n",
        "\n",
        "    elif data_path.endswith('.npz'):\n",
        "        # Compressed numpy format\n",
        "        print(\"  Format: Compressed numpy (.npz)\")\n",
        "        data_file = np.load(data_path)\n",
        "        # Assume main data is stored with key 'data' or use first key\n",
        "        if 'data' in data_file:\n",
        "            data = data_file['data']\n",
        "        else:\n",
        "            data = data_file[list(data_file.keys())[0]]\n",
        "\n",
        "    elif data_path.endswith('.h5') or data_path.endswith('.hdf5'):\n",
        "        # HDF5 format\n",
        "        print(\"  Format: HDF5 file\")\n",
        "        import h5py\n",
        "\n",
        "        with h5py.File(data_path, 'r') as f:\n",
        "            # Assume main data is stored with key 'data' or use first key\n",
        "            if 'data' in f:\n",
        "                data = f['data'][:]\n",
        "            else:\n",
        "                data = f[list(f.keys())[0]][:]\n",
        "\n",
        "    elif data_path.endswith('.pkl') or data_path.endswith('.pickle'):\n",
        "        # Pickle format\n",
        "        print(\"  Format: Pickle file\")\n",
        "        import pickle\n",
        "        with open(data_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            # If pickle contains dict, extract the array\n",
        "            if isinstance(data, dict):\n",
        "                if 'data' in data:\n",
        "                    data = data['data']\n",
        "                else:\n",
        "                    data = list(data.values())[0]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported data format: {data_path}. Supported: .npy, .npz, .h5, .hdf5, .pkl\")\n",
        "\n",
        "    # Validate data format\n",
        "    if not isinstance(data, np.ndarray):\n",
        "        raise ValueError(f\"Data must be numpy array, got {type(data)}\")\n",
        "\n",
        "    if data.ndim != 3:\n",
        "        raise ValueError(f\"Data must be 3D [num_samples, window_length, num_sensors], got {data.ndim}D: {data.shape}\")\n",
        "\n",
        "    num_samples, window_length, num_sensors = data.shape\n",
        "    print(f\"  üìä Dataset shape: {data.shape}\")\n",
        "    print(f\"     Total samples: {num_samples:,}\")\n",
        "    print(f\"     Window length: {window_length}\")\n",
        "    print(f\"     Number of sensors: {num_sensors}\")\n",
        "\n",
        "    # Check if data appears scaled\n",
        "    data_min, data_max = np.min(data), np.max(data)\n",
        "    print(f\"  üìà Data range: [{data_min:.3f}, {data_max:.3f}]\")\n",
        "\n",
        "    if -10 <= data_min and data_max <= 10:\n",
        "        print(\"  ‚úÖ Data appears to be pre-scaled\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è Data range seems large - verify it's properly scaled\")\n",
        "\n",
        "    # Check for invalid values\n",
        "    invalid_count = np.sum(~np.isfinite(data))\n",
        "    if invalid_count > 0:\n",
        "        print(f\"  ‚ö†Ô∏è Found {invalid_count} invalid values (NaN/Inf) - will be handled during training\")\n",
        "\n",
        "    # Split into training and holdout\n",
        "    if num_samples <= 1000:\n",
        "        raise ValueError(f\"Dataset too small ({num_samples} samples). Need more than 1000 samples.\")\n",
        "\n",
        "    holdout_data = data[-1000:].copy()  # Last 1000 samples\n",
        "    training_data = data[:-1000].copy()  # Everything except last 1000\n",
        "\n",
        "    print(f\"  üìä Data split:\")\n",
        "    print(f\"     Training: {training_data.shape[0]:,} samples\")\n",
        "    print(f\"     Holdout: {holdout_data.shape[0]:,} samples\")\n",
        "\n",
        "    return training_data, holdout_data\n",
        "\n",
        "\n",
        "def validate_dataset(dataset: np.ndarray, window_length: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Validate the pre-processed dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: Pre-processed dataset [num_samples, window_length, num_sensors]\n",
        "        window_length: Expected window length\n",
        "\n",
        "    Returns:\n",
        "        Validated dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîç Validating dataset...\")\n",
        "\n",
        "    num_samples, actual_window_length, num_sensors = dataset.shape\n",
        "\n",
        "    # Check window length\n",
        "    if actual_window_length != window_length:\n",
        "        raise ValueError(f\"Window length mismatch: expected {window_length}, got {actual_window_length}\")\n",
        "\n",
        "    # Check for invalid values\n",
        "    invalid_mask = ~np.isfinite(dataset)\n",
        "    invalid_count = np.sum(invalid_mask)\n",
        "\n",
        "    if invalid_count > 0:\n",
        "        invalid_ratio = invalid_count / dataset.size\n",
        "        print(f\"  ‚ö†Ô∏è Found {invalid_count:,} invalid values ({invalid_ratio:.2%} of total)\")\n",
        "\n",
        "        if invalid_ratio > 0.1:\n",
        "            raise ValueError(f\"Too many invalid values ({invalid_ratio:.1%}). Check data quality.\")\n",
        "\n",
        "        # Replace invalid values with sensor means\n",
        "        print(\"  üîß Replacing invalid values with sensor means...\")\n",
        "        for sensor_id in range(num_sensors):\n",
        "            sensor_data = dataset[:, :, sensor_id]\n",
        "            valid_data = sensor_data[np.isfinite(sensor_data)]\n",
        "            if len(valid_data) > 0:\n",
        "                mean_val = np.mean(valid_data)\n",
        "                sensor_mask = invalid_mask[:, :, sensor_id]\n",
        "                dataset[:, :, sensor_id][sensor_mask] = mean_val\n",
        "\n",
        "    # Statistical validation per sensor\n",
        "    print(f\"  üìä Per-sensor statistics:\")\n",
        "    for sensor_id in range(num_sensors):\n",
        "        sensor_data = dataset[:, :, sensor_id]\n",
        "\n",
        "        mean_val = np.mean(sensor_data)\n",
        "        std_val = np.std(sensor_data)\n",
        "        min_val = np.min(sensor_data)\n",
        "        max_val = np.max(sensor_data)\n",
        "\n",
        "        print(f\"    Sensor {sensor_id}: Œº={mean_val:.3f}, œÉ={std_val:.3f}, \"\n",
        "              f\"range=[{min_val:.3f}, {max_val:.3f}]\")\n",
        "\n",
        "        # Check for constant values\n",
        "        if std_val < 1e-6:\n",
        "            print(f\"    ‚ö†Ô∏è Sensor {sensor_id}: Nearly constant values (œÉ={std_val:.2e})\")\n",
        "\n",
        "    print(f\"‚úÖ Dataset validation completed\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Train sensor anomaly detection models\")\n",
        "    parser.add_argument('--data_path', type=str, required=True, help='Path to dataset')\n",
        "    parser.add_argument('--models_dir', type=str, default='./trained_models', help='Directory to save models')\n",
        "    parser.add_argument('--window_length', type=int, default=50, help='Sequence window length')\n",
        "    parser.add_argument('--model_type', type=str, default='lstm_autoencoder',\n",
        "                       choices=['lstm_autoencoder', 'vae'], help='Model type')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    parser.add_argument('--sensors', type=str, default=None,\n",
        "                       help='Comma-separated sensor IDs to train (default: all)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"üöÄ SENSOR MODEL PRE-TRAINING SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìÇ Data path: {args.data_path}\")\n",
        "    print(f\"üíæ Models directory: {args.models_dir}\")\n",
        "    print(f\"üìè Window length: {args.window_length}\")\n",
        "    print(f\"üß† Model type: {args.model_type}\")\n",
        "    print(f\"üîÑ Epochs: {args.epochs}\")\n",
        "    print()\n",
        "\n",
        "    # Load dataset\n",
        "    sensor_data = load_your_dataset(args.data_path)\n",
        "\n",
        "    # Validate dataset\n",
        "    validated_data = validate_dataset(sensor_data, args.window_length)\n",
        "\n",
        "    if not validated_data:\n",
        "        print(\"‚ùå No valid sensors found for training\")\n",
        "        return\n",
        "\n",
        "    # Filter sensors if specified\n",
        "    if args.sensors:\n",
        "        requested_sensors = [int(x.strip()) for x in args.sensors.split(',')]\n",
        "        validated_data = {sid: data for sid, data in validated_data.items()\n",
        "                         if sid in requested_sensors}\n",
        "        print(f\"üéØ Training only sensors: {list(validated_data.keys())}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SensorModelTrainer(\n",
        "        window_length=args.window_length,\n",
        "        model_type=args.model_type,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    # Train models\n",
        "    print(f\"\\nüèãÔ∏è TRAINING {len(validated_data)} SENSOR MODELS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    training_results = {}\n",
        "    successful_training = 0\n",
        "\n",
        "    for sensor_id, data in validated_data.items():\n",
        "        try:\n",
        "            print(f\"\\nüéØ SENSOR {sensor_id}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Train model\n",
        "            model, training_info = trainer.train_sensor_model(data, sensor_id)\n",
        "\n",
        "            # Save model\n",
        "            model_path, metadata_path = trainer.save_model(model, training_info, args.models_dir)\n",
        "\n",
        "            training_results[sensor_id] = {\n",
        "                'success': True,\n",
        "                'model_path': model_path,\n",
        "                'metadata_path': metadata_path,\n",
        "                'training_info': training_info\n",
        "            }\n",
        "\n",
        "            successful_training += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Training failed: {str(e)}\")\n",
        "            training_results[sensor_id] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nüìä TRAINING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"‚úÖ Successful: {successful_training}/{len(validated_data)} sensors\")\n",
        "    print(f\"üíæ Models saved to: {args.models_dir}\")\n",
        "\n",
        "    if successful_training > 0:\n",
        "        print(f\"\\nüèÜ TRAINED SENSORS:\")\n",
        "        for sensor_id, result in training_results.items():\n",
        "            if result['success']:\n",
        "                info = result['training_info']\n",
        "                print(f\"  Sensor {sensor_id}: {info['epochs_trained']} epochs, \"\n",
        "                      f\"final loss: {info['final_loss']:.6f}\")\n",
        "\n",
        "    failed_sensors = [sid for sid, result in training_results.items() if not result['success']]\n",
        "    if failed_sensors:\n",
        "        print(f\"\\n‚ùå FAILED SENSORS: {failed_sensors}\")\n",
        "\n",
        "    # Save training summary\n",
        "    summary_path = os.path.join(args.models_dir, 'training_summary.pkl')\n",
        "    with open(summary_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'training_results': training_results,\n",
        "            'config': vars(args),\n",
        "            'timestamp': datetime.now()\n",
        "        }, f)\n",
        "\n",
        "    print(f\"\\nüíæ Training summary saved to: {summary_path}\")\n",
        "    print(f\"‚úÖ PRE-TRAINING COMPLETED!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb",
      "authorship_tag": "ABX9TyPlGWB6ySwOAkH7WpK5UQsT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}