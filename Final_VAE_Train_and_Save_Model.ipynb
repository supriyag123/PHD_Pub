{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/Final_VAE_Train_and_Save_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bx-5b_puABG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "bd0b6626-9d39-4046-d5fd-13e869d6d29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "970/970 [==============================] - 5s 5ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "Exception encountered when calling layer 'Dense2' (type Dense).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[31023,120,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: \n\nCall arguments received by layer 'Dense2' (type Dense):\n  • inputs=tf.Tensor(shape=(31023, 120, 2), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-86a5f92da598>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sub sequence plotting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature 3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'Dense2' (type Dense).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[31023,120,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: \n\nCall arguments received by layer 'Dense2' (type Dense):\n  • inputs=tf.Tensor(shape=(31023, 120, 2), dtype=float32)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "#get data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/DATAFILES/multivariate_long_sequences-TEST.npy')\n",
        "#window_label = np.load(r'/content/drive/MyDrive/PHD/2021/DATAFILES/WINDOW_LABELLED.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "\n",
        "\n",
        "#create sampling layer\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "#Build the encoder\n",
        "latent_dim = 2\n",
        "intermediate_dim = 1000\n",
        "encoder_inputs =  layers.Input(shape=(window_size, n_features),name=\"encoder_input\")\n",
        "x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\" )(encoder_inputs)\n",
        "x1 = layers.Dense(intermediate_dim, name=\"dense\" )(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "\n",
        "#Dcoder\n",
        "\n",
        "inp_z = Input(shape=(latent_dim,),name=\"decoder\")\n",
        "x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "x2= layers.Dense(intermediate_dim,  name=\"Dense2\")(x1)\n",
        "x3= layers.LSTM(intermediate_dim,activation='tanh', return_sequences=True, name=\"lstm2\")(x2)\n",
        "decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "#decode_out = layers.LSTM(n_features,name='decodeout', return_sequences=True)(x2) #Alternative\n",
        "decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "decoder.summary()\n",
        "#Parameters\n",
        "n_epochs = 500 # total number of epochs\n",
        "klstart = 10 # The number of epochs at which KL loss should be included\n",
        "kl_annealtime = n_epochs-klstart\n",
        "# the starting value of weight is 0\n",
        "# define it as a keras backend variable\n",
        "weight = K.variable(0.0)\n",
        "\n",
        "#Define the VAE as a Model with a custom train_step\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            K.print_tensor(weight)\n",
        "            total_loss = reconstruction_loss + (weight*kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                  }\n",
        "\n",
        "\n",
        "\n",
        "# CALLBACKS\n",
        "es = keras.callbacks.EarlyStopping(patience=10, verbose=1, min_delta=0.001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "class AnnealingCallback(Callback):\n",
        "    def __init__(self, weight):\n",
        "        self.weight = weight\n",
        "    def on_epoch_end (self, epoch, logs={}):\n",
        "        if epoch > klstart :\n",
        "            new_weight = min(K.get_value(self.weight) + (1./ kl_annealtime), 1.)\n",
        "            K.set_value(self.weight, new_weight)\n",
        "        print (\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "#Train the VAE\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "history=vae.fit(train_data,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=50,\n",
        "                 validation_split=0.3,\n",
        "                 callbacks=[es,AnnealingCallback(weight)])\n",
        "\n",
        "\n",
        "encoder.save(r'/content/drive/MyDrive/PHD/2021/vae-encoder-saved-round2.model')\n",
        "decoder.save(r'/content/drive/MyDrive/PHD/2021/vae-decoder-saved-round2.model')\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2021/vae-encoder-saved-round2.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2021/vae-decoder-saved-round2.model')\n",
        "\n",
        "e =\n",
        "#vae2.evaluate(test_data)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "#plt.plot(history.history['val_loss'], label='validation loss')\n",
        "#plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['loss','val_loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['kl_loss'], label='KL loss')\n",
        "#plt.plot(history.history['total_val_loss'], label='reconstruction_loss')\n",
        "#plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('KL Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['kl_loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#PLOT TRAIN RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(train_data)\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Sub sequence plotting', fontsize='30')\n",
        "plt.xlabel('Time', fontsize ='20')\n",
        "plt.ylabel('Feature 3', fontsize='20')\n",
        "plt.plot(train_data[:,:,:],\"r\")\n",
        "plt.plot(X_test_predict[:,:,:],\"b\")\n",
        "plt.show()\n",
        "\n",
        "#PLOT TEST RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(test_data)\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Sub sequence plotting', fontsize='30')\n",
        "plt.xlabel('Time', fontsize ='20')\n",
        "plt.ylabel('Feature 1', fontsize='20')\n",
        "plt.plot(test_data[:,0,0],\"r\")\n",
        "plt.plot(X_test_predict[:,0,0],\"b\")\n",
        "plt.show()\n",
        "\n",
        "fig = px.scatter(None, x=X_test_encoded[2][:,0], y=X_test_encoded[2][:,1],opacity=1, color=window_label.astype(str))\n",
        "fig.update_layout(dict(plot_bgcolor = 'white'))\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()\n",
        "\n",
        "   #vae2.evaluate(test_data)\n",
        "\n",
        "# serialize encoder model to JSON\n",
        "#model_json_encoder = encoder.to_json()\n",
        "#with open(\"model_encoder.json\", \"w\") as json_file:\n",
        "#    json_file.write(model_json_encoder)\n",
        "# serialize encoder weights to HDF5\n",
        "#encoder.save_weights(\"model_encoder.h5\")\n",
        "\n",
        "# serialize whole model to JSON\n",
        "#vae.save(r'/content/drive/MyDrive/PHD/2021/vae-saved.model')\n",
        "#modelVAEjson = vae.to_json()\n",
        "#with open(\"modelVAE.json\", \"w\") as json_file:\n",
        "# json_file.write(modelVAE_json)\n",
        "# serialize weights to HDF5\n",
        "#vae.save_weights(\"modelVAE.h5\")\n",
        "#print(\"Saved model to disk\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1l49YvYwxvvxpB94AufdbhbfzUlMGIUDx",
      "authorship_tag": "ABX9TyMtXt75XRoAeW3A/8r7j3WP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}