{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data%20-%20better%20result-%20mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoP7OuWNxlsJ",
        "outputId": "6f7bdb4d-961f-4ce1-e42c-2d12d7fc6651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "543/543 [==============================] - 5s 5ms/step - loss: 0.9207 - mean_squared_error: 0.9207 - val_loss: 0.9112 - val_mean_squared_error: 0.9112\n",
            "Epoch 2/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.9015 - mean_squared_error: 0.9015 - val_loss: 0.9265 - val_mean_squared_error: 0.9265\n",
            "Epoch 3/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8994 - mean_squared_error: 0.8994 - val_loss: 0.9021 - val_mean_squared_error: 0.9021\n",
            "Epoch 4/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8954 - mean_squared_error: 0.8954 - val_loss: 0.9032 - val_mean_squared_error: 0.9032\n",
            "Epoch 5/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8935 - mean_squared_error: 0.8935 - val_loss: 0.8985 - val_mean_squared_error: 0.8985\n",
            "Epoch 6/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8913 - mean_squared_error: 0.8913 - val_loss: 0.9036 - val_mean_squared_error: 0.9036\n",
            "Epoch 7/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8893 - mean_squared_error: 0.8893 - val_loss: 0.9016 - val_mean_squared_error: 0.9016\n",
            "Epoch 8/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8918 - mean_squared_error: 0.8918 - val_loss: 0.8975 - val_mean_squared_error: 0.8975\n",
            "Epoch 9/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8883 - mean_squared_error: 0.8883 - val_loss: 0.9092 - val_mean_squared_error: 0.9092\n",
            "Epoch 10/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8877 - mean_squared_error: 0.8877 - val_loss: 0.8962 - val_mean_squared_error: 0.8962\n",
            "Epoch 11/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8872 - mean_squared_error: 0.8872 - val_loss: 0.9016 - val_mean_squared_error: 0.9016\n",
            "Epoch 12/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8861 - mean_squared_error: 0.8861 - val_loss: 0.8959 - val_mean_squared_error: 0.8959\n",
            "Epoch 13/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8873 - mean_squared_error: 0.8873 - val_loss: 0.9080 - val_mean_squared_error: 0.9080\n",
            "Epoch 14/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 0.8900 - val_mean_squared_error: 0.8900\n",
            "Epoch 15/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8856 - mean_squared_error: 0.8856 - val_loss: 0.8915 - val_mean_squared_error: 0.8915\n",
            "Epoch 16/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8844 - mean_squared_error: 0.8844 - val_loss: 0.8918 - val_mean_squared_error: 0.8918\n",
            "Epoch 17/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8850 - mean_squared_error: 0.8850 - val_loss: 0.8900 - val_mean_squared_error: 0.8900\n",
            "Epoch 18/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8848 - mean_squared_error: 0.8848 - val_loss: 0.8928 - val_mean_squared_error: 0.8928\n",
            "Epoch 19/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8846 - mean_squared_error: 0.8846 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
            "Epoch 20/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8853 - mean_squared_error: 0.8853 - val_loss: 0.8898 - val_mean_squared_error: 0.8898\n",
            "Epoch 21/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8834 - mean_squared_error: 0.8834 - val_loss: 0.9018 - val_mean_squared_error: 0.9018\n",
            "Epoch 22/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8836 - mean_squared_error: 0.8836 - val_loss: 0.8897 - val_mean_squared_error: 0.8897\n",
            "Epoch 23/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8825 - mean_squared_error: 0.8825 - val_loss: 0.8886 - val_mean_squared_error: 0.8886\n",
            "Epoch 24/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8827 - mean_squared_error: 0.8827 - val_loss: 0.8974 - val_mean_squared_error: 0.8974\n",
            "Epoch 25/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8834 - mean_squared_error: 0.8834 - val_loss: 0.8901 - val_mean_squared_error: 0.8901\n",
            "Epoch 26/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8812 - mean_squared_error: 0.8812 - val_loss: 0.8900 - val_mean_squared_error: 0.8900\n",
            "Epoch 27/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8815 - mean_squared_error: 0.8815 - val_loss: 0.8898 - val_mean_squared_error: 0.8898\n",
            "Epoch 28/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8819 - mean_squared_error: 0.8819 - val_loss: 0.8943 - val_mean_squared_error: 0.8943\n",
            "Epoch 29/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8818 - mean_squared_error: 0.8818 - val_loss: 0.8866 - val_mean_squared_error: 0.8866\n",
            "Epoch 30/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8818 - mean_squared_error: 0.8818 - val_loss: 0.8971 - val_mean_squared_error: 0.8971\n",
            "Epoch 31/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8823 - mean_squared_error: 0.8823 - val_loss: 0.8876 - val_mean_squared_error: 0.8876\n",
            "Epoch 32/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8826 - mean_squared_error: 0.8826 - val_loss: 0.8877 - val_mean_squared_error: 0.8877\n",
            "Epoch 33/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8821 - mean_squared_error: 0.8821 - val_loss: 0.8879 - val_mean_squared_error: 0.8879\n",
            "Epoch 34/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8816 - mean_squared_error: 0.8816 - val_loss: 0.8952 - val_mean_squared_error: 0.8952\n",
            "Epoch 35/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8810 - mean_squared_error: 0.8810 - val_loss: 0.8901 - val_mean_squared_error: 0.8901\n",
            "Epoch 36/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8811 - mean_squared_error: 0.8811 - val_loss: 0.8909 - val_mean_squared_error: 0.8909\n",
            "Epoch 37/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8813 - mean_squared_error: 0.8813 - val_loss: 0.8866 - val_mean_squared_error: 0.8866\n",
            "Epoch 38/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8803 - mean_squared_error: 0.8803 - val_loss: 0.8986 - val_mean_squared_error: 0.8986\n",
            "Epoch 39/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8805 - mean_squared_error: 0.8805 - val_loss: 0.8895 - val_mean_squared_error: 0.8895\n",
            "Epoch 40/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8804 - mean_squared_error: 0.8804 - val_loss: 0.8872 - val_mean_squared_error: 0.8872\n",
            "Epoch 41/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8819 - mean_squared_error: 0.8819 - val_loss: 0.8861 - val_mean_squared_error: 0.8861\n",
            "Epoch 42/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8814 - mean_squared_error: 0.8814 - val_loss: 0.8897 - val_mean_squared_error: 0.8897\n",
            "Epoch 43/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8804 - mean_squared_error: 0.8804 - val_loss: 0.8952 - val_mean_squared_error: 0.8952\n",
            "Epoch 44/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8795 - mean_squared_error: 0.8795 - val_loss: 0.8905 - val_mean_squared_error: 0.8905\n",
            "Epoch 45/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8799 - mean_squared_error: 0.8799 - val_loss: 0.8862 - val_mean_squared_error: 0.8862\n",
            "Epoch 46/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8805 - mean_squared_error: 0.8805 - val_loss: 0.8885 - val_mean_squared_error: 0.8885\n",
            "Epoch 47/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8803 - mean_squared_error: 0.8803 - val_loss: 0.8871 - val_mean_squared_error: 0.8871\n",
            "Epoch 48/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8799 - mean_squared_error: 0.8799 - val_loss: 0.8882 - val_mean_squared_error: 0.8882\n",
            "Epoch 49/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8802 - mean_squared_error: 0.8802 - val_loss: 0.8873 - val_mean_squared_error: 0.8873\n",
            "Epoch 50/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8790 - mean_squared_error: 0.8790 - val_loss: 0.8958 - val_mean_squared_error: 0.8958\n",
            "Epoch 51/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8805 - mean_squared_error: 0.8805 - val_loss: 0.8853 - val_mean_squared_error: 0.8853\n",
            "Epoch 52/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8792 - mean_squared_error: 0.8792 - val_loss: 0.8920 - val_mean_squared_error: 0.8920\n",
            "Epoch 53/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8805 - mean_squared_error: 0.8805 - val_loss: 0.8907 - val_mean_squared_error: 0.8907\n",
            "Epoch 54/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8797 - mean_squared_error: 0.8797 - val_loss: 0.8920 - val_mean_squared_error: 0.8920\n",
            "Epoch 55/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8801 - mean_squared_error: 0.8801 - val_loss: 0.8948 - val_mean_squared_error: 0.8948\n",
            "Epoch 56/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8797 - mean_squared_error: 0.8797 - val_loss: 0.8873 - val_mean_squared_error: 0.8873\n",
            "Epoch 57/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8794 - mean_squared_error: 0.8794 - val_loss: 0.8870 - val_mean_squared_error: 0.8870\n",
            "Epoch 58/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8800 - mean_squared_error: 0.8800 - val_loss: 0.8855 - val_mean_squared_error: 0.8855\n",
            "Epoch 59/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8796 - mean_squared_error: 0.8796 - val_loss: 0.8865 - val_mean_squared_error: 0.8865\n",
            "Epoch 60/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8795 - mean_squared_error: 0.8795 - val_loss: 0.8858 - val_mean_squared_error: 0.8858\n",
            "Epoch 61/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8782 - mean_squared_error: 0.8782 - val_loss: 0.8843 - val_mean_squared_error: 0.8843\n",
            "Epoch 62/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8793 - mean_squared_error: 0.8793 - val_loss: 0.8869 - val_mean_squared_error: 0.8869\n",
            "Epoch 63/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8793 - mean_squared_error: 0.8793 - val_loss: 0.8904 - val_mean_squared_error: 0.8904\n",
            "Epoch 64/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8789 - mean_squared_error: 0.8789 - val_loss: 0.8865 - val_mean_squared_error: 0.8865\n",
            "Epoch 65/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8784 - mean_squared_error: 0.8784 - val_loss: 0.8921 - val_mean_squared_error: 0.8921\n",
            "Epoch 66/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8797 - mean_squared_error: 0.8797 - val_loss: 0.8878 - val_mean_squared_error: 0.8878\n",
            "Epoch 67/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8787 - mean_squared_error: 0.8787 - val_loss: 0.8862 - val_mean_squared_error: 0.8862\n",
            "Epoch 68/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8792 - mean_squared_error: 0.8792 - val_loss: 0.8905 - val_mean_squared_error: 0.8905\n",
            "Epoch 69/500\n",
            "543/543 [==============================] - 3s 6ms/step - loss: 0.8785 - mean_squared_error: 0.8785 - val_loss: 0.8850 - val_mean_squared_error: 0.8850\n",
            "Epoch 70/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8778 - mean_squared_error: 0.8778 - val_loss: 0.8880 - val_mean_squared_error: 0.8880\n",
            "Epoch 71/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8794 - mean_squared_error: 0.8794 - val_loss: 0.8874 - val_mean_squared_error: 0.8874\n",
            "Epoch 72/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8789 - mean_squared_error: 0.8789 - val_loss: 0.8857 - val_mean_squared_error: 0.8857\n",
            "Epoch 73/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8790 - mean_squared_error: 0.8790 - val_loss: 0.8859 - val_mean_squared_error: 0.8859\n",
            "Epoch 74/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8775 - mean_squared_error: 0.8775 - val_loss: 0.8964 - val_mean_squared_error: 0.8964\n",
            "Epoch 75/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8782 - mean_squared_error: 0.8782 - val_loss: 0.8873 - val_mean_squared_error: 0.8873\n",
            "Epoch 76/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8788 - mean_squared_error: 0.8788 - val_loss: 0.8849 - val_mean_squared_error: 0.8849\n",
            "Epoch 77/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8788 - mean_squared_error: 0.8788 - val_loss: 0.8853 - val_mean_squared_error: 0.8853\n",
            "Epoch 78/500\n",
            "543/543 [==============================] - 4s 7ms/step - loss: 0.8794 - mean_squared_error: 0.8794 - val_loss: 0.8852 - val_mean_squared_error: 0.8852\n",
            "Epoch 79/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8785 - mean_squared_error: 0.8785 - val_loss: 0.8865 - val_mean_squared_error: 0.8865\n",
            "Epoch 80/500\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8785 - mean_squared_error: 0.8785 - val_loss: 0.8894 - val_mean_squared_error: 0.8894\n",
            "Epoch 81/500\n",
            "536/543 [============================>.] - ETA: 0s - loss: 0.8787 - mean_squared_error: 0.8787Restoring model weights from the end of the best epoch: 61.\n",
            "543/543 [==============================] - 3s 5ms/step - loss: 0.8778 - mean_squared_error: 0.8778 - val_loss: 0.8888 - val_mean_squared_error: 0.8888\n",
            "Epoch 81: early stopping\n",
            "943/943 [==============================] - 2s 2ms/step\n",
            "105/105 [==============================] - 0s 2ms/step\n",
            "r2 score is == 0.12338892440599891\n",
            "r2 score is == 0.11568493695962023\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "generator_multiply = 1 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TRAIN.npy')\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "\n",
        "\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy') #------for Hourly data\n",
        "index = 500\n",
        "#We missed i=500 from processing the iosw. So here we are dropping row with index =500\n",
        "train_data= np.delete(train_data, index, axis=0)\n",
        "\n",
        "\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TEST_hourly.npy')\n",
        "#all_data = np.concatenate((train_data,test_data),axis=0)\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-TRAIN_hourly.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "\n",
        "#---------------------------VAE ------------------------------------------\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#x_train, x_test, y_train, y_test = train_test_split(train_data, window_label, test_size = 0.2, random_state = 42)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-hourly-latent10-dim128-latest.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-hourly-latent10-dim128-latest.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly.npy')\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly',results1)\n",
        "\n",
        "\n",
        "#Regression fitting\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#----- get a test set from this data, to avoid further wrangling----------------\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
        "\n",
        "\n",
        "maxval = x.shape[0]\n",
        "count_train = int(math.ceil(0.9*maxval))\n",
        "x_train = x[:count_train]\n",
        "x_test = x[count_train:]\n",
        "\n",
        "y_train = y[:count_train]\n",
        "y_test = y[count_train:]\n",
        "\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x_train)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x_train, y_train = x_train[mask, :], y_train[mask]\n",
        "\n",
        "##x_train = x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "#x_test = x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "transformer = StandardScaler()\n",
        "\n",
        "y_train_transformed = transformer.fit_transform(y_train.reshape(-1,1)).flatten()\n",
        "y_test_transformed = transformer.fit_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(LSTM(1024, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(512,return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 256, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 128, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 32, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 8, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 4, activation='relu'))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 1))\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.0001,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "\n",
        "#reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "n_epochs = 500\n",
        "#model.fit(x_train, y_train,epochs=5, batch_size=50, verbose=True)\n",
        "\n",
        "#y_pred = model.predict(x_test)\n",
        "\n",
        "#transform\n",
        "\n",
        "#x_val = x_train[10001:13000]\n",
        "#y_val = y_train_transformed[10001:13000]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train_transformed, test_size = 0.1, random_state = 42)\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=50,\n",
        "                   #validation_data=(x_val,y_val),\n",
        "                   validation_split=0.1,\n",
        "                 callbacks=[es])\n",
        "\n",
        "y_train_pred = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = transformer.inverse_transform(y_pred)\n",
        "\n",
        "y_train = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "y_test = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score_train= r2_score(y_train,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "score= r2_score(y_test,y_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_train[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_test[100:150], color = 'red', label = 'Real data')\n",
        "plt.plot(y_pred[100:150], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/preduber_2.csv',y_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/realuber_2.csv',y_test)\n",
        "print(\"MAE is==\",mean_absolute_error(y_test,y_pred))\n",
        "\n",
        "#--------------------------------------Random Forest-------------------------------------------------------\n",
        "\n",
        "\n",
        "###### Random forrest ergression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " # create regressor object\n",
        "regressor = RandomForestRegressor(n_estimators = 1000, random_state = 0)\n",
        "# fit the regressor with x and y data\n",
        "regressor.fit(x_train[0:10000],y_train_transformed[0:10000])\n",
        "\n",
        "\n",
        "y_train_pred = regressor.predict(x_train[0:10000])\n",
        "y_train_pred = y_train_pred.reshape(-1,1)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred)\n",
        "\n",
        "y_pred = regressor.predict(x_test[0:1000])\n",
        "y_pred = y_pred.reshape(-1,1)\n",
        "y_pred = transformer.inverse_transform(y_pred)\n",
        "\n",
        "\n",
        "score_train= r2_score(y_train[0:10000],y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "score= r2_score(y_test[0:1000],y_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_train[0:1000], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:1000], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plt.scatter(y_test,y_pred);\n",
        "#plt.xlabel('Actual');\n",
        "#plt.ylabel('Predicted');\n",
        "plt.plot(y_test[0:1000], color = 'red', label = 'Real data')\n",
        "plt.plot(y_pred[0:1000], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sns.regplot(x=y_test[0:10],y=y_pred,ci=None,color ='red');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data.ipynb",
      "authorship_tag": "ABX9TyOkBuWTxzF1CArGCe31Kft0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}