{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data%20-%20MLP-%20Final%20-%20Window%20less%20than%2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoP7OuWNxlsJ",
        "outputId": "64c41a18-8c21-4ec6-9cd5-4e751e57b9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "485/485 [==============================] - 4s 5ms/step - loss: 0.7682 - mean_squared_error: 0.7682 - val_loss: 0.6073 - val_mean_squared_error: 0.6073\n",
            "Epoch 2/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6873 - mean_squared_error: 0.6873 - val_loss: 0.7076 - val_mean_squared_error: 0.7076\n",
            "Epoch 3/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.7206 - mean_squared_error: 0.7206 - val_loss: 0.7935 - val_mean_squared_error: 0.7935\n",
            "Epoch 4/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6700 - mean_squared_error: 0.6700 - val_loss: 0.5870 - val_mean_squared_error: 0.5870\n",
            "Epoch 5/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6716 - mean_squared_error: 0.6716 - val_loss: 0.7040 - val_mean_squared_error: 0.7040\n",
            "Epoch 6/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6696 - mean_squared_error: 0.6696 - val_loss: 0.5863 - val_mean_squared_error: 0.5863\n",
            "Epoch 7/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6589 - mean_squared_error: 0.6589 - val_loss: 0.5813 - val_mean_squared_error: 0.5813\n",
            "Epoch 8/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6531 - mean_squared_error: 0.6531 - val_loss: 0.5952 - val_mean_squared_error: 0.5952\n",
            "Epoch 9/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6386 - mean_squared_error: 0.6386 - val_loss: 0.6110 - val_mean_squared_error: 0.6110\n",
            "Epoch 10/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6417 - mean_squared_error: 0.6417 - val_loss: 0.6029 - val_mean_squared_error: 0.6029\n",
            "Epoch 11/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6499 - mean_squared_error: 0.6499 - val_loss: 0.5867 - val_mean_squared_error: 0.5867\n",
            "Epoch 12/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6499 - mean_squared_error: 0.6499 - val_loss: 0.5992 - val_mean_squared_error: 0.5992\n",
            "Epoch 13/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6472 - mean_squared_error: 0.6472 - val_loss: 0.6293 - val_mean_squared_error: 0.6293\n",
            "Epoch 14/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6247 - mean_squared_error: 0.6247 - val_loss: 0.5907 - val_mean_squared_error: 0.5907\n",
            "Epoch 15/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6516 - mean_squared_error: 0.6516 - val_loss: 0.6422 - val_mean_squared_error: 0.6422\n",
            "Epoch 16/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6287 - mean_squared_error: 0.6287 - val_loss: 0.5951 - val_mean_squared_error: 0.5951\n",
            "Epoch 17/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6312 - mean_squared_error: 0.6312 - val_loss: 0.6230 - val_mean_squared_error: 0.6230\n",
            "Epoch 18/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6400 - mean_squared_error: 0.6400 - val_loss: 0.5885 - val_mean_squared_error: 0.5885\n",
            "Epoch 19/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6310 - mean_squared_error: 0.6310 - val_loss: 0.6038 - val_mean_squared_error: 0.6038\n",
            "Epoch 20/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6238 - mean_squared_error: 0.6238 - val_loss: 0.5974 - val_mean_squared_error: 0.5974\n",
            "Epoch 21/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6214 - mean_squared_error: 0.6214 - val_loss: 0.5839 - val_mean_squared_error: 0.5839\n",
            "Epoch 22/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6294 - mean_squared_error: 0.6294 - val_loss: 0.5935 - val_mean_squared_error: 0.5935\n",
            "Epoch 23/500\n",
            "485/485 [==============================] - 4s 8ms/step - loss: 0.6292 - mean_squared_error: 0.6292 - val_loss: 0.5881 - val_mean_squared_error: 0.5881\n",
            "Epoch 24/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6328 - mean_squared_error: 0.6328 - val_loss: 0.6346 - val_mean_squared_error: 0.6346\n",
            "Epoch 25/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6266 - mean_squared_error: 0.6266 - val_loss: 0.5836 - val_mean_squared_error: 0.5836\n",
            "Epoch 26/500\n",
            "485/485 [==============================] - 4s 7ms/step - loss: 0.6249 - mean_squared_error: 0.6249 - val_loss: 0.5834 - val_mean_squared_error: 0.5834\n",
            "Epoch 27/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6349 - mean_squared_error: 0.6349 - val_loss: 0.5903 - val_mean_squared_error: 0.5903\n",
            "Epoch 28/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6248 - mean_squared_error: 0.6248 - val_loss: 0.5928 - val_mean_squared_error: 0.5928\n",
            "Epoch 29/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6318 - mean_squared_error: 0.6318 - val_loss: 0.5834 - val_mean_squared_error: 0.5834\n",
            "Epoch 30/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6173 - mean_squared_error: 0.6173 - val_loss: 0.6097 - val_mean_squared_error: 0.6097\n",
            "Epoch 31/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6300 - mean_squared_error: 0.6300 - val_loss: 0.5912 - val_mean_squared_error: 0.5912\n",
            "Epoch 32/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6245 - mean_squared_error: 0.6245 - val_loss: 0.5927 - val_mean_squared_error: 0.5927\n",
            "Epoch 33/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6188 - mean_squared_error: 0.6188 - val_loss: 0.6186 - val_mean_squared_error: 0.6186\n",
            "Epoch 34/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6234 - mean_squared_error: 0.6234 - val_loss: 0.5971 - val_mean_squared_error: 0.5971\n",
            "Epoch 35/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6232 - mean_squared_error: 0.6232 - val_loss: 0.5827 - val_mean_squared_error: 0.5827\n",
            "Epoch 36/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 0.5993 - val_mean_squared_error: 0.5993\n",
            "Epoch 37/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6249 - mean_squared_error: 0.6249 - val_loss: 0.5843 - val_mean_squared_error: 0.5843\n",
            "Epoch 38/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6210 - mean_squared_error: 0.6210 - val_loss: 0.6395 - val_mean_squared_error: 0.6395\n",
            "Epoch 39/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6206 - mean_squared_error: 0.6206 - val_loss: 0.5843 - val_mean_squared_error: 0.5843\n",
            "Epoch 40/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6140 - mean_squared_error: 0.6140 - val_loss: 0.5824 - val_mean_squared_error: 0.5824\n",
            "Epoch 41/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6201 - mean_squared_error: 0.6201 - val_loss: 0.5861 - val_mean_squared_error: 0.5861\n",
            "Epoch 42/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6154 - mean_squared_error: 0.6154 - val_loss: 0.5817 - val_mean_squared_error: 0.5817\n",
            "Epoch 43/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6224 - mean_squared_error: 0.6224 - val_loss: 0.5821 - val_mean_squared_error: 0.5821\n",
            "Epoch 44/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6141 - mean_squared_error: 0.6141 - val_loss: 0.5820 - val_mean_squared_error: 0.5820\n",
            "Epoch 45/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6200 - mean_squared_error: 0.6200 - val_loss: 0.5821 - val_mean_squared_error: 0.5821\n",
            "Epoch 46/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6233 - mean_squared_error: 0.6233 - val_loss: 0.5998 - val_mean_squared_error: 0.5998\n",
            "Epoch 47/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6170 - mean_squared_error: 0.6170 - val_loss: 0.6390 - val_mean_squared_error: 0.6390\n",
            "Epoch 48/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 0.5856 - val_mean_squared_error: 0.5856\n",
            "Epoch 49/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6171 - mean_squared_error: 0.6171 - val_loss: 0.5824 - val_mean_squared_error: 0.5824\n",
            "Epoch 50/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6213 - mean_squared_error: 0.6213 - val_loss: 0.5916 - val_mean_squared_error: 0.5916\n",
            "Epoch 51/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6233 - mean_squared_error: 0.6233 - val_loss: 0.5815 - val_mean_squared_error: 0.5815\n",
            "Epoch 52/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6134 - mean_squared_error: 0.6134 - val_loss: 0.6016 - val_mean_squared_error: 0.6016\n",
            "Epoch 53/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6317 - mean_squared_error: 0.6317 - val_loss: 0.5833 - val_mean_squared_error: 0.5833\n",
            "Epoch 54/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6164 - mean_squared_error: 0.6164 - val_loss: 0.6106 - val_mean_squared_error: 0.6106\n",
            "Epoch 55/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6252 - mean_squared_error: 0.6252 - val_loss: 0.5830 - val_mean_squared_error: 0.5830\n",
            "Epoch 56/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6192 - mean_squared_error: 0.6192 - val_loss: 0.5814 - val_mean_squared_error: 0.5814\n",
            "Epoch 57/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6185 - mean_squared_error: 0.6185 - val_loss: 0.5836 - val_mean_squared_error: 0.5836\n",
            "Epoch 58/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6205 - mean_squared_error: 0.6205 - val_loss: 0.5812 - val_mean_squared_error: 0.5812\n",
            "Epoch 59/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6219 - mean_squared_error: 0.6219 - val_loss: 0.6948 - val_mean_squared_error: 0.6948\n",
            "Epoch 60/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6181 - mean_squared_error: 0.6181 - val_loss: 0.5812 - val_mean_squared_error: 0.5812\n",
            "Epoch 61/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6207 - mean_squared_error: 0.6207 - val_loss: 0.6142 - val_mean_squared_error: 0.6142\n",
            "Epoch 62/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6221 - mean_squared_error: 0.6221 - val_loss: 0.6418 - val_mean_squared_error: 0.6418\n",
            "Epoch 63/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 0.5859 - val_mean_squared_error: 0.5859\n",
            "Epoch 64/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6113 - mean_squared_error: 0.6113 - val_loss: 0.5835 - val_mean_squared_error: 0.5835\n",
            "Epoch 65/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6153 - mean_squared_error: 0.6153 - val_loss: 0.6073 - val_mean_squared_error: 0.6073\n",
            "Epoch 66/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6131 - mean_squared_error: 0.6131 - val_loss: 0.5839 - val_mean_squared_error: 0.5839\n",
            "Epoch 67/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6155 - mean_squared_error: 0.6155 - val_loss: 0.5885 - val_mean_squared_error: 0.5885\n",
            "Epoch 68/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6132 - mean_squared_error: 0.6132 - val_loss: 0.6066 - val_mean_squared_error: 0.6066\n",
            "Epoch 69/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6177 - mean_squared_error: 0.6177 - val_loss: 0.6048 - val_mean_squared_error: 0.6048\n",
            "Epoch 70/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6172 - mean_squared_error: 0.6172 - val_loss: 0.5848 - val_mean_squared_error: 0.5848\n",
            "Epoch 71/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6176 - mean_squared_error: 0.6176 - val_loss: 0.5965 - val_mean_squared_error: 0.5965\n",
            "Epoch 72/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6130 - mean_squared_error: 0.6130 - val_loss: 0.5990 - val_mean_squared_error: 0.5990\n",
            "Epoch 73/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6193 - mean_squared_error: 0.6193 - val_loss: 0.5893 - val_mean_squared_error: 0.5893\n",
            "Epoch 74/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6183 - mean_squared_error: 0.6183 - val_loss: 0.5986 - val_mean_squared_error: 0.5986\n",
            "Epoch 75/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6126 - mean_squared_error: 0.6126 - val_loss: 0.6044 - val_mean_squared_error: 0.6044\n",
            "Epoch 76/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6186 - mean_squared_error: 0.6186 - val_loss: 0.5924 - val_mean_squared_error: 0.5924\n",
            "Epoch 77/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6163 - mean_squared_error: 0.6163 - val_loss: 0.6010 - val_mean_squared_error: 0.6010\n",
            "Epoch 78/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6209 - mean_squared_error: 0.6209 - val_loss: 0.6039 - val_mean_squared_error: 0.6039\n",
            "Epoch 79/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6201 - mean_squared_error: 0.6201 - val_loss: 0.5825 - val_mean_squared_error: 0.5825\n",
            "Epoch 80/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6143 - mean_squared_error: 0.6143 - val_loss: 0.6035 - val_mean_squared_error: 0.6035\n",
            "Epoch 81/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6148 - mean_squared_error: 0.6148 - val_loss: 0.6530 - val_mean_squared_error: 0.6530\n",
            "Epoch 82/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6112 - mean_squared_error: 0.6112 - val_loss: 0.6149 - val_mean_squared_error: 0.6149\n",
            "Epoch 83/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6096 - mean_squared_error: 0.6096 - val_loss: 0.6012 - val_mean_squared_error: 0.6012\n",
            "Epoch 84/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6119 - mean_squared_error: 0.6119 - val_loss: 0.5933 - val_mean_squared_error: 0.5933\n",
            "Epoch 85/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6148 - mean_squared_error: 0.6148 - val_loss: 0.5940 - val_mean_squared_error: 0.5940\n",
            "Epoch 86/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6093 - mean_squared_error: 0.6093 - val_loss: 0.5854 - val_mean_squared_error: 0.5854\n",
            "Epoch 87/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6155 - mean_squared_error: 0.6155 - val_loss: 0.5867 - val_mean_squared_error: 0.5867\n",
            "Epoch 88/500\n",
            "485/485 [==============================] - 2s 4ms/step - loss: 0.6104 - mean_squared_error: 0.6104 - val_loss: 0.5843 - val_mean_squared_error: 0.5843\n",
            "Epoch 89/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6079 - mean_squared_error: 0.6079 - val_loss: 0.5966 - val_mean_squared_error: 0.5966\n",
            "Epoch 90/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6162 - mean_squared_error: 0.6162 - val_loss: 0.6486 - val_mean_squared_error: 0.6486\n",
            "Epoch 91/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6137 - mean_squared_error: 0.6137 - val_loss: 0.6178 - val_mean_squared_error: 0.6178\n",
            "Epoch 92/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6117 - mean_squared_error: 0.6117 - val_loss: 0.5835 - val_mean_squared_error: 0.5835\n",
            "Epoch 93/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6144 - mean_squared_error: 0.6144 - val_loss: 0.5945 - val_mean_squared_error: 0.5945\n",
            "Epoch 94/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6197 - mean_squared_error: 0.6197 - val_loss: 0.5887 - val_mean_squared_error: 0.5887\n",
            "Epoch 95/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6118 - mean_squared_error: 0.6118 - val_loss: 0.5849 - val_mean_squared_error: 0.5849\n",
            "Epoch 96/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6102 - mean_squared_error: 0.6102 - val_loss: 0.5914 - val_mean_squared_error: 0.5914\n",
            "Epoch 97/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6109 - mean_squared_error: 0.6109 - val_loss: 0.5861 - val_mean_squared_error: 0.5861\n",
            "Epoch 98/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6116 - mean_squared_error: 0.6116 - val_loss: 0.5904 - val_mean_squared_error: 0.5904\n",
            "Epoch 99/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6113 - mean_squared_error: 0.6113 - val_loss: 0.6186 - val_mean_squared_error: 0.6186\n",
            "Epoch 100/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6099 - mean_squared_error: 0.6099 - val_loss: 0.5935 - val_mean_squared_error: 0.5935\n",
            "Epoch 101/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6133 - mean_squared_error: 0.6133 - val_loss: 0.6182 - val_mean_squared_error: 0.6182\n",
            "Epoch 102/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6154 - mean_squared_error: 0.6154 - val_loss: 0.5918 - val_mean_squared_error: 0.5918\n",
            "Epoch 103/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6058 - mean_squared_error: 0.6058 - val_loss: 0.5810 - val_mean_squared_error: 0.5810\n",
            "Epoch 104/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6139 - mean_squared_error: 0.6139 - val_loss: 0.6009 - val_mean_squared_error: 0.6009\n",
            "Epoch 105/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6105 - mean_squared_error: 0.6105 - val_loss: 0.5892 - val_mean_squared_error: 0.5892\n",
            "Epoch 106/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6182 - mean_squared_error: 0.6182 - val_loss: 0.5861 - val_mean_squared_error: 0.5861\n",
            "Epoch 107/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6175 - mean_squared_error: 0.6175 - val_loss: 0.6199 - val_mean_squared_error: 0.6199\n",
            "Epoch 108/500\n",
            "485/485 [==============================] - 3s 7ms/step - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 0.5852 - val_mean_squared_error: 0.5852\n",
            "Epoch 109/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6085 - mean_squared_error: 0.6085 - val_loss: 0.5827 - val_mean_squared_error: 0.5827\n",
            "Epoch 110/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6088 - mean_squared_error: 0.6088 - val_loss: 0.5884 - val_mean_squared_error: 0.5884\n",
            "Epoch 111/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6080 - mean_squared_error: 0.6080 - val_loss: 0.5974 - val_mean_squared_error: 0.5974\n",
            "Epoch 112/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6106 - mean_squared_error: 0.6106 - val_loss: 0.5976 - val_mean_squared_error: 0.5976\n",
            "Epoch 113/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6072 - mean_squared_error: 0.6072 - val_loss: 0.6117 - val_mean_squared_error: 0.6117\n",
            "Epoch 114/500\n",
            "485/485 [==============================] - 3s 6ms/step - loss: 0.6137 - mean_squared_error: 0.6137 - val_loss: 0.5912 - val_mean_squared_error: 0.5912\n",
            "Epoch 115/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6119 - mean_squared_error: 0.6119 - val_loss: 0.5845 - val_mean_squared_error: 0.5845\n",
            "Epoch 116/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6126 - mean_squared_error: 0.6126 - val_loss: 0.5864 - val_mean_squared_error: 0.5864\n",
            "Epoch 117/500\n",
            "485/485 [==============================] - 3s 5ms/step - loss: 0.6149 - mean_squared_error: 0.6149 - val_loss: 0.5879 - val_mean_squared_error: 0.5879\n",
            "Epoch 118/500\n",
            "485/485 [==============================] - 2s 5ms/step - loss: 0.6134 - mean_squared_error: 0.6134 - val_loss: 0.5890 - val_mean_squared_error: 0.5890\n",
            "Epoch 119/500\n",
            "350/485 [====================>.........] - ETA: 0s - loss: 0.6150 - mean_squared_error: 0.6150"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "#from keras import ops\n",
        "\n",
        "generator_multiply = 4 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy') #------for Hourly data\n",
        "index = 500\n",
        "#We missed i=500 from processing the iosw. So here we are dropping row with index =500\n",
        "train_data= np.delete(train_data, index, axis=0)\n",
        "\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-TRAIN_hourly.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "\n",
        "##---------------------------IGNORE THIS IF NOT GENERATING FRESH VAE DATASET--------------------------------------------------------------------------------------------\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-hourly-latent10-dim128-latest.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-hourly-latent10-dim128-latest.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly_X4',results1)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly_X4.npy')\n",
        "\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "#maxval = x.shape[0]\n",
        "#count_train = int(math.ceil(0.9*maxval))\n",
        "#x_train = x[:count_train]\n",
        "#x_test = x[count_train:]\n",
        "\n",
        "#y_train = y[:count_train]\n",
        "#y_test = y[count_train:]\n",
        "\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x, y = x[mask, :], y[mask]\n",
        "\n",
        "\n",
        "\n",
        "###############Scale the target and then split the data into train test----------------------------------------------------------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(y, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "#Looking at the dist, we remove al y less than 20\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "p = np.argwhere(y<20).flatten()\n",
        "xf = np.take(x, p, 0)\n",
        "yf= np.take(y, p, 0)\n",
        "transformer = PowerTransformer()\n",
        "y_transformed = transformer.fit_transform(yf.reshape(-1,1)).flatten()\n",
        "x=xf\n",
        "\n",
        "\n",
        "\n",
        "plt.title(\"Distribution after Transformation\", fontsize=15)\n",
        "sns.histplot(y_transformed,bins=10, kde=True , legend=False)\n",
        "plt.xlabel(\"window\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xf, y_transformed, test_size = 0.05, random_state = 42)\n",
        "\n",
        "#--------------------------------------------------------------------------------------CONSTRUCT, COMPILE, AND TRAIN THE MODEL------------------------------------------------------------------------------------------------------------------\n",
        "#------------MLP------------------------------------------------------\n",
        "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(LSTM(1024, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(512,return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 128))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(units = 32))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "#--------------------------------------------------------------LSTM--------------------------\n",
        "\n",
        "a =  x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "b =  x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(a.shape[1],a.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 16))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 4))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 1, activation = 'linear'))\n",
        "model.summary()\n",
        "x_train = a\n",
        "x_test = b\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.001,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.0001, monitor='loss', mode='min', restore_best_weights=True)\n",
        "n_epochs = 500\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=32,\n",
        "                   validation_split=0.3,\n",
        "                 callbacks=[es])\n",
        "\n",
        "#-----------------------------------------------------TRAIN EVALUATION----------------------------------------------------------------\n",
        "y_train_pred_raw = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred_raw)\n",
        "y_train_true = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "\n",
        "score_train= r2_score(y_train_true,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "plt.plot(y_train_true[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------------------------TEST EVALUATION----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[100:150], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[100:150], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------SAVE MODEL AND RESULTS-----------------------------------------------------------------\n",
        "\n",
        "model.save(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_hourly.keras')\n",
        "# It can be used to reconstruct the model identically.\n",
        "reconstructed_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_hourly.keras')\n",
        "\n",
        "# Let's check:\n",
        "np.testing.assert_allclose(\n",
        "    model.predict(test_input), reconstructed_model.predict(test_input)\n",
        ")\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/preduber_2.csv',y_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/realuber_2.csv',y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data%20-%20MLP-%20Final.ipynb",
      "authorship_tag": "ABX9TyN2+X1rzAzbgms46nJIgUcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}