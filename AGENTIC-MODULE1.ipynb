{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T-BJGPTX7XS5"
      },
      "outputs": [],
      "source": [
        "!pip install statsmodels --upgrade\n",
        "!pip install -U lingam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tools.eval_measures import rmse, aic\n",
        "import ast\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from numpy import arange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from importlib.metadata import version\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "# ================================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ================================================================================\n",
        "\n",
        "# Load MetroPT-3 dataset\n",
        "df=pd.read_csv(r'/content/drive/MyDrive/PHD/metropt+3+dataset (1).zip (Unzipped Files)/MetroPT3(AirCompressor).csv', parse_dates={'datetime':[1]}, index_col=['datetime'])\n",
        "\n",
        "# Handle missing values\n",
        "df = df.replace('?', np.nan)\n",
        "df.isnull().sum()\n",
        "\n",
        "def fill_missing(values):\n",
        "    one_day = 24*6\n",
        "    for row in range(df.shape[0]):\n",
        "        for col in range(df.shape[1]):\n",
        "            if np.isnan(values[row][col]):\n",
        "                values[row,col] = values[row-one_day,col]\n",
        "\n",
        "df = df.astype('float32')\n",
        "fill_missing(df.values)\n",
        "\n",
        "# Resample to hourly data to reduce computation\n",
        "daily_df = df.resample('1H').mean().backfill()\n",
        "\n",
        "# Convert index to column and filter to April 2020\n",
        "daily_df['datetime']=daily_df.index\n",
        "daily_df = daily_df.loc[(daily_df['datetime'] >= '2020-04-01')]\n",
        "\n",
        "# Remove index and datetime columns\n",
        "daily_df.drop(daily_df.columns[0], axis=1, inplace=True) # remove ID column if exists\n",
        "daily_df.drop(daily_df.columns[-1], axis=1, inplace=True) # remove datetime column\n",
        "\n",
        "# Scaling the values\n",
        "whole_series = daily_df\n",
        "\n",
        "scalers={}\n",
        "for i in daily_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(whole_series[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    whole_series[i]=s_s\n",
        "\n",
        "# ================================================================================\n",
        "# GRANGER CAUSALITY FUNCTIONS\n",
        "# ================================================================================\n",
        "\n",
        "def granger_causation_matrix(data, variables, max_lag=25, test='ssr_chi2test', verbose=False):\n",
        "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            try:\n",
        "                test_result = grangercausalitytests(data[[r,c]], maxlag=max_lag, verbose=False)\n",
        "                p_values = [round(test_result[i+1][0][test][1],4) for i in range(max_lag)]\n",
        "                min_p_value = np.min(p_values)\n",
        "                df.loc[r, c] = min_p_value\n",
        "            except:\n",
        "                df.loc[r, c] = 1.0  # No causality if test fails\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "# AUTOMATED FEATURE SELECTION based on Granger Causality\n",
        "def auto_feature_selection(data, max_lag='adaptive', significance_level=0.05, min_features=3, removal_strategy='conservative'):\n",
        "    \"\"\"Automatically remove features with weak causal relationships\"\"\"\n",
        "    print(\"Starting automated feature selection...\")\n",
        "    print(f\"Input data shape: {data.shape}\")\n",
        "\n",
        "    # Adaptive max_lag based on data characteristics\n",
        "    if max_lag == 'adaptive':\n",
        "        data_length = len(data)\n",
        "        if data_length > 2000:  # Large dataset\n",
        "            max_lag = min(24, data_length // 100)  # Up to daily cycle\n",
        "        elif data_length > 1000:  # Medium dataset\n",
        "            max_lag = min(12, data_length // 80)   # Half-day cycle\n",
        "        else:  # Small dataset\n",
        "            max_lag = min(6, data_length // 50)    # Quarter-day cycle\n",
        "\n",
        "        print(f\"Using adaptive max_lag: {max_lag} (data length: {data_length})\")\n",
        "\n",
        "    # Quick Granger causality with adaptive max_lag\n",
        "    print(\"Running Granger causality tests...\")\n",
        "    gc_matrix = granger_causation_matrix(data, data.columns, max_lag)\n",
        "\n",
        "    # Debug: Show the causality matrix\n",
        "    print(\"\\nGranger Causality Matrix (p-values < 0.05 are significant):\")\n",
        "    significant_mask = gc_matrix < significance_level\n",
        "    print(f\"Total significant relationships: {significant_mask.sum().sum()}\")\n",
        "    print(f\"Percentage significant: {(significant_mask.sum().sum() / (len(data.columns)**2)) * 100:.1f}%\")\n",
        "\n",
        "    # Count significant relationships for each feature\n",
        "    feature_scores = {}\n",
        "    for col in data.columns:\n",
        "        # Count how many features this one significantly causes\n",
        "        causes_count = (gc_matrix[col + '_x'] < significance_level).sum()\n",
        "        # Count how many features significantly cause this one\n",
        "        caused_by_count = (gc_matrix.loc[col + '_y'] < significance_level).sum()\n",
        "        total_score = causes_count + caused_by_count\n",
        "        feature_scores[col] = total_score\n",
        "        print(f\"{col}: causes {causes_count}, caused by {caused_by_count}, total score: {total_score}\")\n",
        "\n",
        "    # Different removal strategies\n",
        "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if removal_strategy == 'conservative':\n",
        "        # Only remove features with NO significant relationships\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] > 0]\n",
        "    elif removal_strategy == 'moderate':\n",
        "        # Remove features with very few relationships (less than 25% of possible)\n",
        "        threshold = len(data.columns) * 0.5  # At least 50% relationships\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] >= threshold]\n",
        "    elif removal_strategy == 'aggressive':\n",
        "        # Keep only top 75% of features\n",
        "        keep_count = max(min_features, int(len(data.columns) * 0.75))\n",
        "        keep_features = [f[0] for f in sorted_features[:keep_count]]\n",
        "\n",
        "    # Ensure minimum number of features\n",
        "    if len(keep_features) < min_features:\n",
        "        print(f\"Warning: Only {len(keep_features)} features selected, keeping top {min_features}\")\n",
        "        keep_features = [f[0] for f in sorted_features[:min_features]]\n",
        "\n",
        "    removed_features = set(data.columns) - set(keep_features)\n",
        "    print(f\"\\nFeature Selection Results:\")\n",
        "    print(f\"Selected {len(keep_features)} features out of {len(data.columns)}\")\n",
        "    if removed_features:\n",
        "        print(f\"Removed features: {removed_features}\")\n",
        "    else:\n",
        "        print(\"No features removed - all have significant causal relationships!\")\n",
        "\n",
        "    return data[keep_features]\n",
        "\n",
        "# ================================================================================\n",
        "# DUAL LABELING SYSTEM\n",
        "# ================================================================================\n",
        "\n",
        "def create_detection_labels(df):\n",
        "    \"\"\"\n",
        "    Detection labels - mark periods when failures are currently happening\n",
        "    For Agent 3: Current anomaly detection task\n",
        "    \"\"\"\n",
        "    print(\"Creating DETECTION labels (current failures)...\")\n",
        "\n",
        "    labels = np.zeros(len(df))\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    for start_time, end_time, failure_type in failure_periods:\n",
        "        failure_mask = (df.index >= start_time) & (df.index <= end_time)\n",
        "        failure_indices = np.where(failure_mask)[0]\n",
        "        if len(failure_indices) > 0:\n",
        "            labels[failure_indices] = 1\n",
        "            print(f\"  {failure_type}: {len(failure_indices)} points\")\n",
        "\n",
        "    failure_count = np.sum(labels)\n",
        "    print(f\"Detection labels: {failure_count}/{len(labels)} ({failure_count/len(labels)*100:.2f}%)\")\n",
        "    return labels.astype(int)\n",
        "\n",
        "def create_prediction_labels(df, horizons=[1, 3, 5, 12]):\n",
        "    \"\"\"\n",
        "    Prediction labels - mark periods that should trigger early warnings\n",
        "    For Agent 3: Early warning prediction task\n",
        "    \"\"\"\n",
        "    print(\"Creating PREDICTION labels (early warnings)...\")\n",
        "\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    prediction_labels = {}\n",
        "\n",
        "    for H in horizons:\n",
        "        print(f\"  Creating H{H} (warn {H}h before failure)...\")\n",
        "        labels = np.zeros(len(df))\n",
        "\n",
        "        for start_time, end_time, failure_type in failure_periods:\n",
        "            failure_start = pd.to_datetime(start_time)\n",
        "            warning_start = failure_start - pd.Timedelta(hours=H)\n",
        "\n",
        "            warning_mask = (df.index >= warning_start) & (df.index < start_time)\n",
        "            warning_indices = np.where(warning_mask)[0]\n",
        "\n",
        "            if len(warning_indices) > 0:\n",
        "                labels[warning_indices] = 1\n",
        "\n",
        "        warning_count = np.sum(labels)\n",
        "        prediction_labels[f'H{H}'] = labels.astype(int)\n",
        "        print(f\"    H{H}: {warning_count}/{len(labels)} ({warning_count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    return prediction_labels\n",
        "\n",
        "# ================================================================================\n",
        "# VAR WINDOW SELECTION\n",
        "# ================================================================================\n",
        "\n",
        "# SMART VAR MODEL SELECTION with optional early stopping\n",
        "def smart_var_selection(series, max_lag=None, early_stopping=True, patience=5):\n",
        "    \"\"\"\n",
        "    Smart VAR model selection with configurable early stopping\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    series : np.array\n",
        "        Time series data\n",
        "    max_lag : int\n",
        "        Maximum lag to try (default: sequence length - 1)\n",
        "    early_stopping : bool\n",
        "        Whether to use early stopping (default: True)\n",
        "    patience : int\n",
        "        How many iterations without improvement before stopping (default: 5)\n",
        "    \"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = len(series)//(series.shape[1]*10)\n",
        "\n",
        "    AIC_values = []\n",
        "    best_aic = float('inf')\n",
        "    best_lag = 1\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    print(f\"    Starting VAR selection: will try up to {max_lag} lags\")\n",
        "    if early_stopping:\n",
        "        print(f\"    Early stopping enabled: patience = {patience}\")\n",
        "    else:\n",
        "        print(f\"    Comprehensive search: will try ALL {max_lag} lags\")\n",
        "\n",
        "    try:\n",
        "        # Try variance threshold to avoid numerical issues\n",
        "        selector = VarianceThreshold(0.00002)  # Same as your original\n",
        "        series_filtered = selector.fit_transform(series)\n",
        "\n",
        "        if series_filtered.shape[1] < 2:  # Need at least 2 features\n",
        "            # Try with original data if filtering removes too much\n",
        "            series_filtered = series\n",
        "            print(f\"    Using {series_filtered.shape[1]} features after variance filtering\")\n",
        "\n",
        "        model = VAR(series_filtered)\n",
        "\n",
        "        # TRY LAGS with optional early stopping\n",
        "        for lag in range(max_lag):\n",
        "            try:\n",
        "                results = model.fit(lag)\n",
        "                current_aic = results.aic\n",
        "\n",
        "                # Progress indicator with details\n",
        "                progress_percent = ((lag + 1) / max_lag) * 100\n",
        "                print(f'    Lag {lag:2d}/{max_lag-1} ({progress_percent:5.1f}%) | AIC: {current_aic:8.4f} | BIC: {results.bic:8.4f}', end='')\n",
        "\n",
        "                AIC_values.append(current_aic)\n",
        "\n",
        "                # Track best AIC and early stopping logic\n",
        "                if current_aic < best_aic:\n",
        "                    best_aic = current_aic\n",
        "                    best_lag = lag + 1  # +1 because we return 1-indexed\n",
        "                    no_improvement_count = 0\n",
        "                    print(f' ‚Üê NEW BEST!')\n",
        "                else:\n",
        "                    no_improvement_count += 1\n",
        "                    if early_stopping:\n",
        "                        print(f' (no improvement: {no_improvement_count}/{patience})')\n",
        "                    else:\n",
        "                        print()\n",
        "\n",
        "                # Early stopping check\n",
        "                if early_stopping and no_improvement_count >= patience:\n",
        "                    print(f'    *** EARLY STOP: No improvement for {patience} iterations ***')\n",
        "                    print(f'    *** BEST RESULT: Lag {best_lag} with AIC {best_aic:.4f} ***')\n",
        "                    print(f'    *** SAVED TIME: Skipped {max_lag - lag - 1} remaining lags ***')\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                AIC_values.append(99999)  # Same as your original\n",
        "                no_improvement_count += 1\n",
        "                print(f'    Lag {lag:2d}/{max_lag-1} | FAILED: {str(e)[:50]}...')\n",
        "\n",
        "        # Find minimum AIC index from what we tried\n",
        "        if AIC_values:\n",
        "            minAIC_index = AIC_values.index(min(AIC_values)) + 1\n",
        "            trials_completed = len(AIC_values)\n",
        "            print(f'    ========================================')\n",
        "            print(f'    FINAL RESULT: Optimal lag = {minAIC_index}')\n",
        "            print(f'    FINAL AIC: {min(AIC_values):.4f}')\n",
        "            print(f'    TRIALS: {trials_completed}/{max_lag} ({(trials_completed/max_lag)*100:.1f}%)')\n",
        "            print(f'    ========================================')\n",
        "            return minAIC_index\n",
        "        else:\n",
        "            print(f'    *** ERROR: No valid AIC values found ***')\n",
        "            return 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'    *** CRITICAL ERROR: {e} ***')\n",
        "        return 99999  # Same as your original\n",
        "\n",
        "def extract_windows(array, window_size, labels=None, task_type='detection'):\n",
        "    \"\"\"Extract windowed sequences for agentic tasks\"\"\"\n",
        "    start = 0\n",
        "    last_index = len(array) - 1\n",
        "    max_time = last_index - window_size + 1\n",
        "\n",
        "    sub_windows = (\n",
        "        start +\n",
        "        np.expand_dims(np.arange(window_size), 0) +\n",
        "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
        "    ).astype(int)\n",
        "\n",
        "    windows = array[sub_windows]\n",
        "\n",
        "    if labels is not None:\n",
        "        # Use label at end of window for prediction tasks\n",
        "        window_labels = labels[sub_windows[:, -1]]\n",
        "        return windows, window_labels\n",
        "\n",
        "    return windows\n",
        "\n",
        "def auto_filter_windows(window_data, sequences, labels=None, percentile_range=(10, 90)):\n",
        "    \"\"\"Filter windows based on VAR selection quality - REMOVED FOR AGENTIC SYSTEM\"\"\"\n",
        "    pass  # Function removed as requested\n",
        "\n",
        "def map_labels_to_windows(windows, labels):\n",
        "    \"\"\"Map labels to existing windows based on end timestep\"\"\"\n",
        "    # Assuming windows shape is (n_windows, window_size, n_features)\n",
        "    n_windows = windows.shape[0]\n",
        "    window_size = windows.shape[1]\n",
        "\n",
        "    # Get the end index for each window\n",
        "    end_indices = np.arange(window_size - 1, window_size - 1 + n_windows)\n",
        "\n",
        "    # Return labels at those end positions\n",
        "    return labels[end_indices]\n",
        "\n",
        "# ================================================================================\n",
        "# MAIN AGENTIC PIPELINE\n",
        "# ================================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"AGENTIC METROPT-3 PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Feature selection\n",
        "print(\"\\n1. FEATURE SELECTION\")\n",
        "whole_series_auto = auto_feature_selection(data=whole_series, removal_strategy='moderate')\n",
        "n_features = whole_series_auto.shape[1]\n",
        "\n",
        "# Step 2: Create dual labels for agentic tasks\n",
        "print(f\"\\n2. DUAL LABELING FOR AGENTIC TASKS\")\n",
        "detection_labels = create_detection_labels(whole_series_auto)\n",
        "prediction_labels = create_prediction_labels(whole_series_auto, [1, 3, 5, 12])\n",
        "\n",
        "\n",
        "# Step 3: Extract windows and apply VAR selection\n",
        "print(f\"\\n4. WINDOWING AND VAR SELECTION\")\n",
        "K = 50  # Window size for Agent 2\n",
        "print(f\"Creating {K}-length sequences from {len(whole_series_auto)} timesteps with {n_features} features...\")\n",
        "\n",
        "# Create windows ONCE\n",
        "Long = extract_windows(whole_series_auto.values, K)  # No labels yet\n",
        "Long_train = Long\n",
        "print(f\"Created {Long_train.shape[0]} long sequences\")\n",
        "\n",
        "# VAR - optimal lag as label\n",
        "print(\"Starting automated VAR model selection...\")\n",
        "best_window_for_long_seq = []\n",
        "for i in range(Long_train.shape[0]):\n",
        "    if i % 100 == 0:  # Progress indicator\n",
        "        print(f\"Processed {i}/{Long_train.shape[0]} sequences...\")\n",
        "\n",
        "    cur_seq = Long_train[i,:,:]\n",
        "    optimal_lag = smart_var_selection(cur_seq)\n",
        "    best_window_for_long_seq.append(optimal_lag)\n",
        "\n",
        "print(f\"VAR selection completed. Window distribution:\")\n",
        "Window = np.array(best_window_for_long_seq)\n",
        "print(f\"Mean: {Window.mean():.2f}, Std: {Window.std():.2f}, Min: {Window.min()}, Max: {Window.max()}\")\n",
        "\n",
        "# Step 4: Create anomaly labels\n",
        "\n",
        "# Detection label\n",
        "detection_window_labels = map_labels_to_windows(Long_train, detection_labels)\n",
        "\n",
        "# predictionlabel\n",
        "h1_window_labels = map_labels_to_windows(Long_train, prediction_labels['H1'])\n",
        "h3_window_labels = map_labels_to_windows(Long_train, prediction_labels['H3'])\n",
        "h5_window_labels = map_labels_to_windows(Long_train, prediction_labels['H5'])\n",
        "h12_window_labels = map_labels_to_windows(Long_train, prediction_labels['H12'])\n",
        "\n",
        "print(f\"Created {len(Long_train):,} Long subsequence windows\")\n",
        "print(f\"Positive windows (detection): {np.sum(detection_window_labels):,} ({np.mean(detection_window_labels)*100:.2f}%)\")\n",
        "print(f\"Positive windows (H1): {np.sum(h1_window_labels):,} ({np.mean(h1_window_labels)*100:.2f}%)\")\n",
        "print(f\"Positive windows (H3): {np.sum(h3_window_labels):,} ({np.mean(h3_window_labels)*100:.2f}%)\")\n",
        "print(f\"Positive windows (H5): {np.sum(h5_window_labels):,} ({np.mean(h5_window_labels)*100:.2f}%)\")\n",
        "print(f\"Positive windows (H12): {np.sum(h12_window_labels):,} ({np.mean(h12_window_labels)*100:.2f}%)\")\n",
        "\n",
        "\n",
        "# Step 5: Save results for agentic modules\n",
        "print(f\"\\n5. SAVING RESULTS FOR AGENTIC MODULES\")\n",
        "output_dir = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'\n",
        "\n",
        "\n",
        "np.save(f'{output_dir}multivariate_long_sequences-TRAIN-AUTO.npy', Long_train)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_WINDOW-AUTO.npy', Window)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-DETECTION-AUTO.npy', detection_window_labels)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-H1-AUTO.npy', h1_window_labels)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-H3-AUTO.npy', h3_window_labels)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-H5-AUTO.npy', h5_window_labels)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-H12-AUTO.npy', h12_window_labels)\n",
        "\n",
        "\n",
        "# Save other components\n",
        "with open(f'{output_dir}scalers.pkl', 'wb') as f:\n",
        "    pickle.dump(scalers, f)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'n_features': n_features,\n",
        "    'window_size': K,\n",
        "    'n_sequences': len(Long_train),\n",
        "    'feature_names': list(whole_series_auto.columns),\n",
        "    'date_range': (whole_series_auto.index.min(), whole_series_auto.index.max())\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"‚úÖ Saved agentic data:\")\n",
        "print(f\"   Sequences: {Long.shape}\")\n",
        "print(f\"   Labels: {Window.shape}\")\n",
        "print(f\"   Detection labels: {detection_labels.shape}\")\n",
        "print(f\"   Prediction labels: {len(prediction_labels)} horizons\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# CREATE AGENTIC DATA CONTAINER\n",
        "# ================================================================================\n",
        "\n",
        "class AgenticMetroPTData:\n",
        "    \"\"\"Data container for agentic MetroPT system\"\"\"\n",
        "    def __init__(self):\n",
        "        self.sequences = windows\n",
        "        self.labels = window_labels\n",
        "        self.detection_labels = detection_labels\n",
        "        self.prediction_labels = prediction_labels\n",
        "        self.lofo_splits = lofo_splits\n",
        "        self.scalers = scalers\n",
        "        self.metadata = metadata\n",
        "        self.raw_data = whole_series_auto\n",
        "\n",
        "    def get_sensor_data(self, sensor_idx):\n",
        "        \"\"\"Get individual sensor data for Agent 1\"\"\"\n",
        "        sensor_name = self.metadata['feature_names'][sensor_idx]\n",
        "        sensor_data = self.raw_data.iloc[:, sensor_idx]\n",
        "        return sensor_data, sensor_name\n",
        "\n",
        "    def get_task_data(self, task_type='prediction', horizon='H5'):\n",
        "        \"\"\"Get data for specific agentic task\"\"\"\n",
        "        if task_type == 'detection':\n",
        "            return self.sequences, self.detection_labels\n",
        "        else:\n",
        "            task_labels = self.prediction_labels[horizon]\n",
        "            return self.sequences, task_labels\n",
        "\n",
        "    def get_fold_data(self, fold_idx):\n",
        "        \"\"\"Get train/test data for LOFO fold\"\"\"\n",
        "        split = self.lofo_splits[fold_idx]\n",
        "        return split\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Print summary for agents\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"AGENTIC METROPT DATA SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìä Sequences: {self.sequences.shape}\")\n",
        "        print(f\"üè∑Ô∏è  Labels: Detection + {len(self.prediction_labels)} prediction horizons\")\n",
        "        print(f\"üìã CV Folds: {len(self.lofo_splits)} LOFO splits\")\n",
        "        print(f\"üîß Features: {self.metadata['n_features']} sensors\")\n",
        "        print(f\"ü™ü Window size: {self.metadata['window_size']}\")\n",
        "        print(f\"üìÖ Date range: {self.metadata['date_range']}\")\n",
        "        print(\"\\nü§ñ READY FOR AGENTIC IMPLEMENTATION!\")\n",
        "\n",
        "# Create agentic data container\n",
        "agentic_data = AgenticMetroPTData()\n",
        "agentic_data.summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODULE 1: DUAL LABELING COMPLETE! ‚úÖ\")\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ Ready for Module 2: Individual Time Series Agent\")\n",
        "print(\"üöÄ Ready for Module 3: Dynamic Window Agent\")\n",
        "print(\"üöÄ Ready for Module 4: Fusion Agent\")\n",
        "print(\"üöÄ Ready for Module 5: Orchestrator\")\n",
        "\n",
        "print(f\"\\nüí° USAGE:\")\n",
        "print(f\"   agentic_data = AgenticMetroPTData()\")\n",
        "print(f\"   sequences, labels = agentic_data.get_task_data('prediction', 'H5')\")\n",
        "print(f\"   sensor_data, name = agentic_data.get_sensor_data(0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-RMQkgLak9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xp-Mv3Ksamxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zEUESMBM_Eo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6ZgIvrdRNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE1.ipynb",
      "authorship_tag": "ABX9TyMLmY/5tlB/LMihEK2Vl1TF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}