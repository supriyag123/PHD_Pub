{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T-BJGPTX7XS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7856f2f-6b7f-4e1c-84fc-3ec9cb4e5616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. WINDOWING AND VAR SELECTION\n",
            "Creating 50-length sequences from 3676 timesteps with 13 features...\n"
          ]
        }
      ],
      "source": [
        "!pip install statsmodels --upgrade\n",
        "!pip install -U lingam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tools.eval_measures import rmse, aic\n",
        "import ast\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from numpy import arange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from importlib.metadata import version\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "# ================================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ================================================================================\n",
        "\n",
        "# Load MetroPT-3 dataset\n",
        "df=pd.read_csv(r'/content/drive/MyDrive/PHD/metropt+3+dataset (1).zip (Unzipped Files)/MetroPT3(AirCompressor).csv', parse_dates={'datetime':[1]}, index_col=['datetime'])\n",
        "\n",
        "# Handle missing values\n",
        "df = df.replace('?', np.nan)\n",
        "df.isnull().sum()\n",
        "\n",
        "def fill_missing(values):\n",
        "    one_day = 24*6\n",
        "    for row in range(df.shape[0]):\n",
        "        for col in range(df.shape[1]):\n",
        "            if np.isnan(values[row][col]):\n",
        "                values[row,col] = values[row-one_day,col]\n",
        "\n",
        "df = df.astype('float32')\n",
        "fill_missing(df.values)\n",
        "\n",
        "# Resample to hourly data to reduce computation\n",
        "daily_df = df.resample('1H').mean().backfill()\n",
        "\n",
        "# Convert index to column and filter to April 2020\n",
        "daily_df['datetime']=daily_df.index\n",
        "daily_df = daily_df.loc[(daily_df['datetime'] >= '2020-04-01')]\n",
        "\n",
        "# Remove index and datetime columns\n",
        "daily_df.drop(daily_df.columns[0], axis=1, inplace=True) # remove ID column if exists\n",
        "daily_df.drop(daily_df.columns[-1], axis=1, inplace=True) # remove datetime column\n",
        "\n",
        "# Scaling the values\n",
        "whole_series = daily_df\n",
        "\n",
        "scalers={}\n",
        "for i in daily_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(whole_series[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    whole_series[i]=s_s\n",
        "\n",
        "# ================================================================================\n",
        "# GRANGER CAUSALITY FUNCTIONS\n",
        "# ================================================================================\n",
        "\n",
        "def granger_causation_matrix(data, variables, max_lag=25, test='ssr_chi2test', verbose=False):\n",
        "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            try:\n",
        "                test_result = grangercausalitytests(data[[r,c]], maxlag=max_lag, verbose=False)\n",
        "                p_values = [round(test_result[i+1][0][test][1],4) for i in range(max_lag)]\n",
        "                min_p_value = np.min(p_values)\n",
        "                df.loc[r, c] = min_p_value\n",
        "            except:\n",
        "                df.loc[r, c] = 1.0  # No causality if test fails\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "def auto_feature_selection(data, max_lag='adaptive', significance_level=0.05, min_features=3, removal_strategy='moderate'):\n",
        "    \"\"\"Automatically remove features with weak causal relationships\"\"\"\n",
        "    print(\"Starting automated feature selection...\")\n",
        "    print(f\"Input data shape: {data.shape}\")\n",
        "\n",
        "    # Adaptive max_lag based on data characteristics\n",
        "    if max_lag == 'adaptive':\n",
        "        data_length = len(data)\n",
        "        if data_length > 2000:\n",
        "            max_lag = min(24, data_length // 100)\n",
        "        elif data_length > 1000:\n",
        "            max_lag = min(12, data_length // 80)\n",
        "        else:\n",
        "            max_lag = min(6, data_length // 50)\n",
        "\n",
        "    # Granger causality analysis\n",
        "    gc_matrix = granger_causation_matrix(data, data.columns, max_lag)\n",
        "    significant_mask = gc_matrix < significance_level\n",
        "\n",
        "    # Count significant relationships for each feature\n",
        "    feature_scores = {}\n",
        "    for col in data.columns:\n",
        "        causes_count = (gc_matrix[col + '_x'] < significance_level).sum()\n",
        "        caused_by_count = (gc_matrix.loc[col + '_y'] < significance_level).sum()\n",
        "        total_score = causes_count + caused_by_count\n",
        "        feature_scores[col] = total_score\n",
        "\n",
        "    # Feature selection strategy\n",
        "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if removal_strategy == 'conservative':\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] > 0]\n",
        "    elif removal_strategy == 'moderate':\n",
        "        threshold = len(data.columns) * 0.5\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] >= threshold]\n",
        "    elif removal_strategy == 'aggressive':\n",
        "        keep_count = max(min_features, int(len(data.columns) * 0.75))\n",
        "        keep_features = [f[0] for f in sorted_features[:keep_count]]\n",
        "\n",
        "    if len(keep_features) < min_features:\n",
        "        keep_features = [f[0] for f in sorted_features[:min_features]]\n",
        "\n",
        "    print(f\"Selected {len(keep_features)} features out of {len(data.columns)}\")\n",
        "    return data[keep_features]\n",
        "\n",
        "# ================================================================================\n",
        "# DUAL LABELING SYSTEM\n",
        "# ================================================================================\n",
        "\n",
        "def create_detection_labels(df):\n",
        "    \"\"\"\n",
        "    Detection labels - mark periods when failures are currently happening\n",
        "    For Agent 3: Current anomaly detection task\n",
        "    \"\"\"\n",
        "    print(\"Creating DETECTION labels (current failures)...\")\n",
        "\n",
        "    labels = np.zeros(len(df))\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    for start_time, end_time, failure_type in failure_periods:\n",
        "        failure_mask = (df.index >= start_time) & (df.index <= end_time)\n",
        "        failure_indices = np.where(failure_mask)[0]\n",
        "        if len(failure_indices) > 0:\n",
        "            labels[failure_indices] = 1\n",
        "            print(f\"  {failure_type}: {len(failure_indices)} points\")\n",
        "\n",
        "    failure_count = np.sum(labels)\n",
        "    print(f\"Detection labels: {failure_count}/{len(labels)} ({failure_count/len(labels)*100:.2f}%)\")\n",
        "    return labels.astype(int)\n",
        "\n",
        "def create_prediction_labels(df, horizons=[1, 3, 5, 12]):\n",
        "    \"\"\"\n",
        "    Prediction labels - mark periods that should trigger early warnings\n",
        "    For Agent 3: Early warning prediction task\n",
        "    \"\"\"\n",
        "    print(\"Creating PREDICTION labels (early warnings)...\")\n",
        "\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    prediction_labels = {}\n",
        "\n",
        "    for H in horizons:\n",
        "        print(f\"  Creating H{H} (warn {H}h before failure)...\")\n",
        "        labels = np.zeros(len(df))\n",
        "\n",
        "        for start_time, end_time, failure_type in failure_periods:\n",
        "            failure_start = pd.to_datetime(start_time)\n",
        "            warning_start = failure_start - pd.Timedelta(hours=H)\n",
        "\n",
        "            warning_mask = (df.index >= warning_start) & (df.index < start_time)\n",
        "            warning_indices = np.where(warning_mask)[0]\n",
        "\n",
        "            if len(warning_indices) > 0:\n",
        "                labels[warning_indices] = 1\n",
        "\n",
        "        warning_count = np.sum(labels)\n",
        "        prediction_labels[f'H{H}'] = labels.astype(int)\n",
        "        print(f\"    H{H}: {warning_count}/{len(labels)} ({warning_count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    return prediction_labels\n",
        "\n",
        "# ================================================================================\n",
        "# VAR WINDOW SELECTION\n",
        "# ================================================================================\n",
        "\n",
        "def smart_var_selection(series, max_lag=None, early_stopping=True, patience=5):\n",
        "    \"\"\"Smart VAR model selection for Agent 2 (dynamic windowing)\"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = len(series)//(series.shape[1]*10)\n",
        "\n",
        "    AIC_values = []\n",
        "    best_aic = float('inf')\n",
        "    best_lag = 1\n",
        "\n",
        "    try:\n",
        "        selector = VarianceThreshold(0.00002)\n",
        "        series_filtered = selector.fit_transform(series)\n",
        "        if series_filtered.shape[1] < 2:\n",
        "            series_filtered = series\n",
        "\n",
        "        model = VAR(series_filtered)\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        for lag in range(max_lag):\n",
        "            try:\n",
        "                results = model.fit(lag)\n",
        "                current_aic = results.aic\n",
        "                AIC_values.append(current_aic)\n",
        "\n",
        "                if current_aic < best_aic:\n",
        "                    best_aic = current_aic\n",
        "                    best_lag = lag + 1\n",
        "                    no_improvement_count = 0\n",
        "                else:\n",
        "                    no_improvement_count += 1\n",
        "\n",
        "                if early_stopping and no_improvement_count >= patience:\n",
        "                    break\n",
        "\n",
        "            except Exception:\n",
        "                AIC_values.append(99999)\n",
        "                no_improvement_count += 1\n",
        "\n",
        "        return AIC_values.index(min(AIC_values)) + 1 if AIC_values else 1\n",
        "\n",
        "    except Exception:\n",
        "        return 1\n",
        "\n",
        "def extract_windows(array, window_size, labels=None, task_type='detection'):\n",
        "    \"\"\"Extract windowed sequences for agentic tasks\"\"\"\n",
        "    start = 0\n",
        "    last_index = len(array) - 1\n",
        "    max_time = last_index - window_size + 1\n",
        "\n",
        "    sub_windows = (\n",
        "        start +\n",
        "        np.expand_dims(np.arange(window_size), 0) +\n",
        "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
        "    ).astype(int)\n",
        "\n",
        "    windows = array[sub_windows]\n",
        "\n",
        "    if labels is not None:\n",
        "        # Use label at end of window for prediction tasks\n",
        "        window_labels = labels[sub_windows[:, -1]]\n",
        "        return windows, window_labels\n",
        "\n",
        "    return windows\n",
        "\n",
        "def auto_filter_windows(window_data, sequences, labels=None, percentile_range=(10, 90)):\n",
        "    \"\"\"Filter windows based on VAR selection quality - REMOVED FOR AGENTIC SYSTEM\"\"\"\n",
        "    pass  # Function removed as requested\n",
        "\n",
        "# ================================================================================\n",
        "# MAIN AGENTIC PIPELINE\n",
        "# ================================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"AGENTIC METROPT-3 PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Feature selection\n",
        "print(\"\\n1. FEATURE SELECTION\")\n",
        "whole_series_auto = auto_feature_selection(data=whole_series, removal_strategy='moderate')\n",
        "n_features = whole_series_auto.shape[1]\n",
        "\n",
        "# Step 2: Create dual labels for agentic tasks\n",
        "print(f\"\\n2. DUAL LABELING FOR AGENTIC TASKS\")\n",
        "detection_labels = create_detection_labels(whole_series_auto)\n",
        "prediction_labels = create_prediction_labels(whole_series_auto, [1, 3, 5, 12])\n",
        "\n",
        "\n",
        "# Step 3: Extract windows and apply VAR selection\n",
        "print(f\"\\n4. WINDOWING AND VAR SELECTION\")\n",
        "K = 50  # Window size for Agent 2\n",
        "print(f\"Creating {K}-length sequences from {len(whole_series_auto)} timesteps with {n_features} features...\")\n",
        "\n",
        "\n",
        "# For this pipeline, use prediction task (Agent 3 early warning)\n",
        "windows, window_labels = extract_windows(\n",
        "    whole_series_auto.values,\n",
        "    K,\n",
        "    prediction_labels['H5'],  # 5-hour early warning\n",
        "    'prediction'\n",
        ")\n",
        "\n",
        "# Create separate datasets for each task\n",
        "detection_windows, detection_window_labels = extract_windows(whole_series_auto.values,K, detection_labels)\n",
        "h1_windows, h1_window_labels = extract_windows(whole_series_auto.values,K,, prediction_labels['H1'])\n",
        "h3_windows, h3_window_labels = extract_windows(whole_series_auto.values,K,, prediction_labels['H3'])\n",
        "h5_windows, h5_window_labels = extract_windows(whole_series_auto.values,K,, prediction_labels['H5'])\n",
        "h12_windows, h12_window_labels = extract_windows(whole_series_auto.values,K,, prediction_labels['H12'])\n",
        "\n",
        "print(f\"Created {len(windows):,} windows\")\n",
        "print(f\"Positive windows: {np.sum(window_labels):,} ({np.mean(window_labels)*100:.2f}%)\")\n",
        "\n",
        "# Step 5: Save results for agentic modules\n",
        "print(f\"\\n5. SAVING RESULTS FOR AGENTIC MODULES\")\n",
        "output_dir = r'/content/drive/MyDrive/PHD/2025/AGENTIC_METROPT/'\n",
        "\n",
        "# Save processed data (no filtering applied)\n",
        "np.save(f'{output_dir}agentic_sequences.npy', windows)\n",
        "np.save(f'{output_dir}agentic_labels.npy', window_labels)\n",
        "\n",
        "# Save dual labels\n",
        "np.save(f'{output_dir}detection_labels.npy', detection_labels)\n",
        "for horizon_key, pred_labels in prediction_labels.items():\n",
        "    np.save(f'{output_dir}prediction_labels_{horizon_key}.npy', pred_labels)\n",
        "\n",
        "# Save other components\n",
        "with open(f'{output_dir}lofo_splits.pkl', 'wb') as f:\n",
        "    pickle.dump(lofo_splits, f)\n",
        "\n",
        "with open(f'{output_dir}scalers.pkl', 'wb') as f:\n",
        "    pickle.dump(scalers, f)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'n_features': n_features,\n",
        "    'window_size': K,\n",
        "    'n_sequences': len(windows),\n",
        "    'feature_names': list(whole_series_auto.columns),\n",
        "    'date_range': (whole_series_auto.index.min(), whole_series_auto.index.max())\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"✅ Saved agentic data:\")\n",
        "print(f\"   Sequences: {windows.shape}\")\n",
        "print(f\"   Labels: {window_labels.shape}\")\n",
        "print(f\"   Detection labels: {detection_labels.shape}\")\n",
        "print(f\"   Prediction labels: {len(prediction_labels)} horizons\")\n",
        "print(f\"   LOFO splits: {len(lofo_splits)} folds\")\n",
        "\n",
        "# ================================================================================\n",
        "# CREATE AGENTIC DATA CONTAINER\n",
        "# ================================================================================\n",
        "\n",
        "class AgenticMetroPTData:\n",
        "    \"\"\"Data container for agentic MetroPT system\"\"\"\n",
        "    def __init__(self):\n",
        "        self.sequences = windows\n",
        "        self.labels = window_labels\n",
        "        self.detection_labels = detection_labels\n",
        "        self.prediction_labels = prediction_labels\n",
        "        self.lofo_splits = lofo_splits\n",
        "        self.scalers = scalers\n",
        "        self.metadata = metadata\n",
        "        self.raw_data = whole_series_auto\n",
        "\n",
        "    def get_sensor_data(self, sensor_idx):\n",
        "        \"\"\"Get individual sensor data for Agent 1\"\"\"\n",
        "        sensor_name = self.metadata['feature_names'][sensor_idx]\n",
        "        sensor_data = self.raw_data.iloc[:, sensor_idx]\n",
        "        return sensor_data, sensor_name\n",
        "\n",
        "    def get_task_data(self, task_type='prediction', horizon='H5'):\n",
        "        \"\"\"Get data for specific agentic task\"\"\"\n",
        "        if task_type == 'detection':\n",
        "            return self.sequences, self.detection_labels\n",
        "        else:\n",
        "            task_labels = self.prediction_labels[horizon]\n",
        "            return self.sequences, task_labels\n",
        "\n",
        "    def get_fold_data(self, fold_idx):\n",
        "        \"\"\"Get train/test data for LOFO fold\"\"\"\n",
        "        split = self.lofo_splits[fold_idx]\n",
        "        return split\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Print summary for agents\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"AGENTIC METROPT DATA SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"📊 Sequences: {self.sequences.shape}\")\n",
        "        print(f\"🏷️  Labels: Detection + {len(self.prediction_labels)} prediction horizons\")\n",
        "        print(f\"📋 CV Folds: {len(self.lofo_splits)} LOFO splits\")\n",
        "        print(f\"🔧 Features: {self.metadata['n_features']} sensors\")\n",
        "        print(f\"🪟 Window size: {self.metadata['window_size']}\")\n",
        "        print(f\"📅 Date range: {self.metadata['date_range']}\")\n",
        "        print(\"\\n🤖 READY FOR AGENTIC IMPLEMENTATION!\")\n",
        "\n",
        "# Create agentic data container\n",
        "agentic_data = AgenticMetroPTData()\n",
        "agentic_data.summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODULE 1: DUAL LABELING COMPLETE! ✅\")\n",
        "print(\"=\"*60)\n",
        "print(\"🚀 Ready for Module 2: Individual Time Series Agent\")\n",
        "print(\"🚀 Ready for Module 3: Dynamic Window Agent\")\n",
        "print(\"🚀 Ready for Module 4: Fusion Agent\")\n",
        "print(\"🚀 Ready for Module 5: Orchestrator\")\n",
        "\n",
        "print(f\"\\n💡 USAGE:\")\n",
        "print(f\"   agentic_data = AgenticMetroPTData()\")\n",
        "print(f\"   sequences, labels = agentic_data.get_task_data('prediction', 'H5')\")\n",
        "print(f\"   sensor_data, name = agentic_data.get_sensor_data(0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-RMQkgLak9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xp-Mv3Ksamxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zEUESMBM_Eo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6ZgIvrdRNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE1.ipynb",
      "authorship_tag": "ABX9TyO8DIlqENgwI2RsCp5uM3b4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}