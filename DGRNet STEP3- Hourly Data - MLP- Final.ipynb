{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data%20-%20MLP-%20Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoP7OuWNxlsJ",
        "outputId": "e90f7076-c816-4cd4-a9d9-cfe387870fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6108/6108 [==============================] - 36s 5ms/step - loss: 0.8136 - mean_squared_error: 0.8136 - val_loss: 0.7965 - val_mean_squared_error: 0.7965\n",
            "Epoch 2/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7987 - mean_squared_error: 0.7987 - val_loss: 0.8046 - val_mean_squared_error: 0.8046\n",
            "Epoch 3/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7953 - mean_squared_error: 0.7953 - val_loss: 0.7984 - val_mean_squared_error: 0.7984\n",
            "Epoch 4/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7947 - mean_squared_error: 0.7947 - val_loss: 0.7939 - val_mean_squared_error: 0.7939\n",
            "Epoch 5/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7938 - mean_squared_error: 0.7938 - val_loss: 0.8044 - val_mean_squared_error: 0.8044\n",
            "Epoch 6/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7930 - mean_squared_error: 0.7930 - val_loss: 0.7982 - val_mean_squared_error: 0.7982\n",
            "Epoch 7/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7924 - mean_squared_error: 0.7924 - val_loss: 0.7917 - val_mean_squared_error: 0.7917\n",
            "Epoch 8/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7919 - mean_squared_error: 0.7919 - val_loss: 0.7947 - val_mean_squared_error: 0.7947\n",
            "Epoch 9/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7914 - mean_squared_error: 0.7914 - val_loss: 0.7916 - val_mean_squared_error: 0.7916\n",
            "Epoch 10/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7906 - mean_squared_error: 0.7906 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 11/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7911 - mean_squared_error: 0.7911 - val_loss: 0.7944 - val_mean_squared_error: 0.7944\n",
            "Epoch 12/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7908 - mean_squared_error: 0.7908 - val_loss: 0.7965 - val_mean_squared_error: 0.7965\n",
            "Epoch 13/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7903 - mean_squared_error: 0.7903 - val_loss: 0.7951 - val_mean_squared_error: 0.7951\n",
            "Epoch 14/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7901 - mean_squared_error: 0.7901 - val_loss: 0.7944 - val_mean_squared_error: 0.7944\n",
            "Epoch 15/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7899 - mean_squared_error: 0.7899 - val_loss: 0.7911 - val_mean_squared_error: 0.7911\n",
            "Epoch 16/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7894 - mean_squared_error: 0.7894 - val_loss: 0.7904 - val_mean_squared_error: 0.7904\n",
            "Epoch 17/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7893 - mean_squared_error: 0.7893 - val_loss: 0.7938 - val_mean_squared_error: 0.7938\n",
            "Epoch 18/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7891 - mean_squared_error: 0.7891 - val_loss: 0.7925 - val_mean_squared_error: 0.7925\n",
            "Epoch 19/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7892 - mean_squared_error: 0.7892 - val_loss: 0.7953 - val_mean_squared_error: 0.7953\n",
            "Epoch 20/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7892 - mean_squared_error: 0.7892 - val_loss: 0.7914 - val_mean_squared_error: 0.7914\n",
            "Epoch 21/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7889 - mean_squared_error: 0.7889 - val_loss: 0.7907 - val_mean_squared_error: 0.7907\n",
            "Epoch 22/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7889 - mean_squared_error: 0.7889 - val_loss: 0.7911 - val_mean_squared_error: 0.7911\n",
            "Epoch 23/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7884 - mean_squared_error: 0.7884 - val_loss: 0.7955 - val_mean_squared_error: 0.7955\n",
            "Epoch 24/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7884 - mean_squared_error: 0.7884 - val_loss: 0.7959 - val_mean_squared_error: 0.7959\n",
            "Epoch 25/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7882 - mean_squared_error: 0.7882 - val_loss: 0.7909 - val_mean_squared_error: 0.7909\n",
            "Epoch 26/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7883 - mean_squared_error: 0.7883 - val_loss: 0.7980 - val_mean_squared_error: 0.7980\n",
            "Epoch 27/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7880 - mean_squared_error: 0.7880 - val_loss: 0.7905 - val_mean_squared_error: 0.7905\n",
            "Epoch 28/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7882 - mean_squared_error: 0.7882 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 29/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7877 - mean_squared_error: 0.7877 - val_loss: 0.7927 - val_mean_squared_error: 0.7927\n",
            "Epoch 30/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7879 - mean_squared_error: 0.7879 - val_loss: 0.8000 - val_mean_squared_error: 0.8000\n",
            "Epoch 31/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7876 - mean_squared_error: 0.7876 - val_loss: 0.7925 - val_mean_squared_error: 0.7925\n",
            "Epoch 32/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7875 - mean_squared_error: 0.7875 - val_loss: 0.7915 - val_mean_squared_error: 0.7915\n",
            "Epoch 33/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7875 - mean_squared_error: 0.7875 - val_loss: 0.7903 - val_mean_squared_error: 0.7903\n",
            "Epoch 34/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7871 - mean_squared_error: 0.7871 - val_loss: 0.7909 - val_mean_squared_error: 0.7909\n",
            "Epoch 35/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7874 - mean_squared_error: 0.7874 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 36/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 37/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7874 - mean_squared_error: 0.7874 - val_loss: 0.7963 - val_mean_squared_error: 0.7963\n",
            "Epoch 38/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7869 - mean_squared_error: 0.7869 - val_loss: 0.7893 - val_mean_squared_error: 0.7893\n",
            "Epoch 39/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7871 - mean_squared_error: 0.7871 - val_loss: 0.7906 - val_mean_squared_error: 0.7906\n",
            "Epoch 40/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7874 - mean_squared_error: 0.7874 - val_loss: 0.7982 - val_mean_squared_error: 0.7982\n",
            "Epoch 41/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7873 - mean_squared_error: 0.7873 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 42/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7913 - val_mean_squared_error: 0.7913\n",
            "Epoch 43/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7871 - mean_squared_error: 0.7871 - val_loss: 0.7895 - val_mean_squared_error: 0.7895\n",
            "Epoch 44/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7868 - mean_squared_error: 0.7868 - val_loss: 0.7911 - val_mean_squared_error: 0.7911\n",
            "Epoch 45/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7871 - mean_squared_error: 0.7871 - val_loss: 0.7883 - val_mean_squared_error: 0.7883\n",
            "Epoch 46/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7894 - val_mean_squared_error: 0.7894\n",
            "Epoch 47/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7899 - val_mean_squared_error: 0.7899\n",
            "Epoch 48/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 49/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 0.7900 - val_mean_squared_error: 0.7900\n",
            "Epoch 50/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7885 - val_mean_squared_error: 0.7885\n",
            "Epoch 51/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7866 - mean_squared_error: 0.7866 - val_loss: 0.7922 - val_mean_squared_error: 0.7922\n",
            "Epoch 52/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7865 - mean_squared_error: 0.7865 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 53/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7866 - mean_squared_error: 0.7866 - val_loss: 0.7953 - val_mean_squared_error: 0.7953\n",
            "Epoch 54/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7866 - mean_squared_error: 0.7866 - val_loss: 0.7892 - val_mean_squared_error: 0.7892\n",
            "Epoch 55/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7865 - mean_squared_error: 0.7865 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 56/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 57/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7865 - mean_squared_error: 0.7865 - val_loss: 0.7901 - val_mean_squared_error: 0.7901\n",
            "Epoch 58/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7864 - mean_squared_error: 0.7864 - val_loss: 0.7885 - val_mean_squared_error: 0.7885\n",
            "Epoch 59/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 0.7900 - val_mean_squared_error: 0.7900\n",
            "Epoch 60/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 0.7899 - val_mean_squared_error: 0.7899\n",
            "Epoch 61/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7863 - mean_squared_error: 0.7863 - val_loss: 0.7886 - val_mean_squared_error: 0.7886\n",
            "Epoch 62/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7860 - mean_squared_error: 0.7860 - val_loss: 0.7891 - val_mean_squared_error: 0.7891\n",
            "Epoch 63/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7858 - mean_squared_error: 0.7858 - val_loss: 0.7945 - val_mean_squared_error: 0.7945\n",
            "Epoch 64/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7860 - mean_squared_error: 0.7860 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 65/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7859 - mean_squared_error: 0.7859 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 66/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 0.7952 - val_mean_squared_error: 0.7952\n",
            "Epoch 67/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7860 - mean_squared_error: 0.7860 - val_loss: 0.7924 - val_mean_squared_error: 0.7924\n",
            "Epoch 68/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7858 - mean_squared_error: 0.7858 - val_loss: 0.7911 - val_mean_squared_error: 0.7911\n",
            "Epoch 69/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7859 - mean_squared_error: 0.7859 - val_loss: 0.7905 - val_mean_squared_error: 0.7905\n",
            "Epoch 70/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7859 - mean_squared_error: 0.7859 - val_loss: 0.7910 - val_mean_squared_error: 0.7910\n",
            "Epoch 71/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7858 - mean_squared_error: 0.7858 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 72/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 73/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7858 - mean_squared_error: 0.7858 - val_loss: 0.7903 - val_mean_squared_error: 0.7903\n",
            "Epoch 74/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 0.7882 - val_mean_squared_error: 0.7882\n",
            "Epoch 75/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7938 - val_mean_squared_error: 0.7938\n",
            "Epoch 76/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7858 - mean_squared_error: 0.7858 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 77/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 78/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 0.7886 - val_mean_squared_error: 0.7886\n",
            "Epoch 79/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7856 - mean_squared_error: 0.7856 - val_loss: 0.7902 - val_mean_squared_error: 0.7902\n",
            "Epoch 80/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 0.7913 - val_mean_squared_error: 0.7913\n",
            "Epoch 81/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 0.7924 - val_mean_squared_error: 0.7924\n",
            "Epoch 82/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7879 - val_mean_squared_error: 0.7879\n",
            "Epoch 83/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 0.7924 - val_mean_squared_error: 0.7924\n",
            "Epoch 84/500\n",
            "6108/6108 [==============================] - 36s 6ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7892 - val_mean_squared_error: 0.7892\n",
            "Epoch 85/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7852 - mean_squared_error: 0.7852 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 86/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7851 - mean_squared_error: 0.7851 - val_loss: 0.7923 - val_mean_squared_error: 0.7923\n",
            "Epoch 87/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7892 - val_mean_squared_error: 0.7892\n",
            "Epoch 88/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7852 - mean_squared_error: 0.7852 - val_loss: 0.7883 - val_mean_squared_error: 0.7883\n",
            "Epoch 89/500\n",
            "6108/6108 [==============================] - 41s 7ms/step - loss: 0.7854 - mean_squared_error: 0.7854 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 90/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7959 - val_mean_squared_error: 0.7959\n",
            "Epoch 91/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 0.7905 - val_mean_squared_error: 0.7905\n",
            "Epoch 92/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7851 - mean_squared_error: 0.7851 - val_loss: 0.7882 - val_mean_squared_error: 0.7882\n",
            "Epoch 93/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 0.7879 - val_mean_squared_error: 0.7879\n",
            "Epoch 94/500\n",
            "6108/6108 [==============================] - 36s 6ms/step - loss: 0.7854 - mean_squared_error: 0.7854 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 95/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 0.7893 - val_mean_squared_error: 0.7893\n",
            "Epoch 96/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7855 - mean_squared_error: 0.7855 - val_loss: 0.7888 - val_mean_squared_error: 0.7888\n",
            "Epoch 97/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7851 - mean_squared_error: 0.7851 - val_loss: 0.7915 - val_mean_squared_error: 0.7915\n",
            "Epoch 98/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7851 - mean_squared_error: 0.7851 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
            "Epoch 99/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
            "Epoch 100/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 101/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7885 - val_mean_squared_error: 0.7885\n",
            "Epoch 102/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7953 - val_mean_squared_error: 0.7953\n",
            "Epoch 103/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7852 - mean_squared_error: 0.7852 - val_loss: 0.7895 - val_mean_squared_error: 0.7895\n",
            "Epoch 104/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7875 - val_mean_squared_error: 0.7875\n",
            "Epoch 105/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7918 - val_mean_squared_error: 0.7918\n",
            "Epoch 106/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7851 - mean_squared_error: 0.7851 - val_loss: 0.7940 - val_mean_squared_error: 0.7940\n",
            "Epoch 107/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7923 - val_mean_squared_error: 0.7923\n",
            "Epoch 108/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 109/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 110/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7898 - val_mean_squared_error: 0.7898\n",
            "Epoch 111/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7984 - val_mean_squared_error: 0.7984\n",
            "Epoch 112/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7882 - val_mean_squared_error: 0.7882\n",
            "Epoch 113/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7850 - mean_squared_error: 0.7850 - val_loss: 0.7899 - val_mean_squared_error: 0.7899\n",
            "Epoch 114/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 115/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7882 - val_mean_squared_error: 0.7882\n",
            "Epoch 116/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 117/500\n",
            "6108/6108 [==============================] - 32s 5ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7894 - val_mean_squared_error: 0.7894\n",
            "Epoch 118/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7892 - val_mean_squared_error: 0.7892\n",
            "Epoch 119/500\n",
            "6108/6108 [==============================] - 34s 5ms/step - loss: 0.7847 - mean_squared_error: 0.7847 - val_loss: 0.7935 - val_mean_squared_error: 0.7935\n",
            "Epoch 120/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7847 - mean_squared_error: 0.7847 - val_loss: 0.7877 - val_mean_squared_error: 0.7877\n",
            "Epoch 121/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7914 - val_mean_squared_error: 0.7914\n",
            "Epoch 122/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7918 - val_mean_squared_error: 0.7918\n",
            "Epoch 123/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7849 - mean_squared_error: 0.7849 - val_loss: 0.7906 - val_mean_squared_error: 0.7906\n",
            "Epoch 124/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7899 - val_mean_squared_error: 0.7899\n",
            "Epoch 125/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 126/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7881 - val_mean_squared_error: 0.7881\n",
            "Epoch 127/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7873 - val_mean_squared_error: 0.7873\n",
            "Epoch 128/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7896 - val_mean_squared_error: 0.7896\n",
            "Epoch 129/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7897 - val_mean_squared_error: 0.7897\n",
            "Epoch 130/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7848 - mean_squared_error: 0.7848 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 131/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7886 - val_mean_squared_error: 0.7886\n",
            "Epoch 132/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7890 - val_mean_squared_error: 0.7890\n",
            "Epoch 133/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7888 - val_mean_squared_error: 0.7888\n",
            "Epoch 134/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7879 - val_mean_squared_error: 0.7879\n",
            "Epoch 135/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7889 - val_mean_squared_error: 0.7889\n",
            "Epoch 136/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7872 - val_mean_squared_error: 0.7872\n",
            "Epoch 137/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7877 - val_mean_squared_error: 0.7877\n",
            "Epoch 138/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7847 - mean_squared_error: 0.7847 - val_loss: 0.7883 - val_mean_squared_error: 0.7883\n",
            "Epoch 139/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7847 - mean_squared_error: 0.7847 - val_loss: 0.7871 - val_mean_squared_error: 0.7871\n",
            "Epoch 140/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7942 - val_mean_squared_error: 0.7942\n",
            "Epoch 141/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7898 - val_mean_squared_error: 0.7898\n",
            "Epoch 142/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7874 - val_mean_squared_error: 0.7874\n",
            "Epoch 143/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7908 - val_mean_squared_error: 0.7908\n",
            "Epoch 144/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7846 - mean_squared_error: 0.7846 - val_loss: 0.7918 - val_mean_squared_error: 0.7918\n",
            "Epoch 145/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7876 - val_mean_squared_error: 0.7876\n",
            "Epoch 146/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 147/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
            "Epoch 148/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7845 - mean_squared_error: 0.7845 - val_loss: 0.7918 - val_mean_squared_error: 0.7918\n",
            "Epoch 149/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7876 - val_mean_squared_error: 0.7876\n",
            "Epoch 150/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7875 - val_mean_squared_error: 0.7875\n",
            "Epoch 151/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7893 - val_mean_squared_error: 0.7893\n",
            "Epoch 152/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 153/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7900 - val_mean_squared_error: 0.7900\n",
            "Epoch 154/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7910 - val_mean_squared_error: 0.7910\n",
            "Epoch 155/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7889 - val_mean_squared_error: 0.7889\n",
            "Epoch 156/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7891 - val_mean_squared_error: 0.7891\n",
            "Epoch 157/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7867 - val_mean_squared_error: 0.7867\n",
            "Epoch 158/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7891 - val_mean_squared_error: 0.7891\n",
            "Epoch 159/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 0.7873 - val_mean_squared_error: 0.7873\n",
            "Epoch 160/500\n",
            "6108/6108 [==============================] - 36s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
            "Epoch 161/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7844 - mean_squared_error: 0.7844 - val_loss: 0.7911 - val_mean_squared_error: 0.7911\n",
            "Epoch 162/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 163/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 164/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7901 - val_mean_squared_error: 0.7901\n",
            "Epoch 165/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7874 - val_mean_squared_error: 0.7874\n",
            "Epoch 166/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7898 - val_mean_squared_error: 0.7898\n",
            "Epoch 167/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7885 - val_mean_squared_error: 0.7885\n",
            "Epoch 168/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7912 - val_mean_squared_error: 0.7912\n",
            "Epoch 169/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7893 - val_mean_squared_error: 0.7893\n",
            "Epoch 170/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7934 - val_mean_squared_error: 0.7934\n",
            "Epoch 171/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7906 - val_mean_squared_error: 0.7906\n",
            "Epoch 172/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7868 - val_mean_squared_error: 0.7868\n",
            "Epoch 173/500\n",
            "6108/6108 [==============================] - 36s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7877 - val_mean_squared_error: 0.7877\n",
            "Epoch 174/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7894 - val_mean_squared_error: 0.7894\n",
            "Epoch 175/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7906 - val_mean_squared_error: 0.7906\n",
            "Epoch 176/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7867 - val_mean_squared_error: 0.7867\n",
            "Epoch 177/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7905 - val_mean_squared_error: 0.7905\n",
            "Epoch 178/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7873 - val_mean_squared_error: 0.7873\n",
            "Epoch 179/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7924 - val_mean_squared_error: 0.7924\n",
            "Epoch 180/500\n",
            "6108/6108 [==============================] - 34s 5ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7908 - val_mean_squared_error: 0.7908\n",
            "Epoch 181/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7873 - val_mean_squared_error: 0.7873\n",
            "Epoch 182/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7871 - val_mean_squared_error: 0.7871\n",
            "Epoch 183/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7881 - val_mean_squared_error: 0.7881\n",
            "Epoch 184/500\n",
            "6108/6108 [==============================] - 34s 5ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7875 - val_mean_squared_error: 0.7875\n",
            "Epoch 185/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7872 - val_mean_squared_error: 0.7872\n",
            "Epoch 186/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7900 - val_mean_squared_error: 0.7900\n",
            "Epoch 187/500\n",
            "6108/6108 [==============================] - 33s 5ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7872 - val_mean_squared_error: 0.7872\n",
            "Epoch 188/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7840 - mean_squared_error: 0.7840 - val_loss: 0.7937 - val_mean_squared_error: 0.7937\n",
            "Epoch 189/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7836 - mean_squared_error: 0.7836 - val_loss: 0.7893 - val_mean_squared_error: 0.7893\n",
            "Epoch 190/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 191/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7835 - mean_squared_error: 0.7835 - val_loss: 0.7881 - val_mean_squared_error: 0.7881\n",
            "Epoch 192/500\n",
            "6108/6108 [==============================] - 39s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7909 - val_mean_squared_error: 0.7909\n",
            "Epoch 193/500\n",
            "6108/6108 [==============================] - 36s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7906 - val_mean_squared_error: 0.7906\n",
            "Epoch 194/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 0.7892 - val_mean_squared_error: 0.7892\n",
            "Epoch 195/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 196/500\n",
            "6108/6108 [==============================] - 41s 7ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7868 - val_mean_squared_error: 0.7868\n",
            "Epoch 197/500\n",
            "6108/6108 [==============================] - 40s 7ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7872 - val_mean_squared_error: 0.7872\n",
            "Epoch 198/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7874 - val_mean_squared_error: 0.7874\n",
            "Epoch 199/500\n",
            "6108/6108 [==============================] - 40s 6ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
            "Epoch 200/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7842 - mean_squared_error: 0.7842 - val_loss: 0.7865 - val_mean_squared_error: 0.7865\n",
            "Epoch 201/500\n",
            "6108/6108 [==============================] - 35s 6ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 0.7877 - val_mean_squared_error: 0.7877\n",
            "Epoch 202/500\n",
            "6108/6108 [==============================] - 34s 6ms/step - loss: 0.7836 - mean_squared_error: 0.7836 - val_loss: 0.7872 - val_mean_squared_error: 0.7872\n",
            "Epoch 203/500\n",
            "6108/6108 [==============================] - 37s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7888 - val_mean_squared_error: 0.7888\n",
            "Epoch 204/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7886 - val_mean_squared_error: 0.7886\n",
            "Epoch 205/500\n",
            "6108/6108 [==============================] - 40s 6ms/step - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 0.7925 - val_mean_squared_error: 0.7925\n",
            "Epoch 206/500\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7880 - val_mean_squared_error: 0.7880\n",
            "Epoch 207/500\n",
            "6108/6108 [==============================] - 40s 6ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 208/500\n",
            "6108/6108 [==============================] - 45s 7ms/step - loss: 0.7841 - mean_squared_error: 0.7841 - val_loss: 0.7864 - val_mean_squared_error: 0.7864\n",
            "Epoch 209/500\n",
            "6108/6108 [==============================] - 41s 7ms/step - loss: 0.7837 - mean_squared_error: 0.7837 - val_loss: 0.7878 - val_mean_squared_error: 0.7878\n",
            "Epoch 210/500\n",
            "6108/6108 [==============================] - 44s 7ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7879 - val_mean_squared_error: 0.7879\n",
            "Epoch 211/500\n",
            "6100/6108 [============================>.] - ETA: 0s - loss: 0.7838 - mean_squared_error: 0.7838Restoring model weights from the end of the best epoch: 191.\n",
            "6108/6108 [==============================] - 38s 6ms/step - loss: 0.7838 - mean_squared_error: 0.7838 - val_loss: 0.7884 - val_mean_squared_error: 0.7884\n",
            "Epoch 211: early stopping\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pylab as pl\n",
        "#from keras import ops\n",
        "\n",
        "generator_multiply = 10 #each input record will generate 100 random vectors from the latent space, given the mu and sigma generated by the encoder\n",
        "\n",
        "\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy') #------for Hourly data\n",
        "index = 500\n",
        "#We missed i=500 from processing the iosw. So here we are dropping row with index =500\n",
        "train_data= np.delete(train_data, index, axis=0)\n",
        "\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-TRAIN_hourly.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "#----------------check window distribution - we see upto 20 has very high fequency. so we remove that and take the rest to generate more samples, to create overall uniform distribution...\n",
        "#First we test window 20 to 30 and see if this method can fit the model well\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(window_label, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "p = np.argwhere((20<window_label) & (30>window_label)).flatten()\n",
        "train_f = np.take(train_data, p, 0)\n",
        "window_f= np.take(window_label, p, 0)\n",
        "\n",
        "\n",
        "\n",
        "##---------------------------IGNORE THIS IF NOT GENERATING FRESH VAE DATASET--------------------------------------------------------------------------------------------\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-hourly-latent10-dim128-latest.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-hourly-latent10-dim128-latest.model')\n",
        "\n",
        "X_train_encoded = encoder.predict(train_data)\n",
        "mu, logvar, z = X_train_encoded\n",
        "sigma = tf.exp(0.5 * logvar)\n",
        "batch = tf.shape(mu)[0]  #number of recors / batchs\n",
        "dim = tf.shape(mu)[1] #Ndimension of latent variable\n",
        "store = list()\n",
        "storetemp = list()\n",
        "\n",
        "#For each batch, iterate, get the generator_multipy number of latent vectors with same window_size.\n",
        "#For each z, concatenate z_mean, so it will become 100 dimensional vector\n",
        "\n",
        "for i in range(0,batch):\n",
        "  all_Z_i = tf.random.normal(shape=(generator_multiply,dim), mean = mu[i,:], stddev=sigma[i,:]) #all randorm vectors for this record i\n",
        "  X_train_decoded = decoder.predict(all_Z_i)\n",
        "  X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0],window_size*n_features))\n",
        "  a = np.arange(generator_multiply)\n",
        "  a.fill(window_label[i])\n",
        "  c=np.concatenate(((X_train_decoded,a[:,None])),axis=1)\n",
        "  store.append(c)\n",
        "\n",
        "results1=np.concatenate(store,axis=0)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly_window20-30_times10',results1)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "results1=np.load(r'/content/drive/MyDrive/PHD/2024/labelled_subsquence_data_hourly_window20-30_times10.npy')\n",
        "\n",
        "x=results1[:,:-1]\n",
        "y=results1[:,window_size*n_features]\n",
        "\n",
        "#maxval = x.shape[0]\n",
        "#count_train = int(math.ceil(0.9*maxval))\n",
        "#x_train = x[:count_train]\n",
        "#x_test = x[count_train:]\n",
        "\n",
        "#y_train = y[:count_train]\n",
        "#y_test = y[count_train:]\n",
        "\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "iso = IsolationForest(contamination=0.4)\n",
        "yhat = iso.fit_predict(x)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "x, y = x[mask, :], y[mask]\n",
        "\n",
        "\n",
        "\n",
        "###############Scale the target and then split the data into train test----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#Looking at the dist, we remove al y less than 20\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "#p = np.argwhere((20<y) & (y<50)).flatten()\n",
        "p = np.argwhere(20<y).flatten()\n",
        "xf = np.take(x, p, 0)\n",
        "yf= np.take(y, p, 0)\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Distribution before Transformation\", fontsize=15)\n",
        "sns.histplot(yf, kde=True, color=\"red\")\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "from scipy.stats import boxcox\n",
        "transformer = PowerTransformer(method='box-cox', standardize=True)\n",
        "y_transformed = transformer.fit_transform(y.reshape(-1,1)).flatten()\n",
        "x=xf\n",
        "\n",
        "\n",
        "\n",
        "plt.title(\"Distribution after Transformation\", fontsize=15)\n",
        "sns.histplot(y_transformed,bins=10, kde=True , legend=False)\n",
        "plt.xlabel(\"window\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_transformed, test_size = 0.1, random_state = 42)\n",
        "\n",
        "#--------------------------------------------------------------------------------------CONSTRUCT, COMPILE, AND TRAIN THE MODEL------------------------------------------------------------------------------------------------------------------\n",
        "#------------MLP------------------------------------------------------\n",
        "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(LSTM(1024, input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(512,return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "#model.add(Dense(units = 128))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(units = 32))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "#--------------------------------------------------------------LSTM--------------------------\n",
        "\n",
        "a =  x_train.reshape((x_train.shape[0], window_size, n_features))  #DONT RUN IF MLP\n",
        "b =  x_test.reshape((x_test.shape[0], window_size, n_features))    #DONT RUN IF MLP\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(a.shape[1],a.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(Dense(units = 1024))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 512))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "#model.add(Dense(units = 256))\n",
        "#model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(units = 16))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 16))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 8))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "#model.add(Dense(units = 4))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(Dense(units = 1, activation = 'linear'))\n",
        "model.summary()\n",
        "x_train = a\n",
        "x_test = b\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "optimizr = keras.optimizers.Adam(learning_rate=0.001,clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer= optimizr, metrics=['mean_squared_error'])\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(patience=20, verbose=1, min_delta=0.0001, monitor='loss', mode='min', restore_best_weights=True)\n",
        "n_epochs = 500\n",
        "\n",
        "history=model.fit( x_train,y_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=32,\n",
        "                   validation_split=0.3,\n",
        "                 callbacks=[es])\n",
        "\n",
        "#-----------------------------------------------------TRAIN EVALUATION----------------------------------------------------------------\n",
        "y_train_pred_raw = model.predict(x_train)\n",
        "y_train_pred = transformer.inverse_transform(y_train_pred_raw)\n",
        "y_train_true = transformer.inverse_transform(y_train.reshape(-1,1)).flatten()\n",
        "\n",
        "score_train= r2_score(y_train_true,y_train_pred)\n",
        "print(\"r2 score is ==\",score_train)\n",
        "\n",
        "plt.plot(y_train_true[0:100], color = 'red', label = 'Real data')\n",
        "plt.plot(y_train_pred[0:100], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------------------------TEST EVALUATION----------------------------------------------------------------\n",
        "\n",
        "y_pred_raw = model.predict(x_test)\n",
        "y_test_pred = transformer.inverse_transform(y_pred_raw)\n",
        "y_test_true = transformer.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "score= r2_score(y_test_true,y_test_pred)\n",
        "print(\"r2 score is ==\",score)\n",
        "\n",
        "\n",
        "plt.plot(y_test_true[100:150], color = 'red', label = 'Real data')\n",
        "plt.plot(y_test_pred[100:150], color = 'blue', label = 'Predicted data')\n",
        "plt.title('Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------------SAVE MODEL AND RESULTS-----------------------------------------------------------------\n",
        "\n",
        "model.save(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_hourly.keras')\n",
        "# It can be used to reconstruct the model identically.\n",
        "reconstructed_model = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/DGRNet-MLP-Versions/MLP_model_hourly.keras')\n",
        "\n",
        "# Let's check:\n",
        "np.testing.assert_allclose(\n",
        "    model.predict(test_input), reconstructed_model.predict(test_input)\n",
        ")\n",
        "\n",
        "\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/preduber_2.csv',y_pred)\n",
        "np.savetxt(r'/content/drive/MyDrive/PHD/2024/MLPOutput/realuber_2.csv',y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet%20STEP3-%20Hourly%20Data%20-%20MLP-%20Final.ipynb",
      "authorship_tag": "ABX9TyOl2mCY8bZ//dbU9mFKZf+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}