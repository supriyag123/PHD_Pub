{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "8f3b8086-060d-4cf2-a514-bec6188e222a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Adaptive MLP Analysis\n",
            "Analyzing files: generated-data-OPTIMIZED.npy, generated-data-true-window-OPTIMIZED.npy\n",
            "📁 Output directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/\n",
            "================================================================================\n",
            "🎯 ADAPTIVE MLP ANALYSIS\n",
            "================================================================================\n",
            "📊 Loading VAE output files...\n",
            "   Data: generated-data-OPTIMIZED.npy\n",
            "   Windows: generated-data-true-window-OPTIMIZED.npy\n",
            "✅ Loaded: X=(350000, 650), y=(350000,)\n",
            "   Data range: [-1.4552, 1.5141]\n",
            "   Windows range: [2, 24]\n",
            "   Unique windows: 23\n",
            "\n",
            "🔗 Analyzing feature-target correlations...\n",
            "   Max |correlation|: 0.072594\n",
            "   Mean |correlation|: 0.025965\n",
            "   Median |correlation|: 0.021110\n",
            "   Std |correlation|: 0.017960\n",
            "   Feature breakdown:\n",
            "     Very weak (<0.01): 117/650 (18.0%)\n",
            "     Weak (0.01-0.05): 429/650 (66.0%)\n",
            "     Moderate (0.05-0.1): 104/650 (16.0%)\n",
            "     Strong (>0.1): 0/650 (0.0%)\n",
            "   🏗️ Architecture recommendation: WIDE\n",
            "      (Moderate correlations need width for complexity)\n",
            "\n",
            "📊 Splitting data...\n",
            "   Train: 210000 samples, 650 features\n",
            "   Validation: 70000 samples\n",
            "   Test: 70000 samples\n",
            "\n",
            "🏗️ Building WIDE MLP architecture...\n",
            "\n",
            "🔍 Grid search for WIDE architecture...\n",
            "   Testing 3 LR × 2 batch × 2 L1 × 2 L2 = 24 combinations\n",
            "\n",
            "🏗️ Building WIDE MLP architecture...\n",
            "   Model built: 508,929 parameters\n",
            "   ✓ New best: LR=0.0005, Batch=128, L1=0.0001, L2=0.0010, Val_Loss=0.426945\n",
            "   Model built: 508,929 parameters\n",
            "   Model built: 508,929 parameters\n",
            "   Model built: 508,929 parameters\n",
            "   Model built: 508,929 parameters\n",
            "   ✓ New best: LR=0.0005, Batch=256, L1=0.0001, L2=0.0010, Val_Loss=0.425087\n",
            "   Model built: 508,929 parameters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1961122612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Analyzing files: {data_file}, {windows_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_adaptive_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1961122612.py\u001b[0m in \u001b[0;36mrun_adaptive_mlp\u001b[0;34m(data_filename, windows_filename)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \"\"\"\n\u001b[1;32m    677\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaptiveMLPSolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_adaptive_mlp_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1961122612.py\u001b[0m in \u001b[0;36mrun_adaptive_mlp_analysis\u001b[0;34m(self, data_filename, windows_filename)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# 5. Grid search for optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             best_params = self.grid_search_hyperparameters(\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0marchitecture_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1961122612.py\u001b[0m in \u001b[0;36mgrid_search_hyperparameters\u001b[0;34m(self, x_train, y_train, x_val, y_val, architecture_type, base_lr, base_batch_size)\u001b[0m\n\u001b[1;32m    283\u001b[0m                             ]\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                             history = model.fit(\n\u001b[0m\u001b[1;32m    286\u001b[0m                                 \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 for step, data in zip(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    184\u001b[0m                         [self._variant_tensor]) as scope:\n\u001b[1;32m    185\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         result = gen_optional_ops.optional_get_value(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_get_value\u001b[0;34m(optional, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OptionalGetValue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         output_types, \"output_shapes\", output_shapes)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Complete Cached MLP with Incremental Training\n",
        "# Grid search once → Train once → Reuse everything → Incremental training option\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import hashlib\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.regularizers import l1_l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class CompleteCachedMLP:\n",
        "    \"\"\"\n",
        "    Complete MLP solution with full caching and incremental training\n",
        "    - Grid search: Done once, cached forever\n",
        "    - Model training: Done once, cached forever\n",
        "    - Results: Saved for reuse\n",
        "    - Incremental training: Available on demand\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.y_scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "        self.correlation_analysis = None\n",
        "\n",
        "        # Organized directory structure\n",
        "        self.cache_dir = f\"{output_dir}mlp_cache/\"\n",
        "        self.models_dir = f\"{output_dir}mlp_models/\"\n",
        "        self.results_dir = f\"{output_dir}mlp_results/\"\n",
        "        self.checkpoints_dir = f\"{output_dir}mlp_checkpoints/\"\n",
        "        self.incremental_dir = f\"{output_dir}mlp_incremental/\"\n",
        "\n",
        "        for directory in [self.cache_dir, self.models_dir, self.results_dir,\n",
        "                         self.checkpoints_dir, self.incremental_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        print(f\"📁 Directories initialized:\")\n",
        "        print(f\"   Cache: {self.cache_dir}\")\n",
        "        print(f\"   Models: {self.models_dir}\")\n",
        "        print(f\"   Results: {self.results_dir}\")\n",
        "\n",
        "    def get_data_hash(self, data_filename, windows_filename):\n",
        "        \"\"\"Generate unique hash for data files\"\"\"\n",
        "        data_path = os.path.join(self.output_dir, data_filename)\n",
        "        windows_path = os.path.join(self.output_dir, windows_filename)\n",
        "\n",
        "        hash_input = f\"{data_filename}_{windows_filename}\"\n",
        "\n",
        "        if os.path.exists(data_path) and os.path.exists(windows_path):\n",
        "            data_stat = os.stat(data_path)\n",
        "            windows_stat = os.stat(windows_path)\n",
        "            hash_input += f\"_{data_stat.st_size}_{data_stat.st_mtime}_{windows_stat.st_size}_{windows_stat.st_mtime}\"\n",
        "\n",
        "        return hashlib.md5(hash_input.encode()).hexdigest()[:12]\n",
        "\n",
        "    def load_vae_files(self, data_filename, windows_filename):\n",
        "        \"\"\"Load VAE output files\"\"\"\n",
        "        print(\"📊 Loading VAE output files...\")\n",
        "\n",
        "        data_path = os.path.join(self.output_dir, data_filename)\n",
        "        windows_path = os.path.join(self.output_dir, windows_filename)\n",
        "\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
        "        if not os.path.exists(windows_path):\n",
        "            raise FileNotFoundError(f\"Windows file not found: {windows_path}\")\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        y = np.load(windows_path)\n",
        "\n",
        "        print(f\"✅ Loaded: X={x.shape}, y={y.shape}\")\n",
        "        print(f\"   Data range: [{np.min(x):.4f}, {np.max(x):.4f}]\")\n",
        "        print(f\"   Windows range: [{np.min(y):.0f}, {np.max(y):.0f}]\")\n",
        "\n",
        "        if x.shape[0] != y.shape[0]:\n",
        "            raise ValueError(f\"Sample mismatch: data={x.shape[0]}, windows={y.shape[0]}\")\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def analyze_correlations(self, x, y):\n",
        "        \"\"\"Analyze correlations and recommend architecture\"\"\"\n",
        "        print(\"\\n🔗 Analyzing correlations...\")\n",
        "\n",
        "        correlations = []\n",
        "        for i in range(x.shape[1]):\n",
        "            if np.std(x[:, i]) > 1e-8:\n",
        "                corr = np.corrcoef(x[:, i], y)[0, 1]\n",
        "                if not np.isnan(corr) and not np.isinf(corr):\n",
        "                    correlations.append(abs(corr))\n",
        "\n",
        "        max_corr = max(correlations) if correlations else 0\n",
        "        mean_corr = np.mean(correlations) if correlations else 0\n",
        "\n",
        "        # Architecture recommendation\n",
        "        if max_corr < 0.01:\n",
        "            arch_recommendation = \"VERY_DEEP_WIDE\"\n",
        "        elif max_corr < 0.02:\n",
        "            arch_recommendation = \"DEEP_WIDE\"\n",
        "        elif max_corr < 0.05:\n",
        "            arch_recommendation = \"DEEP\"\n",
        "        elif max_corr < 0.1:\n",
        "            arch_recommendation = \"WIDE\"\n",
        "        else:\n",
        "            arch_recommendation = \"STANDARD\"\n",
        "\n",
        "        analysis = {\n",
        "            'max_correlation': max_corr,\n",
        "            'mean_correlation': mean_corr,\n",
        "            'total_features': len(correlations),\n",
        "            'architecture_recommendation': arch_recommendation\n",
        "        }\n",
        "\n",
        "        print(f\"   Max correlation: {max_corr:.6f}\")\n",
        "        print(f\"   Mean correlation: {mean_corr:.6f}\")\n",
        "        print(f\"   Architecture: {arch_recommendation}\")\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def define_architecture(self, input_dim, architecture_type):\n",
        "        \"\"\"Define architecture based on type\"\"\"\n",
        "        configs = {\n",
        "            \"VERY_DEEP_WIDE\": {\n",
        "                'layers': [\n",
        "                    {'units': 2048, 'dropout': 0.4, 'batch_norm': True},\n",
        "                    {'units': 1024, 'dropout': 0.4, 'batch_norm': True},\n",
        "                    {'units': 512, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 256, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 128, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 64, 'dropout': 0.2, 'batch_norm': False},\n",
        "                    {'units': 32, 'dropout': 0.1, 'batch_norm': False}\n",
        "                ],\n",
        "                'default_lr': 0.001,\n",
        "                'default_batch_size': 32\n",
        "            },\n",
        "            \"DEEP_WIDE\": {\n",
        "                'layers': [\n",
        "                    {'units': 1024, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 512, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 256, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 128, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 64, 'dropout': 0.2, 'batch_norm': False},\n",
        "                    {'units': 32, 'dropout': 0.1, 'batch_norm': False}\n",
        "                ],\n",
        "                'default_lr': 0.002,\n",
        "                'default_batch_size': 64\n",
        "            },\n",
        "            \"DEEP\": {\n",
        "                'layers': [\n",
        "                    {'units': 512, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 256, 'dropout': 0.3, 'batch_norm': True},\n",
        "                    {'units': 128, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 64, 'dropout': 0.2, 'batch_norm': False},\n",
        "                    {'units': 32, 'dropout': 0.1, 'batch_norm': False}\n",
        "                ],\n",
        "                'default_lr': 0.002,\n",
        "                'default_batch_size': 64\n",
        "            },\n",
        "            \"WIDE\": {\n",
        "                'layers': [\n",
        "                    {'units': 512, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 256, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 128, 'dropout': 0.1, 'batch_norm': False},\n",
        "                    {'units': 64, 'dropout': 0.1, 'batch_norm': False}\n",
        "                ],\n",
        "                'default_lr': 0.001,\n",
        "                'default_batch_size': 128\n",
        "            },\n",
        "            \"STANDARD\": {\n",
        "                'layers': [\n",
        "                    {'units': 256, 'dropout': 0.2, 'batch_norm': True},\n",
        "                    {'units': 128, 'dropout': 0.2, 'batch_norm': False},\n",
        "                    {'units': 64, 'dropout': 0.1, 'batch_norm': False},\n",
        "                    {'units': 32, 'dropout': 0.1, 'batch_norm': False}\n",
        "                ],\n",
        "                'default_lr': 0.001,\n",
        "                'default_batch_size': 128\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config = configs[architecture_type]\n",
        "        return config['layers'], config['default_lr'], config['default_batch_size']\n",
        "\n",
        "    def build_mlp_model(self, input_dim, layers, learning_rate=0.001, l1_reg=0.001, l2_reg=0.01):\n",
        "        \"\"\"Build MLP model\"\"\"\n",
        "        model = Sequential()\n",
        "\n",
        "        # First layer\n",
        "        first_layer = layers[0]\n",
        "        model.add(Dense(\n",
        "            first_layer['units'],\n",
        "            input_dim=input_dim,\n",
        "            activation='relu',\n",
        "            kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "        ))\n",
        "\n",
        "        if first_layer['batch_norm']:\n",
        "            model.add(BatchNormalization())\n",
        "        if first_layer['dropout'] > 0:\n",
        "            model.add(Dropout(first_layer['dropout']))\n",
        "\n",
        "        # Hidden layers\n",
        "        for layer in layers[1:]:\n",
        "            model.add(Dense(\n",
        "                layer['units'],\n",
        "                activation='relu',\n",
        "                kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "            ))\n",
        "\n",
        "            if layer['batch_norm']:\n",
        "                model.add(BatchNormalization())\n",
        "            if layer['dropout'] > 0:\n",
        "                model.add(Dropout(layer['dropout']))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "\n",
        "        # Compile\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "        model.compile(loss='huber', optimizer=optimizer, metrics=['mae', 'mse'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    # ========== CACHING SYSTEM ==========\n",
        "\n",
        "    def save_grid_search_cache(self, best_params, architecture_type, correlation_analysis, data_hash):\n",
        "        \"\"\"Save grid search results\"\"\"\n",
        "        cache_file = f\"{self.cache_dir}grid_search_{data_hash}.json\"\n",
        "\n",
        "        cache_data = {\n",
        "            'data_hash': data_hash,\n",
        "            'architecture_type': architecture_type,\n",
        "            'best_params': best_params,\n",
        "            'correlation_analysis': correlation_analysis,\n",
        "            'timestamp': pd.Timestamp.now().isoformat(),\n",
        "            'completed': True\n",
        "        }\n",
        "\n",
        "        with open(cache_file, 'w') as f:\n",
        "            json.dump(cache_data, f, indent=2)\n",
        "\n",
        "        print(f\"💾 Grid search cached: {cache_file}\")\n",
        "\n",
        "    def load_grid_search_cache(self, data_hash):\n",
        "        \"\"\"Load grid search results\"\"\"\n",
        "        cache_file = f\"{self.cache_dir}grid_search_{data_hash}.json\"\n",
        "\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    cache_data = json.load(f)\n",
        "\n",
        "                print(f\"✅ Grid search cache found:\")\n",
        "                print(f\"   Architecture: {cache_data['architecture_type']}\")\n",
        "                print(f\"   Max correlation: {cache_data['correlation_analysis']['max_correlation']:.6f}\")\n",
        "                print(f\"   Cached: {cache_data['timestamp']}\")\n",
        "\n",
        "                return (cache_data['best_params'],\n",
        "                       cache_data['architecture_type'],\n",
        "                       cache_data['correlation_analysis'])\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Grid search cache corrupted: {e}\")\n",
        "\n",
        "        return None, None, None\n",
        "\n",
        "    def save_trained_model_cache(self, model, y_scaler, metadata, data_hash):\n",
        "        \"\"\"Save trained model\"\"\"\n",
        "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        model_file = f\"{self.models_dir}model_{data_hash}_{timestamp}.keras\"\n",
        "        scaler_file = f\"{self.models_dir}scaler_{data_hash}_{timestamp}.pkl\"\n",
        "        metadata_file = f\"{self.models_dir}metadata_{data_hash}_{timestamp}.json\"\n",
        "\n",
        "        # Save model\n",
        "        model.save(model_file)\n",
        "\n",
        "        # Save scaler\n",
        "        with open(scaler_file, 'wb') as f:\n",
        "            pickle.dump(y_scaler, f)\n",
        "\n",
        "        # Save metadata\n",
        "        full_metadata = {\n",
        "            'data_hash': data_hash,\n",
        "            'model_file': model_file,\n",
        "            'scaler_file': scaler_file,\n",
        "            'timestamp': timestamp,\n",
        "            **metadata\n",
        "        }\n",
        "\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            json.dump(full_metadata, f, indent=2)\n",
        "\n",
        "        # Update latest pointer\n",
        "        latest_file = f\"{self.models_dir}latest_{data_hash}.json\"\n",
        "        latest_data = {\n",
        "            'model_file': model_file,\n",
        "            'scaler_file': scaler_file,\n",
        "            'metadata_file': metadata_file,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "\n",
        "        with open(latest_file, 'w') as f:\n",
        "            json.dump(latest_data, f, indent=2)\n",
        "\n",
        "        print(f\"💾 Model saved: {model_file}\")\n",
        "        return model_file, scaler_file, metadata_file\n",
        "\n",
        "    def load_trained_model_cache(self, data_hash):\n",
        "        \"\"\"Load trained model\"\"\"\n",
        "        latest_file = f\"{self.models_dir}latest_{data_hash}.json\"\n",
        "\n",
        "        if os.path.exists(latest_file):\n",
        "            try:\n",
        "                with open(latest_file, 'r') as f:\n",
        "                    latest_data = json.load(f)\n",
        "\n",
        "                model_file = latest_data['model_file']\n",
        "                scaler_file = latest_data['scaler_file']\n",
        "                metadata_file = latest_data['metadata_file']\n",
        "\n",
        "                if all(os.path.exists(f) for f in [model_file, scaler_file, metadata_file]):\n",
        "                    # Load model\n",
        "                    model = keras.models.load_model(model_file)\n",
        "\n",
        "                    # Load scaler\n",
        "                    with open(scaler_file, 'rb') as f:\n",
        "                        y_scaler = pickle.load(f)\n",
        "\n",
        "                    # Load metadata\n",
        "                    with open(metadata_file, 'r') as f:\n",
        "                        metadata = json.load(f)\n",
        "\n",
        "                    print(f\"✅ Trained model found:\")\n",
        "                    print(f\"   Trained: {latest_data['timestamp']}\")\n",
        "                    print(f\"   R²: {metadata.get('final_r2', 'Unknown')}\")\n",
        "\n",
        "                    return model, y_scaler, metadata\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Model cache corrupted: {e}\")\n",
        "\n",
        "        return None, None, None\n",
        "\n",
        "    def save_test_results(self, x_test, y_true, y_pred, evaluation_results, data_hash):\n",
        "        \"\"\"Save test data and results\"\"\"\n",
        "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save test data\n",
        "        test_file = f\"{self.results_dir}test_data_{data_hash}_{timestamp}.npz\"\n",
        "        np.savez_compressed(\n",
        "            test_file,\n",
        "            x_test=x_test,\n",
        "            y_true=y_true,\n",
        "            y_pred=y_pred\n",
        "        )\n",
        "\n",
        "        # Save results\n",
        "        results_file = f\"{self.results_dir}results_{data_hash}_{timestamp}.json\"\n",
        "\n",
        "        # Convert numpy types for JSON\n",
        "        json_results = {}\n",
        "        for key, value in evaluation_results.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                json_results[key] = value.tolist()\n",
        "            elif isinstance(value, (np.integer, np.floating)):\n",
        "                json_results[key] = float(value)\n",
        "            else:\n",
        "                json_results[key] = value\n",
        "\n",
        "        json_results['timestamp'] = timestamp\n",
        "\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "        # Update latest pointer\n",
        "        latest_file = f\"{self.results_dir}latest_{data_hash}.json\"\n",
        "        latest_data = {\n",
        "            'test_file': test_file,\n",
        "            'results_file': results_file,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "\n",
        "        with open(latest_file, 'w') as f:\n",
        "            json.dump(latest_data, f, indent=2)\n",
        "\n",
        "        print(f\"💾 Results saved:\")\n",
        "        print(f\"   Test data: {test_file}\")\n",
        "        print(f\"   Results: {results_file}\")\n",
        "\n",
        "        return test_file, results_file\n",
        "\n",
        "    def load_test_results(self, data_hash):\n",
        "        \"\"\"Load test results\"\"\"\n",
        "        latest_file = f\"{self.results_dir}latest_{data_hash}.json\"\n",
        "\n",
        "        if os.path.exists(latest_file):\n",
        "            try:\n",
        "                with open(latest_file, 'r') as f:\n",
        "                    latest_data = json.load(f)\n",
        "\n",
        "                test_file = latest_data['test_file']\n",
        "                results_file = latest_data['results_file']\n",
        "\n",
        "                if os.path.exists(test_file) and os.path.exists(results_file):\n",
        "                    # Load test data\n",
        "                    test_data = np.load(test_file)\n",
        "\n",
        "                    # Load results\n",
        "                    with open(results_file, 'r') as f:\n",
        "                        results = json.load(f)\n",
        "\n",
        "                    print(f\"✅ Test results found:\")\n",
        "                    print(f\"   Generated: {latest_data['timestamp']}\")\n",
        "                    print(f\"   R²: {results.get('r2', 'Unknown')}\")\n",
        "\n",
        "                    return test_data, results\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Results cache corrupted: {e}\")\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    # ========== TRAINING SYSTEM ==========\n",
        "\n",
        "    def grid_search_hyperparameters(self, x_train, y_train, x_val, y_val, architecture_type, base_lr, base_batch_size):\n",
        "        \"\"\"Grid search with progress tracking\"\"\"\n",
        "        print(f\"\\n🔍 Grid search for {architecture_type}...\")\n",
        "\n",
        "        # Define search space\n",
        "        if architecture_type in [\"VERY_DEEP_WIDE\", \"DEEP_WIDE\"]:\n",
        "            lr_options = [base_lr * 0.5, base_lr, base_lr * 2]\n",
        "            batch_options = [base_batch_size // 2, base_batch_size, base_batch_size * 2]\n",
        "            l1_options = [0.0001, 0.001, 0.01]\n",
        "            l2_options = [0.001, 0.01, 0.1]\n",
        "        else:\n",
        "            lr_options = [base_lr * 0.5, base_lr, base_lr * 1.5]\n",
        "            batch_options = [base_batch_size, base_batch_size * 2]\n",
        "            l1_options = [0.0001, 0.001]\n",
        "            l2_options = [0.001, 0.01]\n",
        "\n",
        "        total = len(lr_options) * len(batch_options) * len(l1_options) * len(l2_options)\n",
        "        print(f\"   Testing {total} combinations...\")\n",
        "\n",
        "        best_score = -np.inf\n",
        "        best_params = None\n",
        "        count = 0\n",
        "\n",
        "        layers, _, _ = self.define_architecture(x_train.shape[1], architecture_type)\n",
        "\n",
        "        for lr in lr_options:\n",
        "            for batch in batch_options:\n",
        "                for l1 in l1_options:\n",
        "                    for l2 in l2_options:\n",
        "                        count += 1\n",
        "                        try:\n",
        "                            print(f\"   {count}/{total}: LR={lr:.4f}, Batch={batch}, L1={l1:.4f}, L2={l2:.4f}\")\n",
        "\n",
        "                            model = self.build_mlp_model(x_train.shape[1], layers, lr, l1, l2)\n",
        "\n",
        "                            history = model.fit(\n",
        "                                x_train, y_train,\n",
        "                                validation_data=(x_val, y_val),\n",
        "                                epochs=100,\n",
        "                                batch_size=int(batch),\n",
        "                                callbacks=[EarlyStopping(monitor='val_loss', patience=20)],\n",
        "                                verbose=0\n",
        "                            )\n",
        "\n",
        "                            score = -min(history.history['val_loss'])\n",
        "\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_params = {\n",
        "                                    'learning_rate': lr,\n",
        "                                    'batch_size': int(batch),\n",
        "                                    'l1_reg': l1,\n",
        "                                    'l2_reg': l2\n",
        "                                }\n",
        "                                print(f\"     ✓ New best: {-score:.6f}\")\n",
        "\n",
        "                            del model\n",
        "                            keras.backend.clear_session()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"     ✗ Failed: {e}\")\n",
        "\n",
        "        if best_params:\n",
        "            print(f\"\\n🏆 Best parameters:\")\n",
        "            for k, v in best_params.items():\n",
        "                print(f\"   {k}: {v}\")\n",
        "        else:\n",
        "            best_params = {\n",
        "                'learning_rate': base_lr,\n",
        "                'batch_size': base_batch_size,\n",
        "                'l1_reg': 0.001,\n",
        "                'l2_reg': 0.01\n",
        "            }\n",
        "\n",
        "        return best_params\n",
        "\n",
        "    def train_final_model(self, x_train, y_train, x_val, y_val, architecture_type, best_params, data_hash):\n",
        "        \"\"\"Train final model with checkpointing\"\"\"\n",
        "        print(f\"\\n🚀 Training final {architecture_type} model...\")\n",
        "\n",
        "        # Build model\n",
        "        layers, _, _ = self.define_architecture(x_train.shape[1], architecture_type)\n",
        "        self.model = self.build_mlp_model(\n",
        "            x_train.shape[1], layers,\n",
        "            best_params['learning_rate'],\n",
        "            best_params['l1_reg'],\n",
        "            best_params['l2_reg']\n",
        "        )\n",
        "\n",
        "        print(f\"   Parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        # Checkpoint setup\n",
        "        checkpoint_file = f\"{self.checkpoints_dir}training_{data_hash}.weights.h5\"\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                checkpoint_file,\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=100,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=30,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train\n",
        "        history = self.model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=1000,\n",
        "            batch_size=best_params['batch_size'],\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"✅ Training complete!\")\n",
        "        return history\n",
        "\n",
        "    def evaluate_performance(self, x_test, y_test, history):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        print(\"\\n📊 Evaluating performance...\")\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_scaled = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = self.y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "        y_true = self.y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Accuracy within tolerance\n",
        "        acc_05 = np.mean(np.abs(y_true - y_pred) <= 0.5) * 100\n",
        "        acc_1 = np.mean(np.abs(y_true - y_pred) <= 1) * 100\n",
        "        acc_15 = np.mean(np.abs(y_true - y_pred) <= 1.5) * 100\n",
        "        acc_2 = np.mean(np.abs(y_true - y_pred) <= 2) * 100\n",
        "\n",
        "        results = {\n",
        "            'r2': r2, 'mse': mse, 'mae': mae, 'rmse': rmse,\n",
        "            'acc_05': acc_05, 'acc_1': acc_1, 'acc_15': acc_15, 'acc_2': acc_2,\n",
        "            'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        print(f\"📈 Results:\")\n",
        "        print(f\"   R²: {r2:.6f}\")\n",
        "        print(f\"   MAE: {mae:.4f}\")\n",
        "        print(f\"   Accuracy ±1: {acc_1:.1f}%\")\n",
        "\n",
        "        # Plot results\n",
        "        self.plot_results(y_true, y_pred, history, r2, mae)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_results(self, y_true, y_pred, history, r2, mae):\n",
        "        \"\"\"Plot comprehensive results\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Training history\n",
        "        axes[0,0].plot(history.history['loss'], label='Training')\n",
        "        axes[0,0].plot(history.history['val_loss'], label='Validation')\n",
        "        axes[0,0].set_title('Loss')\n",
        "        axes[0,0].set_yscale('log')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # MAE history\n",
        "        axes[0,1].plot(history.history['mae'], label='Training')\n",
        "        axes[0,1].plot(history.history['val_mae'], label='Validation')\n",
        "        axes[0,1].set_title('MAE')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Predictions scatter\n",
        "        axes[0,2].scatter(y_true, y_pred, alpha=0.6, s=3)\n",
        "        axes[0,2].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "        axes[0,2].set_xlabel('True')\n",
        "        axes[0,2].set_ylabel('Predicted')\n",
        "        axes[0,2].set_title(f'Predictions (R²={r2:.4f})')\n",
        "        axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residuals\n",
        "        residuals = y_true - y_pred\n",
        "        axes[1,0].scatter(y_pred, residuals, alpha=0.6, s=3)\n",
        "        axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
        "        axes[1,0].set_xlabel('Predicted')\n",
        "        axes[1,0].set_ylabel('Residuals')\n",
        "        axes[1,0].set_title('Residuals')\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Error distribution\n",
        "        axes[1,1].hist(residuals, bins=50, alpha=0.7)\n",
        "        axes[1,1].set_xlabel('Residuals')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "        axes[1,1].set_title('Error Distribution')\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy bars\n",
        "        tolerances = [0.5, 1.0, 1.5, 2.0]\n",
        "        accuracies = [np.mean(np.abs(residuals) <= tol) * 100 for tol in tolerances]\n",
        "        axes[1,2].bar(tolerances, accuracies, alpha=0.7)\n",
        "        axes[1,2].set_xlabel('Tolerance')\n",
        "        axes[1,2].set_ylabel('Accuracy (%)')\n",
        "        axes[1,2].set_title('Accuracy vs Tolerance')\n",
        "        axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # ========== INCREMENTAL TRAINING ==========\n",
        "\n",
        "    def incremental_training(self, x_new, y_new, data_hash, epochs=100, learning_rate_factor=0.1):\n",
        "        \"\"\"Incremental training on new data\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No base model found! Train initial model first.\")\n",
        "\n",
        "        print(f\"\\n🔄 Incremental training on {len(x_new)} new samples...\")\n",
        "\n",
        "        # Scale new targets\n",
        "        y_new_scaled = self.y_scaler.transform(y_new.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Reduce learning rate\n",
        "        original_lr = self.model.optimizer.learning_rate.numpy()\n",
        "        new_lr = original_lr * learning_rate_factor\n",
        "        self.model.optimizer.learning_rate.assign(new_lr)\n",
        "\n",
        "        print(f\"   Learning rate: {original_lr:.6f} → {new_lr:.6f}\")\n",
        "\n",
        "        # Split new data\n",
        "        x_train_new, x_val_new, y_train_new, y_val_new = train_test_split(\n",
        "            x_new, y_new_scaled, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Incremental checkpoint\n",
        "        inc_checkpoint = f\"{self.incremental_dir}incremental_{data_hash}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.weights.h5\"\n",
        "\n",
        "        # Callbacks for incremental training\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "            ModelCheckpoint(inc_checkpoint, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "        ]\n",
        "\n",
        "        # Train incrementally\n",
        "        history = self.model.fit(\n",
        "            x_train_new, y_train_new,\n",
        "            validation_data=(x_val_new, y_val_new),\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Save incremental model\n",
        "        inc_model_file = f\"{self.incremental_dir}incremental_model_{data_hash}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.keras\"\n",
        "        self.model.save(inc_model_file)\n",
        "\n",
        "        print(f\"✅ Incremental training complete!\")\n",
        "        print(f\"💾 Incremental model saved: {inc_model_file}\")\n",
        "\n",
        "        return history, inc_model_file\n",
        "\n",
        "    # ========== MAIN INTERFACE ==========\n",
        "\n",
        "    def check_status(self, data_filename, windows_filename):\n",
        "        \"\"\"Check what's already completed\"\"\"\n",
        "        data_hash = self.get_data_hash(data_filename, windows_filename)\n",
        "\n",
        "        print(f\"\\n📋 STATUS CHECK (Hash: {data_hash})\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Check grid search\n",
        "        grid_params, arch_type, corr_analysis = self.load_grid_search_cache(data_hash)\n",
        "        grid_done = grid_params is not None\n",
        "\n",
        "        # Check trained model\n",
        "        model, scaler, metadata = self.load_trained_model_cache(data_hash)\n",
        "        training_done = model is not None\n",
        "\n",
        "        # Check results\n",
        "        test_data, results = self.load_test_results(data_hash)\n",
        "        results_done = test_data is not None\n",
        "\n",
        "        print(f\"Grid Search: {'✅ DONE' if grid_done else '❌ NEEDED'}\")\n",
        "        print(f\"Model Training: {'✅ DONE' if training_done else '❌ NEEDED'}\")\n",
        "        print(f\"Test Results: {'✅ DONE' if results_done else '❌ NEEDED'}\")\n",
        "        print(f\"Complete Pipeline: {'✅ READY' if all([grid_done, training_done, results_done]) else '❌ INCOMPLETE'}\")\n",
        "\n",
        "        if grid_done:\n",
        "            print(f\"   Architecture: {arch_type}\")\n",
        "            print(f\"   Max correlation: {corr_analysis['max_correlation']:.6f}\")\n",
        "\n",
        "        if training_done:\n",
        "            print(f\"   Final R²: {metadata.get('final_r2', 'Unknown')}\")\n",
        "\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return {\n",
        "            'data_hash': data_hash,\n",
        "            'grid_done': grid_done,\n",
        "            'training_done': training_done,\n",
        "            'results_done': results_done,\n",
        "            'all_done': all([grid_done, training_done, results_done])\n",
        "        }\n",
        "\n",
        "    def run_complete_pipeline(self, data_filename, windows_filename, force_retrain=False):\n",
        "        \"\"\"\n",
        "        Complete pipeline with full caching\n",
        "\n",
        "        Args:\n",
        "            data_filename: VAE synthetic data file\n",
        "            windows_filename: VAE windows file\n",
        "            force_retrain: Force complete retrain ignoring cache\n",
        "        \"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"🎯 COMPLETE CACHED MLP PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        data_hash = self.get_data_hash(data_filename, windows_filename)\n",
        "        print(f\"🔑 Data hash: {data_hash}\")\n",
        "\n",
        "        # Check status\n",
        "        status = self.check_status(data_filename, windows_filename)\n",
        "\n",
        "        if status['all_done'] and not force_retrain:\n",
        "            print(\"\\n🎉 EVERYTHING ALREADY COMPLETED!\")\n",
        "            print(\"Loading from cache...\")\n",
        "\n",
        "            # Load everything\n",
        "            grid_params, arch_type, corr_analysis = self.load_grid_search_cache(data_hash)\n",
        "            model, y_scaler, metadata = self.load_trained_model_cache(data_hash)\n",
        "            test_data, results = self.load_test_results(data_hash)\n",
        "\n",
        "            # Set instance variables\n",
        "            self.model = model\n",
        "            self.y_scaler = y_scaler\n",
        "            self.best_params = grid_params\n",
        "            self.correlation_analysis = corr_analysis\n",
        "\n",
        "            print(f\"\\n📊 CACHED RESULTS:\")\n",
        "            print(f\"   Architecture: {arch_type}\")\n",
        "            print(f\"   R²: {results['r2']:.6f}\")\n",
        "            print(f\"   MAE: {results['mae']:.4f}\")\n",
        "            print(f\"   Accuracy ±1: {results['acc_1']:.1f}%\")\n",
        "\n",
        "            return {\n",
        "                'status': 'loaded_from_cache',\n",
        "                'model': model,\n",
        "                'results': results,\n",
        "                'test_data': test_data,\n",
        "                'metadata': metadata\n",
        "            }\n",
        "\n",
        "        if force_retrain:\n",
        "            print(\"\\n🔄 FORCED RETRAIN - Ignoring all caches\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load data\n",
        "            x, y = self.load_vae_files(data_filename, windows_filename)\n",
        "\n",
        "            # 2. Grid search (with caching)\n",
        "            if not force_retrain and status['grid_done']:\n",
        "                print(\"\\n⚡ Using cached grid search\")\n",
        "                best_params, architecture_type, correlation_analysis = self.load_grid_search_cache(data_hash)\n",
        "            else:\n",
        "                print(\"\\n🔍 Running grid search...\")\n",
        "                # Analyze correlations\n",
        "                correlation_analysis = self.analyze_correlations(x, y)\n",
        "                architecture_type = correlation_analysis['architecture_recommendation']\n",
        "\n",
        "                # Split data\n",
        "                x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "                x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "                # Scale for grid search\n",
        "                temp_scaler = StandardScaler()\n",
        "                y_train_scaled = temp_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "                y_val_scaled = temp_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "                # Get architecture config\n",
        "                layers, default_lr, default_batch_size = self.define_architecture(x_train.shape[1], architecture_type)\n",
        "\n",
        "                # Run grid search\n",
        "                best_params = self.grid_search_hyperparameters(\n",
        "                    x_train, y_train_scaled, x_val, y_val_scaled,\n",
        "                    architecture_type, default_lr, default_batch_size\n",
        "                )\n",
        "\n",
        "                # Cache grid search results\n",
        "                self.save_grid_search_cache(best_params, architecture_type, correlation_analysis, data_hash)\n",
        "\n",
        "            # Set instance variables\n",
        "            self.correlation_analysis = correlation_analysis\n",
        "            self.best_params = best_params\n",
        "\n",
        "            # 3. Model training (with caching)\n",
        "            if not force_retrain and status['training_done']:\n",
        "                print(\"\\n⚡ Using cached trained model\")\n",
        "                model, y_scaler, metadata = self.load_trained_model_cache(data_hash)\n",
        "                self.model = model\n",
        "                self.y_scaler = y_scaler\n",
        "\n",
        "                # Check if results also cached\n",
        "                test_data, results = self.load_test_results(data_hash)\n",
        "                if test_data is not None:\n",
        "                    print(\"✅ Test results also cached\")\n",
        "                    return {\n",
        "                        'status': 'loaded_model_from_cache',\n",
        "                        'model': model,\n",
        "                        'results': results,\n",
        "                        'test_data': test_data,\n",
        "                        'metadata': metadata\n",
        "                    }\n",
        "            else:\n",
        "                print(\"\\n🚀 Training model...\")\n",
        "\n",
        "                # Prepare data (if not already done)\n",
        "                if 'x_train' not in locals():\n",
        "                    x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "                    x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "                # Scale targets\n",
        "                y_train_scaled = self.y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "                y_val_scaled = self.y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
        "                y_test_scaled = self.y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "                print(f\"   Data split: Train={len(x_train)}, Val={len(x_val)}, Test={len(x_test)}\")\n",
        "\n",
        "                # Train model\n",
        "                history = self.train_final_model(\n",
        "                    x_train, y_train_scaled, x_val, y_val_scaled,\n",
        "                    architecture_type, best_params, data_hash\n",
        "                )\n",
        "\n",
        "                # Evaluate\n",
        "                evaluation_results = self.evaluate_performance(x_test, y_test_scaled, history)\n",
        "\n",
        "                # Save trained model\n",
        "                training_metadata = {\n",
        "                    'architecture_type': architecture_type,\n",
        "                    'best_params': best_params,\n",
        "                    'correlation_analysis': correlation_analysis,\n",
        "                    'final_r2': evaluation_results['r2'],\n",
        "                    'final_mae': evaluation_results['mae'],\n",
        "                    'training_samples': len(x_train),\n",
        "                    'test_samples': len(x_test)\n",
        "                }\n",
        "\n",
        "                self.save_trained_model_cache(self.model, self.y_scaler, training_metadata, data_hash)\n",
        "\n",
        "                # Save test results\n",
        "                self.save_test_results(\n",
        "                    x_test, evaluation_results['y_true'], evaluation_results['y_pred'],\n",
        "                    evaluation_results, data_hash\n",
        "                )\n",
        "\n",
        "            # Final summary\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"🎉 PIPELINE COMPLETE!\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Architecture: {architecture_type}\")\n",
        "            print(f\"Max Correlation: {correlation_analysis['max_correlation']:.6f}\")\n",
        "            print(f\"Final R²: {evaluation_results['r2']:.6f}\")\n",
        "            print(f\"Final MAE: {evaluation_results['mae']:.4f}\")\n",
        "            print(f\"Accuracy ±1: {evaluation_results['acc_1']:.1f}%\")\n",
        "            print(f\"\\n💾 All results cached for future use!\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            return {\n",
        "                'status': 'newly_completed',\n",
        "                'model': self.model,\n",
        "                'results': evaluation_results,\n",
        "                'correlation_analysis': correlation_analysis,\n",
        "                'best_params': best_params,\n",
        "                'architecture_type': architecture_type\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Pipeline failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "# ========== SIMPLE INTERFACE FUNCTIONS ==========\n",
        "\n",
        "def run_mlp_pipeline(data_filename, windows_filename, force_retrain=False):\n",
        "    \"\"\"\n",
        "    Simple interface for complete MLP pipeline\n",
        "\n",
        "    Args:\n",
        "        data_filename: e.g., 'generated-data-OPTIMIZED.npy'\n",
        "        windows_filename: e.g., 'generated-data-true-window-OPTIMIZED.npy'\n",
        "        force_retrain: Force complete retrain ignoring all caches\n",
        "    \"\"\"\n",
        "    mlp = CompleteCachedMLP()\n",
        "    return mlp.run_complete_pipeline(data_filename, windows_filename, force_retrain)\n",
        "\n",
        "def check_mlp_status(data_filename, windows_filename):\n",
        "    \"\"\"Check what's already completed for given data files\"\"\"\n",
        "    mlp = CompleteCachedMLP()\n",
        "    return mlp.check_status(data_filename, windows_filename)\n",
        "\n",
        "def run_incremental_training(data_filename, windows_filename, x_new, y_new, epochs=100):\n",
        "    \"\"\"\n",
        "    Run incremental training on new data\n",
        "\n",
        "    Args:\n",
        "        data_filename: Original data file (for loading base model)\n",
        "        windows_filename: Original windows file\n",
        "        x_new: New feature data\n",
        "        y_new: New target data\n",
        "        epochs: Training epochs for incremental learning\n",
        "    \"\"\"\n",
        "    mlp = CompleteCachedMLP()\n",
        "\n",
        "    # Load base model\n",
        "    data_hash = mlp.get_data_hash(data_filename, windows_filename)\n",
        "    model, y_scaler, metadata = mlp.load_trained_model_cache(data_hash)\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(\"No trained base model found! Run main pipeline first.\")\n",
        "\n",
        "    mlp.model = model\n",
        "    mlp.y_scaler = y_scaler\n",
        "\n",
        "    # Run incremental training\n",
        "    return mlp.incremental_training(x_new, y_new, data_hash, epochs)\n",
        "\n",
        "def get_test_predictions(data_filename, windows_filename):\n",
        "    \"\"\"\n",
        "    Get saved test predictions for streaming simulation\n",
        "\n",
        "    Returns:\n",
        "        dict with 'x_test', 'y_true', 'y_pred' arrays\n",
        "    \"\"\"\n",
        "    mlp = CompleteCachedMLP()\n",
        "    data_hash = mlp.get_data_hash(data_filename, windows_filename)\n",
        "\n",
        "    test_data, results = mlp.load_test_results(data_hash)\n",
        "    if test_data is None:\n",
        "        raise ValueError(\"No test results found! Run main pipeline first.\")\n",
        "\n",
        "    return {\n",
        "        'x_test': test_data['x_test'],\n",
        "        'y_true': test_data['y_true'],\n",
        "        'y_pred': test_data['y_pred'],\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "# ========== MAIN EXECUTION ==========\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎯 Complete Cached MLP System\")\n",
        "\n",
        "    # SPECIFY YOUR FILES\n",
        "    data_file = 'generated-data-OPTIMIZED.npy'\n",
        "    windows_file = 'generated-data-true-window-OPTIMIZED.npy'\n",
        "\n",
        "    # Check status first\n",
        "    print(\"Checking current status...\")\n",
        "    status = check_mlp_status(data_file, windows_file)\n",
        "\n",
        "    if status['all_done']:\n",
        "        print(\"Everything already completed! Loading results...\")\n",
        "        results = run_mlp_pipeline(data_file, windows_file)\n",
        "        print(f\"Final R²: {results['results']['r2']:.6f}\")\n",
        "    else:\n",
        "        print(\"Running pipeline...\")\n",
        "        results = run_mlp_pipeline(data_file, windows_file)\n",
        "\n",
        "        if results:\n",
        "            print(f\"Pipeline complete! R²: {results['results']['r2']:.6f}\")\n",
        "        else:\n",
        "            print(\"Pipeline failed!\")\n",
        "\n",
        "# ========== USAGE EXAMPLES ==========\n",
        "\n",
        "\"\"\"\n",
        "# Basic usage - runs once, caches everything\n",
        "results = run_mlp_pipeline('data.npy', 'windows.npy')\n",
        "\n",
        "# Check what's already done\n",
        "status = check_mlp_status('data.npy', 'windows.npy')\n",
        "\n",
        "# Force retrain everything\n",
        "results = run_mlp_pipeline('data.npy', 'windows.npy', force_retrain=True)\n",
        "\n",
        "# Get test predictions for next steps\n",
        "test_data = get_test_predictions('data.npy', 'windows.npy')\n",
        "x_test = test_data['x_test']\n",
        "y_true = test_data['y_true']\n",
        "y_pred = test_data['y_pred']\n",
        "\n",
        "# Incremental training with new data\n",
        "new_x = np.random.randn(1000, 50)  # New synthetic data\n",
        "new_y = np.random.randint(5, 25, 1000)  # New windows\n",
        "inc_history, inc_model = run_incremental_training(\n",
        "    'data.npy', 'windows.npy', new_x, new_y, epochs=50\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyO2wgdutVK7pM6cPuiv2Qdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}