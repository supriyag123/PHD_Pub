{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE-METROPM-LARGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "caa4c627-62bf-4c77-fa60-3ac98e979e3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existing generated data found. Generating new data...\n",
            "Generating synthetic data in 5 batches...\n",
            " Batch 1/5 complete\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "# Load data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy')\n",
        "\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "maxval = train_data.shape[0]\n",
        "count_train = int(math.ceil(0.8*maxval))\n",
        "x_train = train_data[:count_train]\n",
        "x_test = train_data[count_train:]\n",
        "\n",
        "# Always define the custom Sampling layer (needed for loading existing models)\n",
        "saving.get_custom_objects().clear()\n",
        "\n",
        "@saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def __init__(self, factor):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "\n",
        "def safe_decode(decoder, z, batch_size=32):\n",
        "    outputs = []\n",
        "    for i in range(0, len(z), batch_size):\n",
        "        batch = z[i:i+batch_size]\n",
        "        out = decoder(batch)\n",
        "        outputs.append(out.numpy())\n",
        "    return np.concatenate(outputs, axis=0)\n",
        "\n",
        "# Check if trained VAE models already exist\n",
        "encoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-encoder-latent5-dim256.keras'\n",
        "decoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "    print(\"Found existing trained VAE models. Loading...\")\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "    print(\"VAE models loaded successfully!\")\n",
        "    history = None  # No training history since we didn't train\n",
        "\n",
        "else:\n",
        "    print(\"No existing VAE models found. Training new VAE...\")\n",
        "\n",
        "    # Build the encoder\n",
        "    latent_dim = 5\n",
        "    intermediate_dim = 256\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(window_size, n_features), name=\"encoder_input\")\n",
        "    x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "    xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "    x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\")(xx)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "    z = Sampling(1)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "\n",
        "    # Decoder\n",
        "    inp_z = Input(shape=(latent_dim,), name=\"decoder\")\n",
        "    x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "    x2 = layers.Dense(int(intermediate_dim/2), name=\"Dense2\")(x1)\n",
        "    x22 = layers.LSTM(int(intermediate_dim/2), activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "    x3 = layers.LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "    decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "    decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "    # Parameters\n",
        "    n_epochs = 20\n",
        "    klstart = 5\n",
        "    kl_annealtime = n_epochs - klstart\n",
        "    weight = K.variable(0.0)\n",
        "\n",
        "    # Define the VAE as a Model with a custom train_step\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(VAE, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(data)\n",
        "                reconstruction = self.decoder(z)\n",
        "                reconstruction_loss = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "                )\n",
        "\n",
        "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = reconstruction_loss + (weight * kl_loss)\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        def test_step(self, data):\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "    # CALLBACKS\n",
        "    es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "    class AnnealingCallback(Callback):\n",
        "        def __init__(self, weight):\n",
        "            self.weight = weight\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if epoch > klstart and epoch < klstart * 1.2:\n",
        "                new_weight = min(K.get_value(self.weight) + (1. / kl_annealtime), 1.)\n",
        "                K.set_value(self.weight, new_weight)\n",
        "            print(\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "    # Train the VAE\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "    history = vae.fit(x_train,\n",
        "                      epochs=n_epochs,\n",
        "                      batch_size=200,\n",
        "                      validation_split=0.1,\n",
        "                      callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "    # Save models\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "    print(\"VAE training complete and models saved!\")\n",
        "\n",
        "    # Reload models to ensure consistency\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "\n",
        "# Plot training history (only if we actually trained)\n",
        "if history is not None:\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['reconstruction_loss'], label='reconstruction_loss')\n",
        "    plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "    # Just Loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training plots since VAE was loaded from existing models\")\n",
        "\n",
        "# PLOT TRAIN RECONSTRUCTION\n",
        "x_train_small = x_train[:10000]\n",
        "X_test_encoded = encoder.predict(x_train_small)\n",
        "z = X_test_encoded[2]     # shape (10000, 5)\n",
        "# Decode safely\n",
        "X_test_predict = safe_decode(decoder, z, batch_size=32)\n",
        "\n",
        "\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_train[0:10000, :, 5], \"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\", label=\"reconstructed\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT TEST RECONSTRUCTION\n",
        "x_test_small = x_test[:10000]\n",
        "X_test_encoded = encoder.predict(x_test_small)\n",
        "z = X_test_encoded[2]     # shape (10000, 5)\n",
        "# Decode safely\n",
        "X_test_predict = safe_decode(decoder, z, batch_size=32)\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_test_small[0:1000, :, 5], \"r\")\n",
        "plt.plot(X_test_predict[0:1000, :, 5], \"b\")\n",
        "plt.show()\n",
        "\n",
        "#ANother test - pick a specific example\n",
        "i = 0  # pick a specific example\n",
        "x_true  = x_test[i, :, 5]          # feature 6\n",
        "z_mean, z_log_var, z = encoder.predict(x_test[i:i+1])\n",
        "x_recon = decoder.predict(z)[0, :, 5]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(x_true, 'r', label='True')\n",
        "plt.plot(x_recon, 'b', label='Recon')\n",
        "plt.legend()\n",
        "plt.title(f'Window {i} â€“ Feature 6')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Check if generated data already exists\n",
        "generated_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated_large_subsquence2_data.npy'\n",
        "\n",
        "if os.path.exists(generated_data_path):\n",
        "    print(\"Found existing generated data. Loading...\")\n",
        "    results1 = np.load(generated_data_path)\n",
        "    print(f\"Loaded generated data with shape: {results1.shape}\")\n",
        "else:\n",
        "    print(\"No existing generated data found. Generating new data...\")\n",
        "    # Generate data for MLP\n",
        "\n",
        "    N_SAMPLES = 10000          # number of real windows to sample\n",
        "    generator_multiply =   5            # synthetic per real window\n",
        "    BATCH_SIZE = 2000           # safe batch size for VAE inference\n",
        "\n",
        "    # =========================================================\n",
        "    # 1. RANDOMLY SAMPLE 50,000 WINDOWS\n",
        "    # =========================================================\n",
        "    idx = np.random.choice(n_seq, N_SAMPLES, replace=False)\n",
        "    sampled_data = train_data[idx]\n",
        "\n",
        "        # =========================================================\n",
        "    # 2. USE VAE (encoder + decoder) TO GENERATE SYNTHETIC WINDOWS\n",
        "    # =========================================================\n",
        "\n",
        "    generated_list = []\n",
        "\n",
        "    num_batches = int(np.ceil(N_SAMPLES / BATCH_SIZE))\n",
        "    print(f\"Generating synthetic data in {num_batches} batches...\")\n",
        "\n",
        "    for b in range(num_batches):\n",
        "        start = b * BATCH_SIZE\n",
        "        end = min((b+1)*BATCH_SIZE, N_SAMPLES)\n",
        "        batch = sampled_data[start:end]\n",
        "\n",
        "        # Encode\n",
        "        z_mean, z_log_var, _ = encoder.predict(batch, verbose=0)\n",
        "\n",
        "        # Generate MULTIPLIER random z for each window\n",
        "        sigma = tf.exp(0.5 * z_log_var)\n",
        "\n",
        "        for m in range(generator_multiply):\n",
        "            z_new = tf.random.normal(\n",
        "                shape=z_mean.shape,\n",
        "                mean=z_mean,\n",
        "                stddev=sigma\n",
        "            )\n",
        "            decoded = decoder.predict(z_new, verbose=0)\n",
        "\n",
        "            # decoded shape: (batch, window_size, n_features)\n",
        "            generated_list.append(decoded)\n",
        "\n",
        "        print(f\" Batch {b+1}/{num_batches} complete\")\n",
        "          # Stack all generated batches\n",
        "    generated_data = np.concatenate(generated_list, axis=0)\n",
        "    print(f\"Synthetic windows: {generated_data.shape}\")\n",
        "    # =========================================================\n",
        "    # 3. COMBINE REAL + SYNTHETIC DATA\n",
        "    # =========================================================\n",
        "    combined_data = np.concatenate([train_data, generated_data], axis=0)\n",
        "    results1 =combined_data\n",
        "    np.save(generated_data_path, results1)\n",
        "    print(f\"Generated and saved new data with shape: {results1.shape}\")\n",
        "\n",
        "# Generate window labels using VAR analysis - Dynamic batching with checkpoints\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "# Check if final VAR analysis results already exist\n",
        "final_window_labels_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy'\n",
        "final_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data2.npy'\n",
        "\n",
        "if os.path.exists(final_window_labels_path) and os.path.exists(final_data_path):\n",
        "    print(\"Found existing VAR analysis results. Loading...\")\n",
        "    y = np.load(final_window_labels_path)\n",
        "    x_final = np.load(final_data_path)\n",
        "    print(f\"Loaded final results - X: {x_final.shape}, Y: {y.shape}\")\n",
        "    x = results1\n",
        "else:\n",
        "    print(\"No existing VAR analysis results found. Need to run VAR analysis...\")\n",
        "\n",
        "    # Make sure we have the generated data (results1)\n",
        "    x = results1\n",
        "    x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "    n_future = 1\n",
        "    K = window_size\n",
        "\n",
        "    from statsmodels.tsa.api import VAR\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    def process_with_dynamic_batching(x_3d, batch_size=50000, checkpoint_dir=\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM\"):\n",
        "        checkpoint_file = f\"{checkpoint_dir}/var_analysis_progress.pkl\"\n",
        "\n",
        "        # Load previous progress if exists\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                start_idx, all_windows = pickle.load(f)\n",
        "            print(f\"Resuming VAR analysis from sample {start_idx}. Already processed: {len(all_windows)} samples\")\n",
        "        else:\n",
        "            start_idx, all_windows = 0, []\n",
        "            print(\"Starting fresh VAR analysis\")\n",
        "\n",
        "        total_samples = x_3d.shape[0]\n",
        "        print(f\"Total samples to process: {total_samples}\")\n",
        "\n",
        "        batch_num = start_idx // batch_size + 1\n",
        "\n",
        "        for i in range(start_idx, total_samples):\n",
        "            # Your exact same VAR analysis logic\n",
        "            rmse_list = []\n",
        "            for k in range(2, round(K)):\n",
        "                cur_seq = x_3d[i, :, :]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "                model = VAR(df_train)\n",
        "                try:\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "                    if i % 5000 == 0:  # Reduce print frequency\n",
        "                        print('VAR could not solve row number', i, k)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            all_windows.append(min_sw)\n",
        "\n",
        "            # Progress reporting and checkpointing\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f'Processed {i + 1}/{total_samples} samples. Current SW = {min_sw}')\n",
        "\n",
        "            # Save checkpoint every 5000 samples\n",
        "            if (i + 1) % 5000 == 0:\n",
        "                with open(checkpoint_file, 'wb') as f:\n",
        "                    pickle.dump((i + 1, all_windows), f)\n",
        "                print(f\"Checkpoint saved at sample {i + 1}\")\n",
        "\n",
        "                # Optional: Clean up memory\n",
        "                gc.collect()\n",
        "\n",
        "            # Save batch results (compatible with original format) every batch_size samples\n",
        "            if (i + 1) % batch_size == 0 or (i + 1) == total_samples:\n",
        "                batch_start = ((i + 1) - 1) // batch_size * batch_size\n",
        "                batch_windows = all_windows[batch_start:i + 1]\n",
        "                batch_data = x[batch_start:i + 1, :]\n",
        "\n",
        "                np.save(f'{checkpoint_dir}/generated-data-true-window2-BATCH{batch_num}.npy', np.array(batch_windows))\n",
        "                np.save(f'{checkpoint_dir}/generated-data2-BATCH{batch_num}.npy', batch_data)\n",
        "\n",
        "                print(f\"Batch {batch_num} saved: samples {batch_start} to {i}\")\n",
        "                batch_num += 1\n",
        "\n",
        "        # Final save\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump((total_samples, all_windows), f)\n",
        "\n",
        "        # Clean up checkpoint file\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "\n",
        "        return np.array(all_windows)\n",
        "\n",
        "    # Run the dynamic batching process\n",
        "    print(\"Starting VAR analysis with dynamic batching...\")\n",
        "    y = process_with_dynamic_batching(x_3d)\n",
        "\n",
        "    # Save final combined results\n",
        "    np.save(final_window_labels_path, y)\n",
        "    np.save(final_data_path, x[:len(y)])\n",
        "    print(f\"VAR analysis complete. Saved final results - X: {x[:len(y)].shape}, Y: {y.shape}\")\n",
        "\n",
        "print(\"VAE training complete. Generated data and window labels saved.\")\n",
        "print(f\"Generated data shape: {x[:len(y)].shape}\")\n",
        "print(f\"Window labels shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE-METROPM-LARGE.ipynb",
      "authorship_tag": "ABX9TyMMxNmIWjWLbTBh62z7P/xg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}