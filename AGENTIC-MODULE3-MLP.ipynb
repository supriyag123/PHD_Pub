{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "6e1fe68d-d0ae-47a3-9d3a-a9c98fff55dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded MLP model from /content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\n",
            "Loaded saved transformer\n",
            "AdaptiveWindowAgent adaptive_window_agent initialized\n",
            "Model loaded: True\n",
            "Transformer fitted: True\n",
            "Loading your actual dataset...\n",
            "Loaded Long_train shape: (3627, 50, 12)\n",
            "Testing with last 10 sequences: (100, 50, 12)\n",
            "\n",
            "Starting real-time VAR testing...\n",
            "======================================================================\n",
            "\n",
            "Sample 1: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.07456951]]\n",
            "After inverse transform: 2.0745694637298584\n",
            "Final predicted window: 2\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   1: MLP= 2, MSE=0.0310, MAE=0.1604, MAPE=0.1933, Avg_MSE=0.0310\n",
            "\n",
            "Sample 2: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.19865508]]\n",
            "After inverse transform: 1.801344871520996\n",
            "Final predicted window: 2\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   2: MLP= 2, MSE=0.0005, MAE=0.0206, MAPE=0.0248, Avg_MSE=0.0158\n",
            "\n",
            "Sample 3: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.56594723]]\n",
            "After inverse transform: 1.4340527057647705\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample   3: MLP= 1, MSE=0.0029, MAE=0.0496, MAPE=0.0577, Avg_MSE=0.0115\n",
            "\n",
            "Sample 4: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.09267227]]\n",
            "After inverse transform: 1.9073277711868286\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   4: MLP= 2, MSE=0.0009, MAE=0.0273, MAPE=0.0329, Avg_MSE=0.0088\n",
            "\n",
            "Sample 5: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.01779409]]\n",
            "After inverse transform: 2.017794132232666\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   5: MLP= 2, MSE=0.0004, MAE=0.0192, MAPE=0.0233, Avg_MSE=0.0071\n",
            "\n",
            "Sample 6: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.02909505]]\n",
            "After inverse transform: 2.029094934463501\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   6: MLP= 2, MSE=0.0038, MAE=0.0540, MAPE=0.0711, Avg_MSE=0.0066\n",
            "\n",
            "Sample 7: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.05236618]]\n",
            "After inverse transform: 1.9476338624954224\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   7: MLP= 2, MSE=0.0029, MAE=0.0469, MAPE=0.0607, Avg_MSE=0.0061\n",
            "\n",
            "Sample 8: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.02546352]]\n",
            "After inverse transform: 2.025463581085205\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   8: MLP= 2, MSE=0.0000, MAE=0.0031, MAPE=0.0040, Avg_MSE=0.0053\n",
            "\n",
            "Sample 9: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.34673703]]\n",
            "After inverse transform: 2.3467369079589844\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample   9: MLP= 2, MSE=0.0084, MAE=0.0772, MAPE=0.1090, Avg_MSE=0.0056\n",
            "\n",
            "Sample 10: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.187189]]\n",
            "After inverse transform: 1.812811017036438\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  10: MLP= 2, MSE=0.0000, MAE=0.0036, MAPE=0.0047, Avg_MSE=0.0051\n",
            "\n",
            "Sample 11: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.87791]]\n",
            "After inverse transform: 1.1220899820327759\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  11: MLP= 1, MSE=0.0078, MAE=0.0749, MAPE=0.1040, Avg_MSE=0.0028\n",
            "\n",
            "Sample 12: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.1352944]]\n",
            "After inverse transform: 1.8647055625915527\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  12: MLP= 2, MSE=0.0109, MAE=0.0877, MAPE=0.1246, Avg_MSE=0.0038\n",
            "\n",
            "Sample 13: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.4058134]]\n",
            "After inverse transform: 1.594186544418335\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  13: MLP= 2, MSE=0.0001, MAE=0.0075, MAPE=0.0096, Avg_MSE=0.0035\n",
            "\n",
            "Sample 14: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.16370656]]\n",
            "After inverse transform: 1.8362934589385986\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  14: MLP= 2, MSE=0.0003, MAE=0.0148, MAPE=0.0211, Avg_MSE=0.0035\n",
            "\n",
            "Sample 15: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.02099545]]\n",
            "After inverse transform: 1.9790045022964478\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  15: MLP= 2, MSE=0.0012, MAE=0.0296, MAPE=0.0393, Avg_MSE=0.0035\n",
            "\n",
            "Sample 16: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.5598121]]\n",
            "After inverse transform: 2.559812068939209\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  16: MLP= 3, MSE=0.0069, MAE=0.0715, MAPE=0.0970, Avg_MSE=0.0038\n",
            "\n",
            "Sample 17: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.4299761]]\n",
            "After inverse transform: 2.429975986480713\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  17: MLP= 2, MSE=0.0000, MAE=0.0028, MAPE=0.0039, Avg_MSE=0.0036\n",
            "\n",
            "Sample 18: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.2688505]]\n",
            "After inverse transform: 1.731149435043335\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  18: MLP= 2, MSE=0.0031, MAE=0.0467, MAPE=0.0659, Avg_MSE=0.0039\n",
            "\n",
            "Sample 19: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.2033272]]\n",
            "After inverse transform: 1.7966728210449219\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  19: MLP= 2, MSE=0.0000, MAE=0.0043, MAPE=0.0056, Avg_MSE=0.0030\n",
            "\n",
            "Sample 20: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.00729604]]\n",
            "After inverse transform: 2.007296085357666\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  20: MLP= 2, MSE=0.0033, MAE=0.0493, MAPE=0.0675, Avg_MSE=0.0034\n",
            "\n",
            "Sample 21: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.01606648]]\n",
            "After inverse transform: 2.016066551208496\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  21: MLP= 2, MSE=0.0452, MAE=0.1626, MAPE=0.2778, Avg_MSE=0.0071\n",
            "\n",
            "Sample 22: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.07703466]]\n",
            "After inverse transform: 2.0770347118377686\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  22: MLP= 2, MSE=0.0070, MAE=0.0691, MAPE=0.1019, Avg_MSE=0.0067\n",
            "\n",
            "Sample 23: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.23493044]]\n",
            "After inverse transform: 1.765069603919983\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  23: MLP= 2, MSE=0.0016, MAE=0.0332, MAPE=0.0493, Avg_MSE=0.0069\n",
            "\n",
            "Sample 24: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.16861497]]\n",
            "After inverse transform: 2.1686148643493652\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  24: MLP= 2, MSE=0.0034, MAE=0.0459, MAPE=0.0746, Avg_MSE=0.0072\n",
            "\n",
            "Sample 25: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.41980475]]\n",
            "After inverse transform: 2.419804811477661\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  25: MLP= 2, MSE=0.0464, MAE=0.2023, MAPE=0.2295, Avg_MSE=0.0117\n",
            "\n",
            "Sample 26: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[1.4227241]]\n",
            "After inverse transform: 3.4227242469787598\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  26: MLP= 3, MSE=0.3146, MAE=0.3845, MAPE=0.8182, Avg_MSE=0.0425\n",
            "\n",
            "Sample 27: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[1.0639875]]\n",
            "After inverse transform: 3.0639874935150146\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  27: MLP= 3, MSE=14.0391, MAE=1.5793, MAPE=8.8893, Avg_MSE=1.4464\n",
            "\n",
            "Sample 28: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.9308607]]\n",
            "After inverse transform: 2.930860757827759\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  28: MLP= 3, MSE=0.0604, MAE=0.1036, MAPE=0.5829, Avg_MSE=1.4521\n",
            "\n",
            "Sample 29: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.38342255]]\n",
            "After inverse transform: 2.383422613143921\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  29: MLP= 2, MSE=0.4312, MAE=0.2768, MAPE=1.5579, Avg_MSE=1.4952\n",
            "\n",
            "Sample 30: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.2846727]]\n",
            "After inverse transform: 0.715327262878418\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  30: MLP= 1, MSE=0.4189, MAE=0.5585, MAPE=0.7501, Avg_MSE=1.5368\n",
            "\n",
            "Sample 31: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.9304126]]\n",
            "After inverse transform: 2.930412530899048\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  31: MLP= 3, MSE=0.2792, MAE=0.4445, MAPE=0.6280, Avg_MSE=1.5602\n",
            "\n",
            "Sample 32: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[1.4247704]]\n",
            "After inverse transform: 3.4247703552246094\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  32: MLP= 3, MSE=0.1701, MAE=0.3628, MAPE=0.4688, Avg_MSE=1.5765\n",
            "\n",
            "Sample 33: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[1.3250856]]\n",
            "After inverse transform: 3.3250856399536133\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  33: MLP= 3, MSE=0.0552, MAE=0.2072, MAPE=0.2665, Avg_MSE=1.5819\n",
            "\n",
            "Sample 34: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[1.6419039]]\n",
            "After inverse transform: 3.641903877258301\n",
            "Final predicted window: 4\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=4\n",
            "Sample  34: MLP= 4, MSE=0.0088, MAE=0.0823, MAPE=0.1069, Avg_MSE=1.5824\n",
            "\n",
            "Sample 35: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.01206948]]\n",
            "After inverse transform: 1.9879305362701416\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  35: MLP= 2, MSE=0.0016, MAE=0.0339, MAPE=0.0483, Avg_MSE=1.5779\n",
            "\n",
            "Sample 36: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.08585989]]\n",
            "After inverse transform: 1.9141401052474976\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  36: MLP= 2, MSE=0.0074, MAE=0.0749, MAPE=0.0984, Avg_MSE=1.5472\n",
            "\n",
            "Sample 37: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.14978606]]\n",
            "After inverse transform: 2.1497859954833984\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  37: MLP= 2, MSE=0.0042, MAE=0.0564, MAPE=0.0742, Avg_MSE=0.1437\n",
            "\n",
            "Sample 38: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.452501]]\n",
            "After inverse transform: 2.452501058578491\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  38: MLP= 2, MSE=0.0119, MAE=0.0915, MAPE=0.1304, Avg_MSE=0.1389\n",
            "\n",
            "Sample 39: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.3700659]]\n",
            "After inverse transform: 0.6299340724945068\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  39: MLP= 1, MSE=0.0006, MAE=0.0212, MAPE=0.0278, Avg_MSE=0.0958\n",
            "\n",
            "Sample 40: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.6558471]]\n",
            "After inverse transform: 0.34415292739868164\n",
            "Final predicted window: 0\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  40: MLP= 0, MSE=0.0000, MAE=0.0022, MAPE=0.0030, Avg_MSE=0.0539\n",
            "\n",
            "Sample 41: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.3436196]]\n",
            "After inverse transform: 0.6563804149627686\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  41: MLP= 1, MSE=0.0052, MAE=0.0602, MAPE=0.0869, Avg_MSE=0.0265\n",
            "\n",
            "Sample 42: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.2538162]]\n",
            "After inverse transform: 0.7461837530136108\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  42: MLP= 1, MSE=0.0035, MAE=0.0524, MAPE=0.0667, Avg_MSE=0.0098\n",
            "\n",
            "Sample 43: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.529106]]\n",
            "After inverse transform: 2.5291061401367188\n",
            "Final predicted window: 3\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  43: MLP= 3, MSE=0.0207, MAE=0.1205, MAPE=0.1722, Avg_MSE=0.0064\n",
            "\n",
            "Sample 44: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.00087554]]\n",
            "After inverse transform: 1.9991244077682495\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  44: MLP= 2, MSE=0.0785, MAE=0.2177, MAPE=0.3606, Avg_MSE=0.0134\n",
            "\n",
            "Sample 45: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.11599243]]\n",
            "After inverse transform: 1.8840075731277466\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  45: MLP= 2, MSE=0.0108, MAE=0.0815, MAPE=0.1324, Avg_MSE=0.0143\n",
            "\n",
            "Sample 46: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.61792004]]\n",
            "After inverse transform: 1.3820799589157104\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  46: MLP= 1, MSE=0.0048, MAE=0.0562, MAPE=0.0849, Avg_MSE=0.0140\n",
            "\n",
            "Sample 47: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-1.2089076]]\n",
            "After inverse transform: 0.7910923957824707\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  47: MLP= 1, MSE=0.0011, MAE=0.0271, MAPE=0.0411, Avg_MSE=0.0137\n",
            "\n",
            "Sample 48: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.7296497]]\n",
            "After inverse transform: 1.2703502178192139\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  48: MLP= 1, MSE=0.0031, MAE=0.0449, MAPE=0.0694, Avg_MSE=0.0128\n",
            "\n",
            "Sample 49: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.38327008]]\n",
            "After inverse transform: 1.616729974746704\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  49: MLP= 2, MSE=0.0028, MAE=0.0434, MAPE=0.0646, Avg_MSE=0.0131\n",
            "\n",
            "Sample 50: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  50: MLP= 2, MSE=0.0182, MAE=0.1182, MAPE=0.1538, Avg_MSE=0.0149\n",
            "\n",
            "Sample 51: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.00530032]]\n",
            "After inverse transform: 1.9946997165679932\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  51: MLP= 2, MSE=0.0363, MAE=0.1670, MAPE=0.2173, Avg_MSE=0.0180\n",
            "\n",
            "Sample 52: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.01728295]]\n",
            "After inverse transform: 2.0172829627990723\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  52: MLP= 2, MSE=0.0062, MAE=0.0688, MAPE=0.0896, Avg_MSE=0.0182\n",
            "\n",
            "Sample 53: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.16519932]]\n",
            "After inverse transform: 2.1651992797851562\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  53: MLP= 2, MSE=0.0019, MAE=0.0381, MAPE=0.0495, Avg_MSE=0.0164\n",
            "\n",
            "Sample 54: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.3246448]]\n",
            "After inverse transform: 1.6753551959991455\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  54: MLP= 2, MSE=0.0006, MAE=0.0223, MAPE=0.0283, Avg_MSE=0.0086\n",
            "\n",
            "Sample 55: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.1376996]]\n",
            "After inverse transform: 1.8623003959655762\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  55: MLP= 2, MSE=0.0190, MAE=0.1227, MAPE=0.1548, Avg_MSE=0.0094\n",
            "\n",
            "Sample 56: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.00891851]]\n",
            "After inverse transform: 2.008918523788452\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  56: MLP= 2, MSE=0.0136, MAE=0.1032, MAPE=0.1318, Avg_MSE=0.0103\n",
            "\n",
            "Sample 57: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.17253454]]\n",
            "After inverse transform: 1.8274654150009155\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  57: MLP= 2, MSE=0.0032, MAE=0.0474, MAPE=0.0666, Avg_MSE=0.0105\n",
            "\n",
            "Sample 58: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.35298002]]\n",
            "After inverse transform: 2.352980136871338\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  58: MLP= 2, MSE=0.0027, MAE=0.0454, MAPE=0.0594, Avg_MSE=0.0104\n",
            "\n",
            "Sample 59: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.06763162]]\n",
            "After inverse transform: 2.067631721496582\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  59: MLP= 2, MSE=0.0016, MAE=0.0354, MAPE=0.0460, Avg_MSE=0.0103\n",
            "\n",
            "Sample 60: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.44869107]]\n",
            "After inverse transform: 2.4486911296844482\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  60: MLP= 2, MSE=0.0001, MAE=0.0082, MAPE=0.0114, Avg_MSE=0.0085\n",
            "\n",
            "Sample 61: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.35938096]]\n",
            "After inverse transform: 2.3593809604644775\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  61: MLP= 2, MSE=0.0022, MAE=0.0423, MAPE=0.0523, Avg_MSE=0.0051\n",
            "\n",
            "Sample 62: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.1710979]]\n",
            "After inverse transform: 2.171097993850708\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  62: MLP= 2, MSE=0.0038, MAE=0.0532, MAPE=0.0722, Avg_MSE=0.0049\n",
            "\n",
            "Sample 63: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.07828445]]\n",
            "After inverse transform: 2.078284502029419\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  63: MLP= 2, MSE=0.0000, MAE=0.0032, MAPE=0.0041, Avg_MSE=0.0047\n",
            "\n",
            "Sample 64: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  64: MLP= 2, MSE=0.0023, MAE=0.0420, MAPE=0.0556, Avg_MSE=0.0049\n",
            "\n",
            "Sample 65: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  65: MLP= 2, MSE=0.0057, MAE=0.0638, MAPE=0.0894, Avg_MSE=0.0035\n",
            "\n",
            "Sample 66: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  66: MLP= 2, MSE=0.0091, MAE=0.0830, MAPE=0.1092, Avg_MSE=0.0031\n",
            "\n",
            "Sample 67: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.09597102]]\n",
            "After inverse transform: 2.09597110748291\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  67: MLP= 2, MSE=0.0023, MAE=0.0400, MAPE=0.0565, Avg_MSE=0.0030\n",
            "\n",
            "Sample 68: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.05664887]]\n",
            "After inverse transform: 2.0566489696502686\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  68: MLP= 2, MSE=0.0049, MAE=0.0584, MAPE=0.0836, Avg_MSE=0.0032\n",
            "\n",
            "Sample 69: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.09643653]]\n",
            "After inverse transform: 1.9035634994506836\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  69: MLP= 2, MSE=0.0010, MAE=0.0258, MAPE=0.0386, Avg_MSE=0.0031\n",
            "\n",
            "Sample 70: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.11407109]]\n",
            "After inverse transform: 1.8859288692474365\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  70: MLP= 2, MSE=0.0003, MAE=0.0143, MAPE=0.0226, Avg_MSE=0.0032\n",
            "\n",
            "Sample 71: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.21636368]]\n",
            "After inverse transform: 1.7836363315582275\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  71: MLP= 2, MSE=0.0217, MAE=0.1210, MAPE=0.1790, Avg_MSE=0.0051\n",
            "\n",
            "Sample 72: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.41955584]]\n",
            "After inverse transform: 1.580444097518921\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  72: MLP= 2, MSE=0.0000, MAE=0.0034, MAPE=0.0049, Avg_MSE=0.0047\n",
            "\n",
            "Sample 73: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.4707737]]\n",
            "After inverse transform: 1.529226303100586\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  73: MLP= 2, MSE=0.0080, MAE=0.0744, MAPE=0.1076, Avg_MSE=0.0055\n",
            "\n",
            "Sample 74: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.28916585]]\n",
            "After inverse transform: 1.7108341455459595\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  74: MLP= 2, MSE=0.1225, MAE=0.3497, MAPE=0.3503, Avg_MSE=0.0175\n",
            "\n",
            "Sample 75: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.64982706]]\n",
            "After inverse transform: 1.350172996520996\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  75: MLP= 1, MSE=0.0220, MAE=0.1317, MAPE=0.1667, Avg_MSE=0.0192\n",
            "\n",
            "Sample 76: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.6198998]]\n",
            "After inverse transform: 1.3801002502441406\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  76: MLP= 1, MSE=0.0082, MAE=0.0841, MAPE=0.0972, Avg_MSE=0.0191\n",
            "\n",
            "Sample 77: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.3108291]]\n",
            "After inverse transform: 1.6891708374023438\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  77: MLP= 2, MSE=0.0019, MAE=0.0391, MAPE=0.0493, Avg_MSE=0.0190\n",
            "\n",
            "Sample 78: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.5171206]]\n",
            "After inverse transform: 1.482879400253296\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  78: MLP= 1, MSE=0.0085, MAE=0.0769, MAPE=0.1104, Avg_MSE=0.0194\n",
            "\n",
            "Sample 79: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.68278104]]\n",
            "After inverse transform: 1.3172190189361572\n",
            "Final predicted window: 1\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  79: MLP= 1, MSE=0.0019, MAE=0.0373, MAPE=0.0518, Avg_MSE=0.0195\n",
            "\n",
            "Sample 80: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.31938946]]\n",
            "After inverse transform: 1.6806105375289917\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  80: MLP= 2, MSE=0.0002, MAE=0.0129, MAPE=0.0170, Avg_MSE=0.0195\n",
            "\n",
            "Sample 81: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.19536592]]\n",
            "After inverse transform: 1.8046340942382812\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  81: MLP= 2, MSE=0.0109, MAE=0.0917, MAPE=0.1188, Avg_MSE=0.0184\n",
            "\n",
            "Sample 82: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.595, MAPE ratio=1.166, MAE ratio=1.279, Consecutive poor predictions=0\n",
            "WARNING:__main__:Insufficient data for retraining\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[-0.00441009]]\n",
            "After inverse transform: 1.9955899715423584\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  82: MLP= 2, MSE=0.0264, MAE=0.1366, MAPE=0.1931, Avg_MSE=0.0210\n",
            "\n",
            "Sample 83: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.04946238]]\n",
            "After inverse transform: 2.04946231842041\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  83: MLP= 2, MSE=0.0341, MAE=0.1694, MAPE=0.2014, Avg_MSE=0.0237\n",
            "*** DRIFT DETECTED at sample 83 ***\n",
            "\n",
            "Sample 84: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.635, MAPE ratio=1.192, MAE ratio=1.307, Consecutive poor predictions=0\n",
            "WARNING:__main__:Drift detected: MSE ratio=1.609, MAPE ratio=1.170, MAE ratio=1.287, Consecutive poor predictions=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  84: MLP= 2, MSE=0.0027, MAE=0.0434, MAPE=0.0614, Avg_MSE=0.0117\n",
            "*** DRIFT DETECTED at sample 84 ***\n",
            "\n",
            "Sample 85: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  85: MLP= 2, MSE=0.0053, MAE=0.0635, MAPE=0.0829, Avg_MSE=0.0100\n",
            "*** DRIFT DETECTED at sample 85 ***\n",
            "\n",
            "Sample 86: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.594, MAPE ratio=1.162, MAE ratio=1.279, Consecutive poor predictions=0\n",
            "WARNING:__main__:Drift detected: MSE ratio=1.593, MAPE ratio=1.140, MAE ratio=1.260, Consecutive poor predictions=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[0.10648827]]\n",
            "After inverse transform: 2.1064882278442383\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  86: MLP= 2, MSE=0.0083, MAE=0.0802, MAPE=0.1035, Avg_MSE=0.0100\n",
            "*** DRIFT DETECTED at sample 86 ***\n",
            "\n",
            "Sample 87: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.02493097]]\n",
            "After inverse transform: 2.024930953979492\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  87: MLP= 2, MSE=0.0000, MAE=0.0014, MAPE=0.0019, Avg_MSE=0.0098\n",
            "*** DRIFT DETECTED at sample 87 ***\n",
            "\n",
            "Sample 88: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.608, MAPE ratio=1.114, MAE ratio=1.237, Consecutive poor predictions=0\n",
            "WARNING:__main__:Drift detected: MSE ratio=1.601, MAPE ratio=1.095, MAE ratio=1.222, Consecutive poor predictions=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[0.02494869]]\n",
            "After inverse transform: 2.0249485969543457\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  88: MLP= 2, MSE=0.0000, MAE=0.0014, MAPE=0.0020, Avg_MSE=0.0090\n",
            "*** DRIFT DETECTED at sample 88 ***\n",
            "\n",
            "Sample 89: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.07065196]]\n",
            "After inverse transform: 1.9293479919433594\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  89: MLP= 2, MSE=0.0001, MAE=0.0105, MAPE=0.0136, Avg_MSE=0.0088\n",
            "*** DRIFT DETECTED at sample 89 ***\n",
            "\n",
            "Sample 90: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.609, MAPE ratio=1.105, MAE ratio=1.232, Consecutive poor predictions=0\n",
            "WARNING:__main__:Drift detected: MSE ratio=1.492, MAPE ratio=1.097, MAE ratio=1.240, Consecutive poor predictions=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[-0.07603101]]\n",
            "After inverse transform: 1.923969030380249\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  90: MLP= 2, MSE=0.0021, MAE=0.0376, MAPE=0.0552, Avg_MSE=0.0090\n",
            "*** DRIFT DETECTED at sample 90 ***\n",
            "\n",
            "Sample 91: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.07623447]]\n",
            "After inverse transform: 1.9237655401229858\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  91: MLP= 2, MSE=0.0073, MAE=0.0742, MAPE=0.0979, Avg_MSE=0.0086\n",
            "\n",
            "Sample 92: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.03084563]]\n",
            "After inverse transform: 1.9691543579101562\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  92: MLP= 2, MSE=0.0066, MAE=0.0698, MAPE=0.0947, Avg_MSE=0.0066\n",
            "*** DRIFT DETECTED at sample 92 ***\n",
            "\n",
            "Sample 93: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=1.798, MAPE ratio=1.198, MAE ratio=1.378, Consecutive poor predictions=0\n",
            "WARNING:__main__:Drift detected: MSE ratio=21.000, MAPE ratio=3.718, MAE ratio=1.677, Consecutive poor predictions=1\n",
            "WARNING:__main__:Drift detected: MSE ratio=114.058, MAPE ratio=9.481, MAE ratio=2.940, Consecutive poor predictions=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[-0.16598846]]\n",
            "After inverse transform: 1.8340115547180176\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  93: MLP= 2, MSE=0.0495, MAE=0.2088, MAPE=0.2371, Avg_MSE=0.0082\n",
            "*** DRIFT DETECTED at sample 93 ***\n",
            "\n",
            "Sample 94: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.37979573]]\n",
            "After inverse transform: 2.379795789718628\n",
            "Final predicted window: 2\n",
            "Removing constant columns: ['V12']\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  94: MLP= 2, MSE=4.1402, MAE=0.8705, MAPE=4.7559, Avg_MSE=0.4219\n",
            "*** DRIFT DETECTED at sample 94 ***\n",
            "\n",
            "Sample 95: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.83759195]]\n",
            "After inverse transform: 1.1624081134796143\n",
            "Final predicted window: 1\n",
            "VAR fit successful with trend='n', lags=1\n",
            "Sample  95: MLP= 1, MSE=20.0850, MAE=1.9174, MAPE=10.4750, Avg_MSE=2.4299\n",
            "*** DRIFT DETECTED at sample 95 ***\n",
            "\n",
            "Sample 96: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=283.404, MAPE ratio=17.346, MAE ratio=4.712, Consecutive poor predictions=3\n",
            "WARNING:__main__:Drift detected: MSE ratio=282.693, MAPE ratio=17.326, MAE ratio=4.671, Consecutive poor predictions=4\n",
            "WARNING:__main__:Drift detected: MSE ratio=283.905, MAPE ratio=17.690, MAE ratio=5.277, Consecutive poor predictions=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[-0.44911188]]\n",
            "After inverse transform: 1.5508880615234375\n",
            "Final predicted window: 2\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  96: MLP= 2, MSE=36.8793, MAE=2.5982, MAPE=14.1941, Avg_MSE=6.1170\n",
            "*** DRIFT DETECTED at sample 96 ***\n",
            "\n",
            "Sample 97: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[-0.02855344]]\n",
            "After inverse transform: 1.9714465141296387\n",
            "Final predicted window: 2\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  97: MLP= 2, MSE=0.0022, MAE=0.0201, MAPE=0.1097, Avg_MSE=6.1172\n",
            "*** DRIFT DETECTED at sample 97 ***\n",
            "\n",
            "Sample 98: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[0.9754352]]\n",
            "After inverse transform: 2.975435256958008\n",
            "Final predicted window: 3\n",
            "VAR fit successful with trend='n', lags=3\n",
            "Sample  98: MLP= 3, MSE=1.2891, MAE=1.0364, MAPE=1.2438, Avg_MSE=6.2461\n",
            "*** DRIFT DETECTED at sample 98 ***\n",
            "\n",
            "Sample 99: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Drift detected: MSE ratio=290.198, MAPE ratio=18.414, MAE ratio=5.979, Consecutive poor predictions=6\n",
            "WARNING:__main__:Drift detected: MSE ratio=308.683, MAPE ratio=19.762, MAE ratio=6.704, Consecutive poor predictions=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction: [[-0.4948423]]\n",
            "After inverse transform: 1.505157709121704\n",
            "Final predicted window: 2\n",
            "VAR fit successful with trend='n', lags=2\n",
            "Sample  99: MLP= 2, MSE=1.2199, MAE=1.0083, MAPE=1.2100, Avg_MSE=6.3681\n",
            "*** DRIFT DETECTED at sample 99 ***\n",
            "\n",
            "Sample 100: Using flattened sequence of size 600\n",
            "Input feature vector shape: (1, 600)\n",
            "Raw prediction: [[2.174047]]\n",
            "After inverse transform: 4.174046993255615\n",
            "Final predicted window: 4\n",
            "VAR fit successful with trend='n', lags=4\n",
            "Sample 100: MLP= 4, MSE=0.3684, MAE=0.5665, MAPE=0.6504, Avg_MSE=6.4047\n",
            "*** DRIFT DETECTED at sample 100 ***\n",
            "\n",
            "======================================================================\n",
            "FINAL PERFORMANCE SUMMARY\n",
            "======================================================================\n",
            "Total predictions: 100\n",
            "Successful predictions: 50\n",
            "Success rate: 50.00%\n",
            "Average MSE: 6.4047\n",
            "Average MAE: 0.8370\n",
            "Average MAPE: 3.3068\n",
            "Drift events: 17\n",
            "Retraining events: 0\n",
            "Transformer fitted: True\n",
            "MSE trend: -0.4973 (positive = improving)\n",
            "MAPE trend: -0.2458 (positive = improving)\n",
            "MAE trend: -0.0664 (positive = improving)\n",
            "\n",
            "Test results saved to: real_data_test_results.json\n"
          ]
        }
      ],
      "source": [
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque, defaultdict, Counter\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Agent A: Adaptive Window Management with Enhanced MLP\n",
        "\n",
        "    Capabilities:\n",
        "    1. Invoke new data and score using trained MLP\n",
        "    2. Calculate actual performance using VAR forecast (real-time only)\n",
        "    3. Track accuracy and performance statistics\n",
        "    4. Monitor for drift in prediction performance\n",
        "    5. Communicate with sensor agents to verify drift\n",
        "    6. Retrain MLP when drift is confirmed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str = \"adaptive_window_agent\",\n",
        "                 model_path: str = None,\n",
        "                 checkpoint_path: str = None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "        self.checkpoint_path = checkpoint_path or \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/ckp2.weights.h5\"\n",
        "\n",
        "        # Core MLP components\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Performance tracking - Updated to use MSE/MAE/MAPE only\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mape_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # Drift detection parameters - Updated thresholds\n",
        "        self.drift_detection_window = 50\n",
        "        self.drift_threshold_mse = 0.2    # 20% increase in MSE\n",
        "        self.drift_threshold_mape = 0.2   # 20% increase in MAPE\n",
        "        self.drift_threshold_mae = 0.2    # 20% increase in MAE\n",
        "        self.consecutive_poor_predictions = 0\n",
        "        self.drift_confirmed = False\n",
        "\n",
        "        # Statistics storage - Updated for new metrics\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'avg_mape': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        # Retraining data storage\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        # Sensor agents for drift confirmation\n",
        "        self.sensor_agents = {}\n",
        "\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "        print(f\"Model loaded: {self.is_model_loaded}\")\n",
        "        print(f\"Transformer fitted: {self.transformer_fitted}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load trained MLP model and recreate transformer using original training data\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                print(f\"Loaded MLP model from {self.model_path}\")\n",
        "                self.is_model_loaded = True\n",
        "\n",
        "                # Try to load saved transformer first\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                    print(\"Loaded saved transformer\")\n",
        "                else:\n",
        "                    # Recreate transformer from original training data\n",
        "                    print(\"No saved transformer found, recreating from original training data...\")\n",
        "\n",
        "                    try:\n",
        "                        # Load your original y training data\n",
        "                        y_original = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "\n",
        "                        # Fit transformer on original training data (same as your training code)\n",
        "                        self.transformer = StandardScaler()\n",
        "                        self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                        self.transformer_fitted = True\n",
        "\n",
        "                        # Save it for future use\n",
        "                        with open(transformer_path, 'wb') as f:\n",
        "                            pickle.dump(self.transformer, f)\n",
        "\n",
        "                        print(f\"Fitted transformer on {len(y_original)} original training samples and saved\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not load original training data: {e}\")\n",
        "                        self.transformer = StandardScaler()\n",
        "                        self.transformer_fitted = False\n",
        "\n",
        "            else:\n",
        "                print(f\"Model file not found at {self.model_path}\")\n",
        "                self.is_model_loaded = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            self.is_model_loaded = False\n",
        "\n",
        "    def evaluate_forecast_performance(self, sequence_3d: np.ndarray, predicted_window: int, n_future: int = 1) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Use predicted window to forecast with VAR and calculate MSE, MAE, MAPE\n",
        "        Handles constant columns and other VAR issues robustly\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Your existing VAR forecast code\n",
        "            x1 = sequence_3d  # Get the subsequences feature here\n",
        "            df = pd.DataFrame(x1, columns=['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12'])\n",
        "            df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "\n",
        "            # Check for constant columns and remove them\n",
        "            constant_columns = []\n",
        "            df_train_cleaned = df_train.copy()\n",
        "\n",
        "            # More robust constant column detection\n",
        "            for col in df_train_cleaned.columns:\n",
        "                col_data = df_train_cleaned[col]\n",
        "                # Check for constant values (all same) or near-constant (very low variance)\n",
        "                if col_data.nunique() <= 1 or col_data.var() < 1e-12:\n",
        "                    constant_columns.append(col)\n",
        "\n",
        "            if constant_columns:\n",
        "                print(f\"Removing constant columns: {constant_columns}\")\n",
        "                df_train_cleaned = df_train_cleaned.drop(columns=constant_columns)\n",
        "                df_test_cleaned = df_test.drop(columns=constant_columns)\n",
        "            else:\n",
        "                df_test_cleaned = df_test\n",
        "\n",
        "            # Double-check: verify no constant columns remain\n",
        "            remaining_constant = []\n",
        "            for col in df_train_cleaned.columns:\n",
        "                if df_train_cleaned[col].nunique() <= 1 or df_train_cleaned[col].var() < 1e-12:\n",
        "                    remaining_constant.append(col)\n",
        "\n",
        "            if remaining_constant:\n",
        "                print(f\"Still found constant columns after cleaning: {remaining_constant}\")\n",
        "                df_train_cleaned = df_train_cleaned.drop(columns=remaining_constant)\n",
        "                df_test_cleaned = df_test_cleaned.drop(columns=remaining_constant)\n",
        "                constant_columns.extend(remaining_constant)\n",
        "\n",
        "            # Check if we have enough non-constant columns for VAR\n",
        "            if len(df_train_cleaned.columns) < 2:\n",
        "                print(f\"Not enough non-constant columns ({len(df_train_cleaned.columns)}) for VAR modeling\")\n",
        "                return {\n",
        "                    'mse': 99999,\n",
        "                    'mape': 99999,\n",
        "                    'mae': 99999,\n",
        "                    'forecast_success': False\n",
        "                }\n",
        "\n",
        "            # Check if predicted window is valid (not larger than available data)\n",
        "            k = min(predicted_window, len(df_train_cleaned) - 2)  # Leave room for VAR lag requirements\n",
        "            if k < 1:\n",
        "                k = 1\n",
        "\n",
        "            # Try VAR modeling with cleaned data\n",
        "            model = VAR(df_train_cleaned)\n",
        "\n",
        "            # Try fitting with different trend options if default fails\n",
        "            model_fitted = None\n",
        "            trend_options = ['n', 'c', 'ct', 'ctt']  # Start with 'n' (no trend) first\n",
        "\n",
        "            for trend in trend_options:\n",
        "                try:\n",
        "                    model_fitted = model.fit(maxlags=k, trend=trend)\n",
        "                    print(f\"VAR fit successful with trend='{trend}', lags={k}\")\n",
        "                    break\n",
        "                except Exception as trend_e:\n",
        "                    print(f\"VAR fit failed with trend='{trend}': {trend_e}\")\n",
        "                    continue\n",
        "\n",
        "            if model_fitted is None:\n",
        "                print(f\"VAR could not fit with any trend option\")\n",
        "                return {\n",
        "                    'mse': 99999,\n",
        "                    'mape': 99999,\n",
        "                    'mae': 99999,\n",
        "                    'forecast_success': False\n",
        "                }\n",
        "\n",
        "            # Make forecast\n",
        "            forecast_input = df_train_cleaned.values[-model_fitted.k_ar:]\n",
        "            fc = model_fitted.forecast(y=forecast_input, steps=n_future)\n",
        "            df_forecast = pd.DataFrame(fc, index=df.index[-n_future:], columns=df_train_cleaned.columns)\n",
        "\n",
        "            # Calculate metrics - use V1 if available, otherwise use first available column\n",
        "            target_col = 'V1' if 'V1' in df_forecast.columns else df_forecast.columns[0]\n",
        "\n",
        "            if target_col not in df_test_cleaned.columns:\n",
        "                print(f\"Target column {target_col} was removed due to being constant\")\n",
        "                return {\n",
        "                    'mse': 99999,\n",
        "                    'mape': 99999,\n",
        "                    'mae': 99999,\n",
        "                    'forecast_success': False\n",
        "                }\n",
        "\n",
        "            # Calculate error metrics\n",
        "            actual = df_test_cleaned[target_col].values[0]\n",
        "            predicted = df_forecast[target_col].values[0]\n",
        "\n",
        "            # Avoid division by zero in relative metrics\n",
        "            if abs(actual) < 1e-10:\n",
        "                print(f\"Actual value too close to zero ({actual}) for MAPE calculation\")\n",
        "                mape = 99999\n",
        "            else:\n",
        "                mape = abs((actual - predicted) / actual)\n",
        "\n",
        "            # Calculate MSE and MAE\n",
        "            mse = ((actual - predicted) ** 2) / max(abs(actual), 1e-10)\n",
        "            mae = abs(actual - predicted)\n",
        "\n",
        "            return {\n",
        "                'mse': mse,\n",
        "                'mape': mape,\n",
        "                'mae': mae,\n",
        "                'forecast_success': True,\n",
        "                'target_column': target_col,\n",
        "                'constant_columns_removed': constant_columns,\n",
        "                'var_lags_used': model_fitted.k_ar\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'VAR could not solve for window {predicted_window}: {e}')\n",
        "            return {\n",
        "                'mse': 99999,\n",
        "                'mape': 99999,\n",
        "                'mae': 99999,\n",
        "                'forecast_success': False,\n",
        "                'error_details': str(e)\n",
        "            }\n",
        "\n",
        "    def _get_most_frequent_window(self) -> int:\n",
        "        \"\"\"Get the most frequently occurring window size from original training data\"\"\"\n",
        "        try:\n",
        "            # Load your original training ground truth data\n",
        "            y_training_data = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "\n",
        "            # Find most frequent window size from training data\n",
        "            window_counts = Counter(y_training_data.astype(int))\n",
        "            most_frequent_window = window_counts.most_common(1)[0][0]\n",
        "\n",
        "            print(f\"Using most frequent window from training data: {most_frequent_window}\")\n",
        "            return most_frequent_window\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load training data for fallback: {e}\")\n",
        "            return 20  # Only if training data unavailable\n",
        "\n",
        "    def predict_window_size(self, feature_vector: np.ndarray, sequence_3d: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Predict window size using MLP and calculate VAR performance in real-time\n",
        "        \"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\n",
        "                'predicted_window': 20,\n",
        "                'confidence': 0.0,\n",
        "                'error': \"Model not loaded\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Ensure feature vector is 2D\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            print(f\"Input feature vector shape: {feature_vector.shape}\")\n",
        "\n",
        "            # 1. INVOKE NEW DATA AND SCORE USING MLP\n",
        "            try:\n",
        "                prediction_raw = self.model.predict(feature_vector, verbose=0)\n",
        "            except Exception as e:\n",
        "                if \"input shape\" in str(e) and \"32\" in str(e):\n",
        "                    print(\"Padding input to batch size 32\")\n",
        "                    feature_batch = np.repeat(feature_vector, 32, axis=0)\n",
        "                    prediction_batch = self.model.predict(feature_batch, verbose=0)\n",
        "                    prediction_raw = prediction_batch[0:1]\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "            print(f\"Raw prediction: {prediction_raw}\")\n",
        "\n",
        "            # 2. TRANSFORM PREDICTION BACK TO ORIGINAL SCALE\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = self.transformer.inverse_transform(prediction_raw)[0, 0]\n",
        "                print(f\"After inverse transform: {predicted_window}\")\n",
        "            else:\n",
        "                predicted_window = prediction_raw[0, 0]\n",
        "                logger.warning(\"Transformer not fitted yet, using raw prediction\")\n",
        "                print(f\"Using raw prediction: {predicted_window}\")\n",
        "\n",
        "            predicted_window = int(round(predicted_window))\n",
        "            print(f\"Final predicted window: {predicted_window}\")\n",
        "\n",
        "            # Create prediction record\n",
        "            prediction_record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'feature_vector': feature_vector.flatten(),\n",
        "                'raw_prediction': prediction_raw[0, 0],\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "\n",
        "            # 3. CALCULATE ACTUAL PERFORMANCE AGAINST VAR FORECAST\n",
        "            forecast_metrics = self.evaluate_forecast_performance(\n",
        "                sequence_3d, predicted_window, n_future=1\n",
        "            )\n",
        "\n",
        "            prediction_record.update({\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'mse': forecast_metrics.get('mse', 99999),\n",
        "                'mape': forecast_metrics.get('mape', 99999),\n",
        "                'mae': forecast_metrics.get('mae', 99999),\n",
        "                'forecast_success': forecast_metrics.get('forecast_success', False)\n",
        "            })\n",
        "\n",
        "            # 4. TRACK PERFORMANCE STATISTICS (Updated to use MSE/MAE/MAPE)\n",
        "            if forecast_metrics.get('forecast_success', False):\n",
        "                # Store individual metrics\n",
        "                self.mse_history.append(forecast_metrics['mse'])\n",
        "                self.mape_history.append(forecast_metrics['mape'])\n",
        "                self.mae_history.append(forecast_metrics['mae'])\n",
        "\n",
        "                # Update performance statistics\n",
        "                self.performance_stats.update({\n",
        "                    'total_predictions': self.performance_stats['total_predictions'] + 1,\n",
        "                    'avg_mse': np.mean(self.mse_history),\n",
        "                    'avg_mape': np.mean(self.mape_history),\n",
        "                    'avg_mae': np.mean(self.mae_history)\n",
        "                })\n",
        "\n",
        "                # Add current metrics to prediction record\n",
        "                prediction_record.update({\n",
        "                    'current_mse': forecast_metrics['mse'],\n",
        "                    'current_mape': forecast_metrics['mape'],\n",
        "                    'current_mae': forecast_metrics['mae'],\n",
        "                    'recent_avg_mse': np.mean(list(self.mse_history)[-10:]) if len(self.mse_history) >= 10 else forecast_metrics['mse'],\n",
        "                    'recent_avg_mape': np.mean(list(self.mape_history)[-10:]) if len(self.mape_history) >= 10 else forecast_metrics['mape'],\n",
        "                    'recent_avg_mae': np.mean(list(self.mae_history)[-10:]) if len(self.mae_history) >= 10 else forecast_metrics['mae']\n",
        "                })\n",
        "\n",
        "                # 5. CHECK FOR DRIFT (Updated to use new metrics)\n",
        "                drift_detected = self._check_for_drift()\n",
        "                prediction_record['drift_detected'] = drift_detected\n",
        "\n",
        "                if drift_detected:\n",
        "                    prediction_record['drift_action'] = self._handle_drift_detection(feature_vector, predicted_window)\n",
        "            else:\n",
        "                # VAR forecast failed - treat as poor prediction\n",
        "                self.consecutive_poor_predictions += 1\n",
        "                prediction_record.update({\n",
        "                    'drift_detected': False,\n",
        "                    'forecast_failed': True\n",
        "                })\n",
        "\n",
        "            # Store prediction\n",
        "            self.prediction_history.append(prediction_record)\n",
        "\n",
        "            # Add to retraining buffer (store features and predicted window)\n",
        "            self.retraining_data['x_buffer'].append(feature_vector.flatten())\n",
        "            self.retraining_data['y_buffer'].append(predicted_window)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'confidence': self._calculate_confidence(prediction_record),\n",
        "                'performance_stats': self.get_recent_performance(),\n",
        "                'drift_detected': prediction_record.get('drift_detected', False),\n",
        "                'prediction_id': len(self.prediction_history),\n",
        "                'transformer_status': 'fitted' if self.transformer_fitted else 'not_fitted'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction error: {e}\")\n",
        "            return {\n",
        "                'predicted_window': 20,\n",
        "                'confidence': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _check_for_drift(self) -> bool:\n",
        "        \"\"\"Monitor and identify drift in prediction performance using MSE/MAE/MAPE\"\"\"\n",
        "        if len(self.mse_history) < self.drift_detection_window:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Get recent and historical performance for each metric\n",
        "            recent_window = 20\n",
        "            historical_start = self.drift_detection_window\n",
        "\n",
        "            # MSE drift detection\n",
        "            recent_mse = np.mean(list(self.mse_history)[-recent_window:])\n",
        "            historical_mse = np.mean(list(self.mse_history)[-historical_start:-recent_window])\n",
        "            mse_increase_ratio = recent_mse / max(historical_mse, 0.001)\n",
        "            mse_drift = mse_increase_ratio > (1 + self.drift_threshold_mse)\n",
        "\n",
        "            # MAPE drift detection\n",
        "            recent_mape = np.mean(list(self.mape_history)[-recent_window:])\n",
        "            historical_mape = np.mean(list(self.mape_history)[-historical_start:-recent_window])\n",
        "            mape_increase_ratio = recent_mape / max(historical_mape, 0.001)\n",
        "            mape_drift = mape_increase_ratio > (1 + self.drift_threshold_mape)\n",
        "\n",
        "            # MAE drift detection\n",
        "            recent_mae = np.mean(list(self.mae_history)[-recent_window:])\n",
        "            historical_mae = np.mean(list(self.mae_history)[-historical_start:-recent_window])\n",
        "            mae_increase_ratio = recent_mae / max(historical_mae, 0.001)\n",
        "            mae_drift = mae_increase_ratio > (1 + self.drift_threshold_mae)\n",
        "\n",
        "            # Track consecutive poor predictions (high MSE/MAE/MAPE)\n",
        "            recent_high_mse = recent_mse > np.percentile(list(self.mse_history), 75)\n",
        "            recent_high_mape = recent_mape > np.percentile(list(self.mape_history), 75)\n",
        "            recent_high_mae = recent_mae > np.percentile(list(self.mae_history), 75)\n",
        "\n",
        "            if recent_high_mse and recent_high_mape and recent_high_mae:\n",
        "                self.consecutive_poor_predictions += 1\n",
        "            else:\n",
        "                self.consecutive_poor_predictions = 0\n",
        "\n",
        "            consecutive_drift = self.consecutive_poor_predictions > 10\n",
        "\n",
        "            # Drift detected if multiple conditions met\n",
        "            drift_score = sum([mse_drift, mape_drift, mae_drift])\n",
        "            drift_detected = (drift_score >= 2) or consecutive_drift\n",
        "\n",
        "            if drift_detected:\n",
        "                logger.warning(f\"Drift detected: MSE ratio={mse_increase_ratio:.3f}, \"\n",
        "                              f\"MAPE ratio={mape_increase_ratio:.3f}, MAE ratio={mae_increase_ratio:.3f}, \"\n",
        "                              f\"Consecutive poor predictions={self.consecutive_poor_predictions}\")\n",
        "                self.performance_stats['drift_events'] += 1\n",
        "\n",
        "            return drift_detected\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Drift detection error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _handle_drift_detection(self, current_features: np.ndarray, predicted_window: int) -> str:\n",
        "        \"\"\"Handle drift detection using new metrics\"\"\"\n",
        "        if self.drift_confirmed:\n",
        "            return \"Already handling drift\"\n",
        "\n",
        "        # Query sensor agents for their drift status\n",
        "        sensor_drift_confirmations = self._query_sensor_agents_for_drift()\n",
        "\n",
        "        # If majority of sensors also detect drift, confirm and retrain\n",
        "        if sensor_drift_confirmations >= len(self.sensor_agents) * 0.6 or len(self.sensor_agents) == 0:\n",
        "            self.drift_confirmed = True\n",
        "            logger.info(\"Drift confirmed by sensor agents. Initiating retraining...\")\n",
        "\n",
        "            # Retrain MLP\n",
        "            retrain_success = self._retrain_model()\n",
        "\n",
        "            if retrain_success:\n",
        "                self.drift_confirmed = False\n",
        "                self.consecutive_poor_predictions = 0\n",
        "                self.performance_stats['retraining_events'] += 1\n",
        "                self.performance_stats['last_retrain_time'] = dt.datetime.now()\n",
        "                return \"Retraining completed successfully\"\n",
        "            else:\n",
        "                return \"Retraining failed\"\n",
        "        else:\n",
        "            return f\"Drift suspected but not confirmed by sensors ({sensor_drift_confirmations}/{len(self.sensor_agents)})\"\n",
        "\n",
        "    def _query_sensor_agents_for_drift(self) -> int:\n",
        "        \"\"\"Query sensor agents to confirm drift\"\"\"\n",
        "        confirmations = 0\n",
        "\n",
        "        for sensor_id, sensor_agent in self.sensor_agents.items():\n",
        "            try:\n",
        "                # This would be actual message passing in full implementation\n",
        "                sensor_drift = np.random.random() > 0.7  # Simulate sensor response\n",
        "                if sensor_drift:\n",
        "                    confirmations += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error querying sensor {sensor_id}: {e}\")\n",
        "\n",
        "        return confirmations\n",
        "\n",
        "    def _retrain_model(self) -> bool:\n",
        "        \"\"\"Retrain MLP using buffered data\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Starting MLP retraining...\")\n",
        "\n",
        "            if len(self.retraining_data['x_buffer']) < 100:\n",
        "                logger.warning(\"Insufficient data for retraining\")\n",
        "                return False\n",
        "\n",
        "            # Prepare retraining data\n",
        "            X_retrain = np.array(list(self.retraining_data['x_buffer']))\n",
        "            y_raw = np.array(list(self.retraining_data['y_buffer']))\n",
        "\n",
        "            # Transform y data for training if transformer is fitted\n",
        "            if self.transformer_fitted:\n",
        "                y_retrain = self.transformer.transform(y_raw.reshape(-1, 1)).flatten()\n",
        "            else:\n",
        "                y_retrain = y_raw\n",
        "\n",
        "            # Create new model with same architecture\n",
        "            new_model = Sequential()\n",
        "            new_model.add(Dense(64, activation='relu', input_shape=(X_retrain.shape[1],)))\n",
        "            new_model.add(Dense(32, activation='relu'))\n",
        "            new_model.add(Dense(16, activation='relu'))\n",
        "            new_model.add(Dense(8, activation='relu'))\n",
        "            new_model.add(Dense(1))\n",
        "\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=0.0003, clipnorm=1)\n",
        "            new_model.compile(loss='mean_squared_error', optimizer=optimizer,\n",
        "                            metrics=['mean_squared_error'])\n",
        "\n",
        "            es = keras.callbacks.EarlyStopping(\n",
        "                patience=10, verbose=0, min_delta=0.0001,\n",
        "                monitor='loss', mode='min', restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            # Train the new model\n",
        "            history = new_model.fit(\n",
        "                X_retrain, y_retrain,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[es],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate new model performance\n",
        "            val_loss = min(history.history['val_loss'])\n",
        "\n",
        "            # Only replace model if new one is better\n",
        "            current_recent_mse = np.mean(list(self.mse_history)[-10:]) if self.mse_history else float('inf')\n",
        "            if val_loss < current_recent_mse * 1.1:\n",
        "                # Replace the model\n",
        "                self.model = new_model\n",
        "\n",
        "                # Save the retrained model\n",
        "                retrain_path = self.model_path.replace('.keras', '_retrained.keras')\n",
        "                self.model.save(retrain_path)\n",
        "\n",
        "                # Clear history to start fresh\n",
        "                self.mse_history.clear()\n",
        "                self.mape_history.clear()\n",
        "                self.mae_history.clear()\n",
        "\n",
        "                logger.info(f\"Model successfully retrained. New validation loss: {val_loss:.4f}\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(f\"New model performance worse ({val_loss:.4f} vs {current_recent_mse:.4f})\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Retraining failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _calculate_confidence(self, prediction_record: Dict) -> float:\n",
        "        \"\"\"Calculate confidence based on recent MSE/MAE/MAPE performance\"\"\"\n",
        "        if len(self.mse_history) < 10:\n",
        "            return 0.5\n",
        "\n",
        "        # Calculate confidence based on recent performance metrics\n",
        "        recent_mse = np.mean(list(self.mse_history)[-10:])\n",
        "        recent_mape = np.mean(list(self.mape_history)[-10:])\n",
        "        recent_mae = np.mean(list(self.mae_history)[-10:])\n",
        "\n",
        "        # Normalize metrics to 0-1 scale (lower is better for all three)\n",
        "        # Use historical percentiles for normalization\n",
        "        mse_percentile = np.percentile(list(self.mse_history), 25) if len(self.mse_history) > 20 else recent_mse\n",
        "        mape_percentile = np.percentile(list(self.mape_history), 25) if len(self.mape_history) > 20 else recent_mape\n",
        "        mae_percentile = np.percentile(list(self.mae_history), 25) if len(self.mae_history) > 20 else recent_mae\n",
        "\n",
        "        # Calculate confidence scores (higher when metrics are lower)\n",
        "        mse_confidence = max(0, 1 - (recent_mse / max(mse_percentile * 4, 0.001)))\n",
        "        mape_confidence = max(0, 1 - (recent_mape / max(mape_percentile * 4, 0.001)))\n",
        "        mae_confidence = max(0, 1 - (recent_mae / max(mae_percentile * 4, 0.001)))\n",
        "\n",
        "        # Average confidence across metrics\n",
        "        confidence = (mse_confidence + mape_confidence + mae_confidence) / 3\n",
        "\n",
        "        # Apply penalty for forecast failures\n",
        "        if not prediction_record.get('forecast_success', True):\n",
        "            confidence *= 0.5\n",
        "\n",
        "        return min(1.0, max(0.1, confidence))\n",
        "\n",
        "    def get_recent_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get recent performance statistics using MSE/MAE/MAPE\"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return {}\n",
        "\n",
        "        # Get successful forecasts only\n",
        "        successful_predictions = [p for p in list(self.prediction_history)[-50:]\n",
        "                                if p.get('forecast_success', False)]\n",
        "\n",
        "        performance_data = {\n",
        "            'total_predictions': len(self.prediction_history),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(self.prediction_history), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'last_retrain': self.performance_stats['last_retrain_time'],\n",
        "            'consecutive_poor': self.consecutive_poor_predictions,\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "        # Add metric statistics if available\n",
        "        if self.mse_history:\n",
        "            performance_data.update({\n",
        "                'recent_mse': np.mean(list(self.mse_history)[-10:]),\n",
        "                'avg_mse': self.performance_stats.get('avg_mse', 0.0),\n",
        "                'mse_trend': self._calculate_trend(list(self.mse_history)[-20:]) if len(self.mse_history) >= 20 else 0.0\n",
        "            })\n",
        "\n",
        "        if self.mape_history:\n",
        "            performance_data.update({\n",
        "                'recent_mape': np.mean(list(self.mape_history)[-10:]),\n",
        "                'avg_mape': self.performance_stats.get('avg_mape', 0.0),\n",
        "                'mape_trend': self._calculate_trend(list(self.mape_history)[-20:]) if len(self.mape_history) >= 20 else 0.0\n",
        "            })\n",
        "\n",
        "        if self.mae_history:\n",
        "            performance_data.update({\n",
        "                'recent_mae': np.mean(list(self.mae_history)[-10:]),\n",
        "                'avg_mae': self.performance_stats.get('avg_mae', 0.0),\n",
        "                'mae_trend': self._calculate_trend(list(self.mae_history)[-20:]) if len(self.mae_history) >= 20 else 0.0\n",
        "            })\n",
        "\n",
        "        return performance_data\n",
        "\n",
        "    def _calculate_trend(self, values: List[float]) -> float:\n",
        "        \"\"\"Calculate trend in performance metrics (positive = improving, negative = degrading)\"\"\"\n",
        "        if len(values) < 5:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            x = np.arange(len(values))\n",
        "            coeffs = np.polyfit(x, values, 1)\n",
        "            # Return negative slope because for MSE/MAE/MAPE, decreasing is better\n",
        "            return -coeffs[0]\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def connect_sensor_agents(self, sensor_agents: Dict):\n",
        "        \"\"\"Connect to sensor agents for drift confirmation\"\"\"\n",
        "        self.sensor_agents = sensor_agents\n",
        "        logger.info(f\"Connected to {len(sensor_agents)} sensor agents\")\n",
        "\n",
        "    def get_performance_plot_data(self) -> Dict[str, List]:\n",
        "        \"\"\"Get data for performance visualization\"\"\"\n",
        "        if not self.prediction_history:\n",
        "            return {}\n",
        "\n",
        "        recent_records = [p for p in self.prediction_history if p.get('forecast_success', False)]\n",
        "\n",
        "        return {\n",
        "            'timestamps': [r['timestamp'] for r in recent_records],\n",
        "            'predicted': [r['predicted_window'] for r in recent_records],\n",
        "            'mse_values': [r.get('mse', 0) for r in recent_records],\n",
        "            'mae_values': [r.get('mae', 0) for r in recent_records],\n",
        "            'mape_values': [r.get('mape', 0) for r in recent_records]\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save current performance state\"\"\"\n",
        "        state = {\n",
        "            'performance_stats': self.performance_stats.copy(),\n",
        "            'prediction_history': list(self.prediction_history)[-100:],\n",
        "            'mse_history': list(self.mse_history),\n",
        "            'mape_history': list(self.mape_history),\n",
        "            'mae_history': list(self.mae_history),\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Convert datetime objects to strings for JSON serialization\n",
        "            for record in state['prediction_history']:\n",
        "                if 'timestamp' in record:\n",
        "                    # Check if timestamp is already a string\n",
        "                    if hasattr(record['timestamp'], 'isoformat'):\n",
        "                        record['timestamp'] = record['timestamp'].isoformat()\n",
        "                # Convert numpy arrays to lists for JSON serialization\n",
        "                if 'feature_vector' in record and hasattr(record['feature_vector'], 'tolist'):\n",
        "                    record['feature_vector'] = record['feature_vector'].tolist()\n",
        "\n",
        "            # Handle last_retrain_time\n",
        "            if state['performance_stats']['last_retrain_time']:\n",
        "                if hasattr(state['performance_stats']['last_retrain_time'], 'isoformat'):\n",
        "                    state['performance_stats']['last_retrain_time'] = state['performance_stats']['last_retrain_time'].isoformat()\n",
        "\n",
        "            # Save to file\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, default=str, indent=2)\n",
        "\n",
        "            logger.info(f\"Performance state saved to {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save performance state: {e}\")\n",
        "            # Try simpler save without complex objects\n",
        "            simple_state = {\n",
        "                'total_predictions': len(self.prediction_history),\n",
        "                'avg_mse': np.mean(self.mse_history) if self.mse_history else 0.0,\n",
        "                'avg_mape': np.mean(self.mape_history) if self.mape_history else 0.0,\n",
        "                'avg_mae': np.mean(self.mae_history) if self.mae_history else 0.0,\n",
        "                'drift_events': self.performance_stats['drift_events'],\n",
        "                'retraining_events': self.performance_stats['retraining_events']\n",
        "            }\n",
        "\n",
        "            with open(filepath.replace('.json', '_simple.json'), 'w') as f:\n",
        "                json.dump(simple_state, f, indent=2)\n",
        "            print(f\"Saved simplified performance state to {filepath.replace('.json', '_simple.json')}\")\n",
        "\n",
        "\n",
        "# Test with YOUR actual data - Real-time VAR calculation only\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the agent with your actual model\n",
        "    agent = AdaptiveWindowAgent(\n",
        "        model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "    )\n",
        "\n",
        "    print(\"Loading your actual dataset...\")\n",
        "\n",
        "    # Load your actual saved dataset\n",
        "    Long_train = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "    print(f\"Loaded Long_train shape: {Long_train.shape}\")\n",
        "\n",
        "    # Take last 10 entries as test module for debugging\n",
        "    test_sequences = Long_train[-100:]\n",
        "    print(f\"Testing with last 10 sequences: {test_sequences.shape}\")\n",
        "\n",
        "    print(\"\\nStarting real-time VAR testing...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for i in range(len(test_sequences)):\n",
        "        sequence_3d = test_sequences[i]  # Shape: (50, 12)\n",
        "\n",
        "        # Simply flatten the sequence to get 600 features (50 * 12 = 600)\n",
        "        features = sequence_3d.flatten()  # This gives you exactly 600 features\n",
        "\n",
        "        print(f\"\\nSample {i+1}: Using flattened sequence of size {len(features)}\")\n",
        "\n",
        "        # Test the agent with real-time VAR calculation\n",
        "        result = agent.predict_window_size(features, sequence_3d=sequence_3d)\n",
        "\n",
        "        # Handle potential missing keys safely\n",
        "        mlp_pred = result.get('predicted_window', 0)\n",
        "        forecast_metrics = result.get('forecast_metrics', {})\n",
        "        error_msg = result.get('error', None)\n",
        "\n",
        "        if error_msg:\n",
        "            print(f\"Sample {i+1}: ERROR - {error_msg}\")\n",
        "            continue\n",
        "\n",
        "        # Print results\n",
        "        performance = agent.get_recent_performance()\n",
        "\n",
        "        if forecast_metrics.get('forecast_success', False):\n",
        "            mse_val = forecast_metrics['mse']\n",
        "            mae_val = forecast_metrics['mae']\n",
        "            mape_val = forecast_metrics['mape']\n",
        "\n",
        "            print(f\"Sample {i+1:3d}: MLP={mlp_pred:2d}, \"\n",
        "                  f\"MSE={mse_val:6.4f}, MAE={mae_val:6.4f}, MAPE={mape_val:6.4f}, \"\n",
        "                  f\"Avg_MSE={performance.get('recent_mse', 0):6.4f}\")\n",
        "        else:\n",
        "            print(f\"Sample {i+1:3d}: MLP={mlp_pred:2d}, VAR forecast failed\")\n",
        "\n",
        "        # Check for drift detection\n",
        "        if result.get('drift_detected', False):\n",
        "            print(f\"*** DRIFT DETECTED at sample {i+1} ***\")\n",
        "            drift_action = result.get('drift_action')\n",
        "            if drift_action:\n",
        "                print(f\"Drift action: {drift_action}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    final_performance = agent.get_recent_performance()\n",
        "\n",
        "    print(f\"Total predictions: {final_performance.get('total_predictions', 0)}\")\n",
        "    print(f\"Successful predictions: {final_performance.get('successful_predictions', 0)}\")\n",
        "    print(f\"Success rate: {final_performance.get('success_rate', 0):.2%}\")\n",
        "    print(f\"Average MSE: {final_performance.get('recent_mse', 0):.4f}\")\n",
        "    print(f\"Average MAE: {final_performance.get('recent_mae', 0):.4f}\")\n",
        "    print(f\"Average MAPE: {final_performance.get('recent_mape', 0):.4f}\")\n",
        "    print(f\"Drift events: {final_performance.get('drift_events', 0)}\")\n",
        "    print(f\"Retraining events: {agent.performance_stats['retraining_events']}\")\n",
        "    print(f\"Transformer fitted: {final_performance.get('transformer_fitted', False)}\")\n",
        "\n",
        "    # Show performance trends if enough data\n",
        "    if len(agent.mse_history) >= 20:\n",
        "        print(f\"MSE trend: {final_performance.get('mse_trend', 0):.4f} (positive = improving)\")\n",
        "        print(f\"MAPE trend: {final_performance.get('mape_trend', 0):.4f} (positive = improving)\")\n",
        "        print(f\"MAE trend: {final_performance.get('mae_trend', 0):.4f} (positive = improving)\")\n",
        "\n",
        "    # Save test results\n",
        "    agent.save_performance_state(\"real_data_test_results.json\")\n",
        "    print(f\"\\nTest results saved to: real_data_test_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyNU5fNZ6fyv0Rzv1GctL9Hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}