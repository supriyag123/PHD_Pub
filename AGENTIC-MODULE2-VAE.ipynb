{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "c6045407-4974-4563-f41d-d4426e92eb05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running optimized generation...\n",
            "============================================================\n",
            "STARTING OPTIMIZED VAE DATA GENERATION\n",
            "============================================================\n",
            "Loading existing VAE...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {}}.\n\nException encountered: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling_1', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling_1', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1253714357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;31m# - Keep full dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running optimized generation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m     results = generator.run_fast_generation(\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;34mr'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mtarget_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350000\u001b[0m  \u001b[0;31m# Full size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1253714357.py\u001b[0m in \u001b[0;36mrun_fast_generation\u001b[0;34m(self, train_data_path, target_samples)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading existing VAE...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    368\u001b[0m             )\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {}}.\n\nException encountered: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'factor': 1}, 'registered_name': 'MyLayers>Sampling', 'build_config': {'input_shape': [[None, 5], [None, 5]]}, 'name': 'sampling_1', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 5], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}"
          ]
        }
      ],
      "source": [
        "# VAE Data Generator Module - Your code with resumability added\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "from statsmodels.tsa.api import VAR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class VAEDataGenerator:\n",
        "    \"\"\"\n",
        "    One-time VAE training and synthetic data generation with resumability\n",
        "    \"\"\"\n",
        "\n",
        "    def register_custom_layers(self):\n",
        "        \"\"\"Register custom layers for model loading\"\"\"\n",
        "        # Clear and register custom layers\n",
        "        saving.get_custom_objects().clear()\n",
        "\n",
        "        @saving.register_keras_serializable(package=\"MyLayers\")\n",
        "        class Sampling(layers.Layer):\n",
        "            \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "            def __init__(self, factor):\n",
        "                super().__init__()\n",
        "                self.factor = factor\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "            def get_config(self):\n",
        "                return {\"factor\": self.factor}\n",
        "\n",
        "        return Sampling\n",
        "\n",
        "    def load_vae_models(self):\n",
        "        \"\"\"Load VAE models with proper custom layer registration\"\"\"\n",
        "        # Register custom layers first\n",
        "        Sampling = self.register_custom_layers()\n",
        "\n",
        "        encoder_path = f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras'\n",
        "        decoder_path = f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "        if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "            print(\"Loading VAE models with custom layers...\")\n",
        "            self.encoder = keras.models.load_model(encoder_path)\n",
        "            self.decoder = keras.models.load_model(decoder_path)\n",
        "            print(\"VAE models loaded successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"VAE models not found!\")\n",
        "            return False\n",
        "        self.output_dir = output_dir\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.vae = None\n",
        "\n",
        "        # Create checkpoints directory\n",
        "        self.checkpoint_dir = f\"{output_dir}checkpoints/\"\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    def train_vae_pipeline(self, train_data_path):\n",
        "        \"\"\"\n",
        "        Complete VAE training pipeline\n",
        "        \"\"\"\n",
        "        print(\"Starting VAE training pipeline...\")\n",
        "\n",
        "        # Load data\n",
        "        train_data = np.load(train_data_path)\n",
        "        n_seq = train_data.shape[0]\n",
        "        window_size = train_data.shape[1]\n",
        "        n_features = train_data.shape[2]\n",
        "\n",
        "        maxval = train_data.shape[0]\n",
        "        count_train = int(math.ceil(0.8*maxval))\n",
        "        x_train = train_data[:count_train]\n",
        "        x_test = train_data[count_train:]\n",
        "\n",
        "        # Clear all previously registered custom objects\n",
        "        saving.get_custom_objects().clear()\n",
        "\n",
        "        # Create a custom layer\n",
        "        @saving.register_keras_serializable(package=\"MyLayers\")\n",
        "        class Sampling(layers.Layer):\n",
        "            \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "            def __init__(self, factor):\n",
        "                super().__init__()\n",
        "                self.factor = factor\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "            def get_config(self):\n",
        "                return {\"factor\": self.factor}\n",
        "\n",
        "        # Build the encoder\n",
        "        latent_dim = 5\n",
        "        intermediate_dim = 256\n",
        "\n",
        "        # Encoder\n",
        "        encoder_inputs =  layers.Input(shape=(window_size, n_features),name=\"encoder_input\")\n",
        "        x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "        xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "        x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\" )(xx)\n",
        "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "        z = Sampling(1)([z_mean, z_log_var])\n",
        "        encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "        encoder.summary()\n",
        "\n",
        "        # Decoder\n",
        "        inp_z = Input(shape=(latent_dim,),name=\"decoder\")\n",
        "        x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "        x2= layers.Dense(int(intermediate_dim/2),  name=\"Dense2\")(x1)\n",
        "        x22= layers.LSTM(int(intermediate_dim/2),activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "        x3 = layers.LSTM(intermediate_dim,activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "        decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "        decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "        decoder.summary()\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "        # Parameters - your code\n",
        "        n_epochs = 150\n",
        "        klstart = 20\n",
        "        kl_annealtime = n_epochs-klstart\n",
        "        weight = K.variable(0.0)\n",
        "\n",
        "        # Define the VAE as a Model with a custom train_step\n",
        "        class VAE(keras.Model):\n",
        "            def __init__(self, encoder, decoder, **kwargs):\n",
        "                super(VAE, self).__init__(**kwargs)\n",
        "                self.encoder = encoder\n",
        "                self.decoder = decoder\n",
        "                self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "                self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "                    name=\"reconstruction_loss\"\n",
        "                )\n",
        "                self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "            @property\n",
        "            def metrics(self):\n",
        "                return [\n",
        "                    self.total_loss_tracker,\n",
        "                    self.reconstruction_loss_tracker,\n",
        "                    self.kl_loss_tracker,\n",
        "                ]\n",
        "\n",
        "            def train_step(self, data):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                        )\n",
        "\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                    total_loss = reconstruction_loss + (weight*kl_loss)\n",
        "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "                self.total_loss_tracker.update_state(total_loss)\n",
        "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "                self.kl_loss_tracker.update_state(kl_loss)\n",
        "                return {\n",
        "                    \"loss\": self.total_loss_tracker.result(),\n",
        "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                    \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                }\n",
        "\n",
        "            def test_step(self, data):\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                        )\n",
        "\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "                    total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "                    return {\n",
        "                        \"loss\": self.total_loss_tracker.result(),\n",
        "                        \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                        \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                          }\n",
        "\n",
        "        # CALLBACKS\n",
        "        es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "        class AnnealingCallback(Callback):\n",
        "            def __init__(self, weight):\n",
        "                super().__init__()\n",
        "                self.weight = weight\n",
        "            def on_epoch_end(self, epoch, logs={}):\n",
        "                if epoch > klstart and epoch < klstart*1.2:\n",
        "                    new_weight = min(K.get_value(self.weight) + (1./ kl_annealtime), 1.)\n",
        "                    K.set_value(self.weight, new_weight)\n",
        "                print (\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "        # Train the VAE\n",
        "        vae = VAE(encoder, decoder)\n",
        "        vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "        history=vae.fit( x_train,\n",
        "                         epochs=n_epochs,\n",
        "                         batch_size=32,\n",
        "                         validation_split=0.1,\n",
        "                         callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "        # Save models\n",
        "        encoder.save(f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras')\n",
        "        decoder.save(f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras')\n",
        "\n",
        "        # Store for data generation\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.window_size = window_size\n",
        "        self.n_features = n_features\n",
        "\n",
        "        print(\"VAE training complete and models saved!\")\n",
        "        return history\n",
        "\n",
        "    def generate_synthetic_dataset_fast(self, train_data_path, cohort_size=350000, checkpoint_every=5000):\n",
        "        \"\"\"\n",
        "        FASTER synthetic dataset generation with optimizations\n",
        "        \"\"\"\n",
        "        print(f\"Generating synthetic dataset FAST - {cohort_size} samples...\")\n",
        "\n",
        "        # Load models if needed with proper custom layer registration\n",
        "        if self.encoder is None:\n",
        "            if not self.load_vae_models():\n",
        "                raise ValueError(\"VAE models not found! Train VAE first.\")\n",
        "\n",
        "        # Load and process data\n",
        "        train_data = np.load(train_data_path)\n",
        "        window_size = train_data.shape[1]\n",
        "        n_features = train_data.shape[2]\n",
        "\n",
        "        # Keep original generator_multiply = 100 (as requested)\n",
        "        generator_multiply = 100\n",
        "        print(f\"Using generator_multiply = {generator_multiply}\")\n",
        "\n",
        "        # Process all base samples (no reduction)\n",
        "        max_base_samples = min(cohort_size // generator_multiply, train_data.shape[0])\n",
        "        if max_base_samples < train_data.shape[0]:\n",
        "            print(f\"Processing {max_base_samples} base samples to reach target {cohort_size}\")\n",
        "            train_data = train_data[:max_base_samples]\n",
        "\n",
        "        # Get encodings\n",
        "        print(\"Computing encodings...\")\n",
        "        X_train_encoded = self.encoder.predict(train_data, batch_size=64, verbose=1)  # Larger batch\n",
        "        mu, logvar, z = X_train_encoded\n",
        "        sigma = tf.exp(0.5 * logvar)\n",
        "\n",
        "        # OPTIMIZATION: Batch processing for decoder\n",
        "        print(\"Generating synthetic data in batches...\")\n",
        "        all_results = []\n",
        "        batch_size = 50  # Process 50 samples at once\n",
        "\n",
        "        for i in range(0, len(mu), batch_size):\n",
        "            batch_end = min(i + batch_size, len(mu))\n",
        "            batch_mu = mu[i:batch_end]\n",
        "            batch_sigma = sigma[i:batch_end]\n",
        "\n",
        "            # Generate all Z vectors for this batch\n",
        "            batch_z = []\n",
        "            for j in range(len(batch_mu)):\n",
        "                z_samples = tf.random.normal(\n",
        "                    shape=(generator_multiply, mu.shape[1]),\n",
        "                    mean=batch_mu[j],\n",
        "                    stddev=batch_sigma[j]\n",
        "                )\n",
        "                batch_z.append(z_samples)\n",
        "\n",
        "            # Decode all at once\n",
        "            all_z = tf.concat(batch_z, axis=0)\n",
        "            decoded_batch = self.decoder.predict(all_z, batch_size=128, verbose=0)  # Large batch\n",
        "            decoded_batch = decoded_batch.reshape((decoded_batch.shape[0], window_size * n_features))\n",
        "            all_results.append(decoded_batch)\n",
        "\n",
        "            if (i // batch_size + 1) % 10 == 0:\n",
        "                print(f\"Processed {i + batch_size}/{len(mu)} base samples...\")\n",
        "\n",
        "        results1 = np.concatenate(all_results, axis=0)\n",
        "        np.save(f'{self.output_dir}generated_large_subsquence_data.npy', results1)\n",
        "        print(f\"Fast synthetic dataset complete: {results1.shape}\")\n",
        "        return results1\n",
        "        \"\"\"\n",
        "        Generate synthetic dataset with resumability - MODIFIED FOR COLAB\n",
        "        \"\"\"\n",
        "        print(f\"Generating synthetic dataset of size {cohort_size} with checkpointing...\")\n",
        "\n",
        "        # Check for existing checkpoint\n",
        "        checkpoint_file = f\"{self.checkpoint_dir}generation_progress.npz\"\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            checkpoint_data = np.load(checkpoint_file, allow_pickle=True)\n",
        "            start_idx = int(checkpoint_data['last_completed_idx'])\n",
        "            existing_files = checkpoint_data['completed_files'].tolist()\n",
        "            print(f\"Resuming from checkpoint: {start_idx}/{cohort_size}\")\n",
        "        else:\n",
        "            start_idx = 0\n",
        "            existing_files = []\n",
        "            print(\"Starting fresh generation...\")\n",
        "\n",
        "        # Load encoder/decoder if not already loaded\n",
        "        if self.encoder is None:\n",
        "            encoder_path = f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras'\n",
        "            decoder_path = f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "            if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "                self.encoder = keras.models.load_model(encoder_path)\n",
        "                self.decoder = keras.models.load_model(decoder_path)\n",
        "                print(\"Loaded existing VAE models\")\n",
        "            else:\n",
        "                raise ValueError(\"VAE models not found! Train VAE first.\")\n",
        "\n",
        "        # Load original training data\n",
        "        train_data = np.load(train_data_path)\n",
        "        window_size = train_data.shape[1]\n",
        "        n_features = train_data.shape[2]\n",
        "        generator_multiply = 100\n",
        "\n",
        "        # Get encodings once (save to avoid recomputation)\n",
        "        encodings_file = f\"{self.checkpoint_dir}encodings.npz\"\n",
        "        if start_idx == 0 and not os.path.exists(encodings_file):\n",
        "            print(\"Computing encodings...\")\n",
        "            X_train_encoded = self.encoder.predict(train_data, verbose=1)\n",
        "            mu, logvar, z = X_train_encoded\n",
        "\n",
        "            # Save encodings to avoid recomputation\n",
        "            np.savez(encodings_file, mu=mu, logvar=logvar)\n",
        "            print(\"Encodings saved to checkpoint\")\n",
        "        else:\n",
        "            print(\"Loading encodings from checkpoint...\")\n",
        "            encodings_data = np.load(encodings_file)\n",
        "            mu = encodings_data['mu']\n",
        "            logvar = encodings_data['logvar']\n",
        "\n",
        "        sigma = tf.exp(0.5 * logvar)\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # Generate data with checkpointing - SAVE CHUNKS TO SEPARATE FILES\n",
        "        target_samples = min(cohort_size, batch.numpy())\n",
        "        chunk_size = checkpoint_every  # Save every 5000 samples\n",
        "\n",
        "        for chunk_start in range(start_idx, target_samples, chunk_size):\n",
        "            chunk_end = min(chunk_start + chunk_size, target_samples)\n",
        "            chunk_file = f\"{self.output_dir}synthetic_chunk_{chunk_start}_{chunk_end}.npy\"\n",
        "\n",
        "            # Skip if chunk already exists\n",
        "            if chunk_file in existing_files:\n",
        "                print(f\"Chunk {chunk_start}-{chunk_end} already exists, skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Generating chunk {chunk_start}-{chunk_end}...\")\n",
        "            chunk_data = []\n",
        "\n",
        "            for i in range(chunk_start, chunk_end):\n",
        "                try:\n",
        "                    all_Z_i = tf.random.normal(\n",
        "                        shape=(generator_multiply, dim),\n",
        "                        mean=mu[i, :],\n",
        "                        stddev=sigma[i, :]\n",
        "                    )\n",
        "                    X_train_decoded = self.decoder.predict(all_Z_i, verbose=0)\n",
        "                    X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0], window_size * n_features))\n",
        "                    chunk_data.append(X_train_decoded)\n",
        "\n",
        "                    # Progress update\n",
        "                    if (i - chunk_start + 1) % 100 == 0:\n",
        "                        print(f\"  Progress: {i - chunk_start + 1}/{chunk_end - chunk_start}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error at sample {i}: {e}\")\n",
        "                    # Save emergency checkpoint\n",
        "                    emergency_checkpoint = {\n",
        "                        'last_completed_idx': i,\n",
        "                        'completed_files': existing_files,\n",
        "                        'error_at': i\n",
        "                    }\n",
        "                    np.savez(f\"{checkpoint_file}_emergency\", **emergency_checkpoint)\n",
        "                    raise\n",
        "\n",
        "            # Save chunk\n",
        "            chunk_array = np.concatenate(chunk_data, axis=0)\n",
        "            np.save(chunk_file, chunk_array)\n",
        "            existing_files.append(chunk_file)\n",
        "\n",
        "            # Update checkpoint\n",
        "            checkpoint = {\n",
        "                'last_completed_idx': chunk_end,\n",
        "                'completed_files': np.array(existing_files),\n",
        "                'total_target': target_samples\n",
        "            }\n",
        "            np.savez(checkpoint_file, **checkpoint)\n",
        "            print(f\"Chunk {chunk_start}-{chunk_end} saved. Progress: {chunk_end}/{target_samples}\")\n",
        "\n",
        "        # Combine all chunks into final file\n",
        "        print(\"Combining all chunks...\")\n",
        "        all_data = []\n",
        "        for chunk_file in existing_files:\n",
        "            chunk_data = np.load(chunk_file)\n",
        "            all_data.append(chunk_data)\n",
        "\n",
        "        results1 = np.concatenate(all_data, axis=0)\n",
        "\n",
        "        # Save final result\n",
        "        np.save(f'{self.output_dir}generated_large_subsquence_data.npy', results1)\n",
        "\n",
        "        # Clean up\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "        if os.path.exists(encodings_file):\n",
        "            os.remove(encodings_file)\n",
        "        print(f\"Synthetic dataset complete: {results1.shape}\")\n",
        "        return results1\n",
        "\n",
        "    def compute_var_windows_fast(self, data_path, start_idx=0, batch_size=5000):\n",
        "        \"\"\"\n",
        "        FASTER VAR computation with optimizations\n",
        "        \"\"\"\n",
        "        print(f\"Computing VAR windows FAST for batch starting at {start_idx}...\")\n",
        "\n",
        "        # OPTIMIZATION: Extended window range 2->30 (as requested)\n",
        "        max_var_lag = 30\n",
        "        print(f\"Testing VAR lags from 2 to {max_var_lag}\")\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        window_size = self.window_size if self.window_size else 50\n",
        "        n_features = self.n_features if self.n_features else 13\n",
        "\n",
        "        x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "        n_future = 1\n",
        "\n",
        "        end_idx = min(start_idx + batch_size, x_3d.shape[0])\n",
        "        best_window_for_long_seq = []\n",
        "\n",
        "        # OPTIMIZATION: Batch processing\n",
        "        for i in range(start_idx, end_idx):\n",
        "            rmse_list = []\n",
        "\n",
        "            # Test lags from 2 to 30 (as requested)\n",
        "            for k in range(2, min(max_var_lag + 1, window_size//2)):\n",
        "                cur_seq = x_3d[i,:,:]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "\n",
        "                try:\n",
        "                    model = VAR(df_train)\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            best_window_for_long_seq.append(min_sw)\n",
        "\n",
        "            if (i - start_idx) % 500 == 0:\n",
        "                print(f'FAST processed {i - start_idx}/{end_idx - start_idx} sequences...')\n",
        "\n",
        "        Window = np.array(best_window_for_long_seq)\n",
        "        batch_file = f'{self.output_dir}generated-data-true-window-BATCH_{start_idx}_{end_idx}.npy'\n",
        "        np.save(batch_file, Window)\n",
        "        print(f\"FAST VAR windows computed for batch {start_idx}-{end_idx}\")\n",
        "        return Window\n",
        "        \"\"\"\n",
        "        Compute VAR windows with resumability - SMALLER BATCHES FOR COLAB\n",
        "        \"\"\"\n",
        "        print(f\"Computing VAR windows for batch starting at {start_idx}...\")\n",
        "\n",
        "        # Load data\n",
        "        x = np.load(data_path)\n",
        "        window_size = self.window_size if self.window_size else 50\n",
        "        n_features = self.n_features if self.n_features else 13  # Updated for your 13 features\n",
        "\n",
        "        x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "        n_future = 1\n",
        "        K = window_size\n",
        "\n",
        "        end_idx = min(start_idx + batch_size, x_3d.shape[0])\n",
        "\n",
        "        # Check if this batch was already completed\n",
        "        batch_file = f'{self.output_dir}generated-data-true-window-BATCH_{start_idx}_{end_idx}.npy'\n",
        "        if os.path.exists(batch_file):\n",
        "            print(f\"Batch {start_idx}-{end_idx} already completed, loading...\")\n",
        "            return np.load(batch_file)\n",
        "\n",
        "        best_window_for_long_seq = []\n",
        "\n",
        "        # Process with mini-checkpoints\n",
        "        mini_checkpoint_every = 1000\n",
        "        checkpoint_file = f\"{self.checkpoint_dir}var_batch_{start_idx}_{end_idx}_progress.npy\"\n",
        "\n",
        "        # Check for existing progress\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            progress_data = np.load(checkpoint_file, allow_pickle=True).item()\n",
        "            completed_idx = progress_data['completed_idx']\n",
        "            best_window_for_long_seq = progress_data['results']\n",
        "            print(f\"Resuming batch from index {completed_idx}\")\n",
        "        else:\n",
        "            completed_idx = start_idx\n",
        "\n",
        "        # Process remaining samples\n",
        "        for i in range(completed_idx, end_idx):\n",
        "            rmse_list = []\n",
        "            for k in range(2, round(K)):\n",
        "                cur_seq = x_3d[i,:,:]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "                model= VAR(df_train)\n",
        "                try:\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse =  mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            best_window_for_long_seq.append(min_sw)\n",
        "\n",
        "            # Mini-checkpoint\n",
        "            if (i - start_idx) % mini_checkpoint_every == 0:\n",
        "                progress = {\n",
        "                    'completed_idx': i + 1,\n",
        "                    'results': best_window_for_long_seq,\n",
        "                    'start_idx': start_idx,\n",
        "                    'end_idx': end_idx\n",
        "                }\n",
        "                np.save(checkpoint_file, progress)\n",
        "                print(f'Mini-checkpoint saved at {i + 1}/{end_idx}')\n",
        "\n",
        "            if (i - start_idx) % 1000 == 0:\n",
        "                print(f'Processed {i - start_idx}/{end_idx - start_idx} sequences...')\n",
        "\n",
        "        Window = np.array(best_window_for_long_seq)\n",
        "\n",
        "        # Save final batch results\n",
        "        np.save(batch_file, Window)\n",
        "\n",
        "        # Clean up checkpoint\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "\n",
        "        print(f\"VAR windows computed and saved for batch {start_idx}-{end_idx}\")\n",
        "        return Window\n",
        "\n",
        "    def run_complete_data_generation_resumable(self, train_data_path):\n",
        "        \"\"\"\n",
        "        Complete data generation pipeline with resumability\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"STARTING RESUMABLE VAE DATA GENERATION PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Train VAE (only if models don't exist)\n",
        "        encoder_path = f'{self.output_dir}METROPM_vae-encoder-latent5-dim256.keras'\n",
        "        if not os.path.exists(encoder_path):\n",
        "            print(\"Training VAE...\")\n",
        "            history = self.train_vae_pipeline(train_data_path)\n",
        "        else:\n",
        "            print(\"VAE models already exist, skipping training...\")\n",
        "            self.encoder = keras.models.load_model(encoder_path)\n",
        "            self.decoder = keras.models.load_model(f'{self.output_dir}METROPM_vae-decoder-latent5-dim256.keras')\n",
        "            # Get data properties\n",
        "            train_data = np.load(train_data_path)\n",
        "            self.window_size = train_data.shape[1]\n",
        "            self.n_features = train_data.shape[2]\n",
        "\n",
        "        # Step 2: Generate synthetic data (resumable)\n",
        "        synthetic_file = f'{self.output_dir}generated_large_subsquence_data.npy'\n",
        "        if not os.path.exists(synthetic_file):\n",
        "            print(\"Generating synthetic data...\")\n",
        "            synthetic_data = self.generate_synthetic_dataset_resumable(train_data_path, cohort_size=350000)\n",
        "        else:\n",
        "            print(\"Synthetic data already exists, loading...\")\n",
        "            synthetic_data = np.load(synthetic_file)\n",
        "\n",
        "        # Step 3: Compute VAR windows in batches (resumable)\n",
        "        final_windows_file = f'{self.output_dir}generated-data-true-window.npy'\n",
        "        if not os.path.exists(final_windows_file):\n",
        "            print(\"Computing VAR windows...\")\n",
        "            batch_size = 10000  # Smaller batches for Colab\n",
        "            total_samples = synthetic_data.shape[0]\n",
        "            all_windows = []\n",
        "\n",
        "            for start_idx in range(0, total_samples, batch_size):\n",
        "                batch_windows = self.compute_var_windows_batch_resumable(\n",
        "                    synthetic_file,\n",
        "                    start_idx,\n",
        "                    batch_size\n",
        "                )\n",
        "                all_windows.append(batch_windows)\n",
        "\n",
        "            # Combine all batches\n",
        "            final_windows = np.concatenate(all_windows, axis=0)\n",
        "\n",
        "            # Save final results\n",
        "            np.save(final_windows_file, final_windows)\n",
        "            np.save(f'{self.output_dir}generated-data.npy', synthetic_data)\n",
        "        else:\n",
        "            print(\"VAR windows already computed, loading...\")\n",
        "            final_windows = np.load(final_windows_file)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"VAE DATA GENERATION COMPLETE!\")\n",
        "        print(f\"Generated {synthetic_data.shape[0]} synthetic samples\")\n",
        "        print(f\"Computed {final_windows.shape[0]} VAR windows\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def run_fast_generation(self, train_data_path, target_samples=350000):\n",
        "        \"\"\"\n",
        "        Optimized pipeline with your requested changes\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"STARTING OPTIMIZED VAE DATA GENERATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Load existing VAE or train\n",
        "        if not self.load_vae_models():\n",
        "            print(\"Training VAE...\")\n",
        "            self.train_vae_pipeline(train_data_path)\n",
        "        else:\n",
        "            print(\"VAE models loaded successfully!\")\n",
        "            # Get data properties\n",
        "            train_data = np.load(train_data_path)\n",
        "            self.window_size = train_data.shape[1]\n",
        "            self.n_features = train_data.shape[2]\n",
        "\n",
        "        # Step 2: Generate synthetic dataset (full size, generator_multiply=100)\n",
        "        print(f\"Generating {target_samples} synthetic samples...\")\n",
        "        synthetic_data = self.generate_synthetic_dataset_fast(train_data_path, cohort_size=target_samples)\n",
        "\n",
        "        # Step 3: Compute VAR windows (testing lags 2->30)\n",
        "        print(\"Computing VAR windows (testing lags 2-30)...\")\n",
        "        batch_size = 5000\n",
        "        total_samples = synthetic_data.shape[0]\n",
        "        all_windows = []\n",
        "\n",
        "        for start_idx in range(0, total_samples, batch_size):\n",
        "            batch_windows = self.compute_var_windows_fast(\n",
        "                f'{self.output_dir}generated_large_subsquence_data.npy',\n",
        "                start_idx,\n",
        "                batch_size\n",
        "            )\n",
        "            all_windows.append(batch_windows)\n",
        "\n",
        "        # Combine and save\n",
        "        final_windows = np.concatenate(all_windows, axis=0)\n",
        "        np.save(f'{self.output_dir}generated-data-true-window-OPTIMIZED.npy', final_windows)\n",
        "        np.save(f'{self.output_dir}generated-data-OPTIMIZED.npy', synthetic_data)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"OPTIMIZED VAE DATA GENERATION COMPLETE!\")\n",
        "        print(f\"Generated {synthetic_data.shape[0]} synthetic samples\")\n",
        "        print(f\"Computed {final_windows.shape[0]} VAR windows\")\n",
        "        print(f\"VAR lag range: 2-30\")\n",
        "        print(f\"Generator multiply: 100 (kept original)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'synthetic_data': synthetic_data,\n",
        "            'var_windows': final_windows\n",
        "        }\n",
        "\n",
        "# Usage - OPTIMIZED VERSION with your specifications\n",
        "if __name__ == \"__main__\":\n",
        "    generator = VAEDataGenerator()\n",
        "\n",
        "    # Optimized generation with your requested changes:\n",
        "    # - Keep generator_multiply = 100 (no reduction)\n",
        "    # - VAR lag range 2->30 (extended from 15)\n",
        "    # - Keep batch processing optimization\n",
        "    # - Keep full dataset size\n",
        "    print(\"Running optimized generation...\")\n",
        "    results = generator.run_fast_generation(\n",
        "        r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-AUTO.npy',\n",
        "        target_samples=350000  # Full size\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb",
      "authorship_tag": "ABX9TyPH0KUheN2wOWLvC7zjBZAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}