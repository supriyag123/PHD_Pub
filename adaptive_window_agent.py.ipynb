{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/adaptive_window_agent.py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agents/adaptive_window_agent.py\n",
        "\n",
        "%%writefile adaptive_window_agent.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.regularizers import l1_l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"Agent that uses Enhanced Feature Engineering MLP to predict optimal window size\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./mlp_checkpoints/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.y_scaler = StandardScaler()\n",
        "        self.x_scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.selector = None\n",
        "\n",
        "        # Create directory structure\n",
        "        self.enhanced_dir = f\"{output_dir}enhanced_mlp/\"\n",
        "        self.checkpoint_dir = f\"{self.enhanced_dir}checkpoints/\"\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Model parameters\n",
        "        self.min_window = 5\n",
        "        self.max_window = 50\n",
        "        self.default_window = 20\n",
        "        self.is_trained = False\n",
        "\n",
        "        # Checkpoint files\n",
        "        self.checkpoint_files = {\n",
        "            'model': f\"{self.enhanced_dir}enhanced_feature_model.keras\",\n",
        "            'scalers': f\"{self.checkpoint_dir}scalers.pkl\",\n",
        "            'selector': f\"{self.checkpoint_dir}selector.pkl\",\n",
        "            'training_state': f\"{self.checkpoint_dir}training_state.json\"\n",
        "        }\n",
        "\n",
        "        # Try to load existing model\n",
        "        self.load_trained_model()\n",
        "\n",
        "    def load_trained_model(self):\n",
        "        \"\"\"Load pre-trained model and scalers if available\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.checkpoint_files['model']):\n",
        "                self.model = load_model(self.checkpoint_files['model'])\n",
        "\n",
        "                if os.path.exists(self.checkpoint_files['scalers']):\n",
        "                    with open(self.checkpoint_files['scalers'], 'rb') as f:\n",
        "                        scalers = pickle.load(f)\n",
        "                    self.x_scaler = scalers['x_scaler']\n",
        "                    self.y_scaler = scalers['y_scaler']\n",
        "\n",
        "                if os.path.exists(self.checkpoint_files['selector']):\n",
        "                    with open(self.checkpoint_files['selector'], 'rb') as f:\n",
        "                        self.selector = pickle.load(f)\n",
        "\n",
        "                self.is_trained = True\n",
        "                print(\"✅ Loaded pre-trained Enhanced MLP model for window prediction\")\n",
        "            else:\n",
        "                print(\"📝 No pre-trained model found, will use lightweight fallback\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load pre-trained model: {e}\")\n",
        "            self.is_trained = False\n",
        "\n",
        "    def create_enhanced_features(self, data_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Create enhanced features from time series data matrix\"\"\"\n",
        "        if data_matrix.shape[0] < 5:\n",
        "            return data_matrix\n",
        "\n",
        "        try:\n",
        "            enhanced_features = []\n",
        "\n",
        "            # 1. Original features\n",
        "            enhanced_features.append(data_matrix)\n",
        "\n",
        "            # 2. Best interaction (Feature 0 × Feature 1 for simplicity)\n",
        "            if data_matrix.shape[1] > 1:\n",
        "                best_interaction = data_matrix[:, 0] * data_matrix[:, 1]\n",
        "                enhanced_features.append(best_interaction.reshape(-1, 1))\n",
        "\n",
        "            # 3. Top feature interactions (limited for real-time)\n",
        "            interaction_features = []\n",
        "            top_features = min(5, data_matrix.shape[1])\n",
        "\n",
        "            for i in range(top_features):\n",
        "                for j in range(i+1, top_features):\n",
        "                    if len(interaction_features) < 10:  # Limit for performance\n",
        "                        # Multiplication\n",
        "                        mult_feat = data_matrix[:, i] * data_matrix[:, j]\n",
        "                        interaction_features.append(mult_feat)\n",
        "\n",
        "                        # Safe division\n",
        "                        if np.all(np.abs(data_matrix[:, j]) > 1e-8):\n",
        "                            div_feat = data_matrix[:, i] / (data_matrix[:, j] + 1e-8)\n",
        "                            interaction_features.append(div_feat)\n",
        "\n",
        "            if interaction_features:\n",
        "                interaction_matrix = np.column_stack(interaction_features)\n",
        "                enhanced_features.append(interaction_matrix)\n",
        "\n",
        "            # 4. Polynomial features (limited)\n",
        "            poly_features = []\n",
        "            for i in range(min(3, data_matrix.shape[1])):\n",
        "                # Quadratic terms\n",
        "                quad_feat = data_matrix[:, i] ** 2\n",
        "                poly_features.append(quad_feat)\n",
        "\n",
        "            if poly_features:\n",
        "                poly_matrix = np.column_stack(poly_features)\n",
        "                enhanced_features.append(poly_matrix)\n",
        "\n",
        "            # 5. Statistical features\n",
        "            stat_features = []\n",
        "            if data_matrix.shape[1] >= 3:\n",
        "                mean_feat = np.mean(data_matrix, axis=1)\n",
        "                std_feat = np.std(data_matrix, axis=1)\n",
        "                stat_features.extend([mean_feat, std_feat])\n",
        "\n",
        "            if stat_features:\n",
        "                stat_matrix = np.column_stack(stat_features)\n",
        "                enhanced_features.append(stat_matrix)\n",
        "\n",
        "            # Combine all features\n",
        "            X_enhanced = np.hstack(enhanced_features)\n",
        "            return X_enhanced\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Feature enhancement failed: {e}, using original features\")\n",
        "            return data_matrix\n",
        "\n",
        "    def extract_features_from_df(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Extract features from DataFrame for window prediction\"\"\"\n",
        "        if len(df) < 5:\n",
        "            # Return minimal features for new streams\n",
        "            return np.array([[0.1, 0.1, 0.1, 0.1, 0.1]])\n",
        "\n",
        "        try:\n",
        "            # Get numeric columns (exclude timestamp)\n",
        "            numeric_cols = [col for col in df.columns if col != 'timestamp']\n",
        "            if not numeric_cols:\n",
        "                return np.array([[0.1, 0.1, 0.1, 0.1, 0.1]])\n",
        "\n",
        "            # Take recent data\n",
        "            recent_data = df[numeric_cols].tail(min(20, len(df))).dropna()\n",
        "            if len(recent_data) < 3:\n",
        "                return np.array([[0.1, 0.1, 0.1, 0.1, 0.1]])\n",
        "\n",
        "            # Convert to matrix and create enhanced features\n",
        "            data_matrix = recent_data.values\n",
        "            enhanced_features = self.create_enhanced_features(data_matrix)\n",
        "\n",
        "            # Take the last row as current features\n",
        "            current_features = enhanced_features[-1:, :]\n",
        "\n",
        "            return current_features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Feature extraction failed: {e}\")\n",
        "            return np.array([[0.1, 0.1, 0.1, 0.1, 0.1]])\n",
        "\n",
        "    def build_lightweight_mlp(self, input_dim: int):\n",
        "        \"\"\"Build lightweight MLP for real-time inference\"\"\"\n",
        "        model = Sequential([\n",
        "            Dense(512, input_dim=input_dim, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(256, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(loss='mae', optimizer=optimizer, metrics=['mse', 'mae'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_fallback_model(self, df: pd.DataFrame):\n",
        "        \"\"\"Train a quick fallback model if no pre-trained model available\"\"\"\n",
        "        if self.is_trained or len(df) < 20:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(\"🚀 Training lightweight window prediction model...\")\n",
        "\n",
        "            # Generate synthetic training data based on current stream characteristics\n",
        "            features_sample = self.extract_features_from_df(df)\n",
        "            input_dim = features_sample.shape[1]\n",
        "\n",
        "            # Create synthetic dataset\n",
        "            X_train = []\n",
        "            y_train = []\n",
        "\n",
        "            for _ in range(500):  # Smaller dataset for speed\n",
        "                # Generate synthetic features similar to current data\n",
        "                base_features = np.random.normal(0, 1, input_dim)\n",
        "                noise = np.random.normal(0, 0.1, input_dim)\n",
        "                synthetic_features = base_features + noise\n",
        "\n",
        "                # Heuristic window size based on feature characteristics\n",
        "                volatility = np.std(synthetic_features)\n",
        "                mean_abs = np.mean(np.abs(synthetic_features))\n",
        "\n",
        "                optimal_window = max(self.min_window,\n",
        "                                   min(self.max_window,\n",
        "                                       int(self.min_window + volatility * 5 + mean_abs * 3)))\n",
        "\n",
        "                X_train.append(synthetic_features)\n",
        "                y_train.append(optimal_window)\n",
        "\n",
        "            X_train = np.array(X_train)\n",
        "            y_train = np.array(y_train)\n",
        "\n",
        "            # Split and scale\n",
        "            X_train_scaled = self.x_scaler.fit_transform(X_train)\n",
        "            y_train_scaled = self.y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "            # Build and train model\n",
        "            self.model = self.build_lightweight_mlp(input_dim)\n",
        "\n",
        "            self.model.fit(\n",
        "                X_train_scaled, y_train_scaled,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                verbose=0,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "            )\n",
        "\n",
        "            # Save the model and scalers\n",
        "            self.model.save(self.checkpoint_files['model'])\n",
        "            with open(self.checkpoint_files['scalers'], 'wb') as f:\n",
        "                pickle.dump({'x_scaler': self.x_scaler, 'y_scaler': self.y_scaler}, f)\n",
        "\n",
        "            self.is_trained = True\n",
        "            print(\"✅ Lightweight model trained and saved\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Fallback model training failed: {e}\")\n",
        "\n",
        "    def predict_window(self, df: pd.DataFrame) -> int:\n",
        "        \"\"\"Predict optimal window size using Enhanced MLP or fallback\"\"\"\n",
        "        try:\n",
        "            # Train fallback model if no model is available\n",
        "            if not self.is_trained:\n",
        "                self.train_fallback_model(df)\n",
        "\n",
        "            # If still no model, return default\n",
        "            if not self.is_trained or self.model is None:\n",
        "                return self.default_window\n",
        "\n",
        "            # Extract and enhance features\n",
        "            features = self.extract_features_from_df(df)\n",
        "\n",
        "            # Apply feature selection if available\n",
        "            if self.selector is not None:\n",
        "                try:\n",
        "                    features = self.selector.transform(features)\n",
        "                except:\n",
        "                    pass  # Continue without selection if it fails\n",
        "\n",
        "            # Scale features\n",
        "            features_scaled = self.x_scaler.transform(features)\n",
        "\n",
        "            # Predict\n",
        "            prediction_scaled = self.model.predict(features_scaled, verbose=0)\n",
        "            prediction = self.y_scaler.inverse_transform(prediction_scaled.reshape(-1, 1))[0, 0]\n",
        "\n",
        "            # Clamp to valid range\n",
        "            window_size = int(max(self.min_window, min(self.max_window, prediction)))\n",
        "\n",
        "            return window_size\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Window prediction failed: {e}, using default\")\n",
        "            return self.default_window\n",
        "\n",
        "    def get_model_info(self) -> dict:\n",
        "        \"\"\"Get information about the current model\"\"\"\n",
        "        return {\n",
        "            'is_trained': self.is_trained,\n",
        "            'model_available': self.model is not None,\n",
        "            'has_selector': self.selector is not None,\n",
        "            'model_path': self.checkpoint_files['model'],\n",
        "            'model_exists': os.path.exists(self.checkpoint_files['model'])\n",
        "        }"
      ],
      "metadata": {
        "id": "oOHjAhnZs1Az",
        "outputId": "391c004e-f82d-48e7-f094-e8ff2bb55765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing adaptive_window_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-RMQkgLak9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zEUESMBM_Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to script iot_demo.ipynb\n",
        "!mv streamlit_app.py app.py"
      ],
      "metadata": {
        "id": "vWk8VjiGqwl0",
        "outputId": "39211f85-6b68-4b33-cd5b-5dbe99749392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook iot_demo.ipynb to script\n",
            "[NbConvertApp] ERROR | Notebook JSON is invalid: Additional properties are not allowed ('errorDetails' was unexpected)\n",
            "\n",
            "Failed validating 'additionalProperties' in error:\n",
            "\n",
            "On instance['cells'][0]['outputs'][0]:\n",
            "{'ename': 'ModuleNotFoundError',\n",
            " 'errorDetails': {'actions': [{'action': 'open_url',\n",
            "                               'actionText': 'Open Examples',\n",
            "                               'url': '/notebooks/snippets/importing_libraries.ipynb'}]},\n",
            " 'evalue': \"No module named 'streamlit'\",\n",
            " 'output_type': 'error',\n",
            " 'traceback': ['\\x1b[0;31m---------------------------------------------------------...',\n",
            "               '\\x1b[0;31mModuleNotFoundError\\x1b[0m                       '\n",
            "               'Traceback (...',\n",
            "               '\\x1b[0;32m/tmp/ipython-input-1877136291.py\\x1b[0m in '\n",
            "               '\\x1b[0;36m<cell line...',\n",
            "               '\\x1b[0;31mModuleNotFoundError\\x1b[0m: No module named '\n",
            "               \"'streamlit'\",\n",
            "               '',\n",
            "               '\\x1b[0;31m---------------------------------------------------------...']}\n",
            "[NbConvertApp] Writing 2424 bytes to iot_demo.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6ZgIvrdRNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/iot_demo.ipynb",
      "authorship_tag": "ABX9TyOR0oSA1kKKwfczDsSVoTMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}