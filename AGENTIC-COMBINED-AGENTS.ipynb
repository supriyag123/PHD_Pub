{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-COMBINED-AGENTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HoP7OuWNxlsJ"
      },
      "outputs": [],
      "source": [
        "# ====== Imports ======\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, json, os, logging, warnings\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras, tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ====== RobustSensorAgent & RobustMasterAgent ======\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 50,\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1):\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {'mean': 0.0, 'std': 1.0, 'q95': 0.0, 'q99': 0.0}\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Metadata sidecar file\n",
        "                metadata_path = model_path.replace('.h5', '_metadata.pkl')\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "                   # self.scaler = metadata.get('scaler', StandardScaler())\n",
        "                    self.rolling_stats = metadata.get('rolling_stats', self.rolling_stats)\n",
        "                    if 'error_history' in metadata:\n",
        "                        self.baseline_errors = np.array(metadata['error_history'])\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format â€“ expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"âœ… AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3. Update rolling stats periodically\n",
        "        if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "            self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model.\"\"\"\n",
        "        try:\n",
        "            #data_scaled = self.scaler.transform(subsequence.reshape(-1, 1))\n",
        "            X = subsequence.reshape(1, self.window_length, 1)  # [batch, timesteps, features]\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "            error = mean_squared_error(subsequence.flatten(), reconstruction.flatten())\n",
        "            return max(0.0, error)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            return np.var(subsequence)\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "        self.rolling_stats['mean'] = np.mean(errors_array)\n",
        "        self.rolling_stats['std'] = np.std(errors_array) + 1e-8\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        return score > threshold\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            return js_divergence > self.drift_threshold\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        if self.rolling_stats['std'] == 0: return 0.5\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        distance_from_threshold = abs(score - threshold) / self.rolling_stats['std']\n",
        "        return min(1.0, distance_from_threshold / 3.0)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SENSOR SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"ðŸš€ Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=50,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1)\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"âœ… Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "    return sensor_agents, master\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "\n",
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Agent A: Adaptive Window Management with Enhanced MLP\n",
        "\n",
        "    Capabilities:\n",
        "    1. Predict window size using trained MLP\n",
        "    2. Calculate actual performance using VAR forecast\n",
        "    3. Track accuracy and performance statistics\n",
        "    4. Monitor for anomaly vs drift in prediction performance\n",
        "    5. Retrain MLP when drift is confirmed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str = \"adaptive_window_agent\",\n",
        "                 model_path: str = None,\n",
        "                 checkpoint_path: str = None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "        self.checkpoint_path = checkpoint_path or \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/ckp2.weights.h5\"\n",
        "\n",
        "        # Core MLP\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Performance tracking\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # Event detection parameters\n",
        "        self.drift_detection_window = 20\n",
        "        self.drift_threshold_mse = 0.2\n",
        "        self.drift_threshold_mae = 0.2\n",
        "        self.consecutive_poor_predictions = 0\n",
        "        self.cooldown_counter = 0\n",
        "\n",
        "        # Stats\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        # Buffers for retraining\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "        print(f\"Model loaded: {self.is_model_loaded}\")\n",
        "        print(f\"Transformer fitted: {self.transformer_fitted}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load trained MLP model and recreate transformer\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                    print(\"Loaded saved transformer\")\n",
        "                else:\n",
        "                    y_original = np.load('/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy')\n",
        "                    self.transformer = StandardScaler()\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(f\"Fitted transformer on {len(y_original)} samples and saved\")\n",
        "            else:\n",
        "                print(f\"Model file not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "\n",
        "    def evaluate_forecast_performance(self, sequence_3d: np.ndarray, predicted_window: int, n_future: int = 1) -> Dict[str, float]:\n",
        "        \"\"\"Use predicted window to forecast with VAR and calculate MSE/MAE\"\"\"\n",
        "        try:\n",
        "            df = pd.DataFrame(sequence_3d, columns=[f'V{i+1}' for i in range(sequence_3d.shape[1])])\n",
        "            df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "\n",
        "            # Drop constant columns\n",
        "            constant_columns = [col for col in df_train.columns if df_train[col].nunique() <= 1 or df_train[col].var() < 1e-12]\n",
        "            df_train = df_train.drop(columns=constant_columns, errors=\"ignore\")\n",
        "            df_test = df_test.drop(columns=constant_columns, errors=\"ignore\")\n",
        "\n",
        "            if len(df_train.columns) < 2:\n",
        "                return {'mse': 99999, 'mae': 99999, 'forecast_success': False}\n",
        "\n",
        "            k = min(predicted_window, len(df_train) - 2)\n",
        "            if k < 1: k = 1\n",
        "\n",
        "            model = VAR(df_train)\n",
        "            model_fitted = None\n",
        "            for trend in ['n', 'c', 'ct', 'ctt']:\n",
        "                try:\n",
        "                    model_fitted = model.fit(maxlags=k, trend=trend)\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "            if model_fitted is None:\n",
        "                return {'mse': 99999, 'mae': 99999, 'forecast_success': False}\n",
        "\n",
        "            forecast_input = df_train.values[-model_fitted.k_ar:]\n",
        "            fc = model_fitted.forecast(y=forecast_input, steps=n_future)\n",
        "            df_forecast = pd.DataFrame(fc, index=df.index[-n_future:], columns=df_train.columns)\n",
        "\n",
        "            common_cols = [c for c in df_forecast.columns if c in df_test.columns]\n",
        "            actual = df_test[common_cols].values.flatten()\n",
        "            predicted = df_forecast[common_cols].values.flatten()\n",
        "\n",
        "            mse = np.mean((actual - predicted) ** 2)\n",
        "            mae = np.mean(np.abs(actual - predicted))\n",
        "\n",
        "            return {\n",
        "                'mse': float(mse),\n",
        "                'mae': float(mae),\n",
        "                'forecast_success': True,\n",
        "                'used_columns': common_cols,\n",
        "                'actual_values': actual.tolist(),\n",
        "                'predicted_values': predicted.tolist()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'mse': 99999, 'mae': 99999, 'forecast_success': False, 'error': str(e)}\n",
        "\n",
        "    def predict_window_size(self, feature_vector: np.ndarray, sequence_3d: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"Predict window size using MLP and evaluate forecast\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'confidence': 0.0, 'error': \"Model not loaded\"}\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            prediction_raw = self.model.predict(feature_vector, verbose=0)\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = self.transformer.inverse_transform(prediction_raw)[0, 0]\n",
        "            else:\n",
        "                predicted_window = prediction_raw[0, 0]\n",
        "            predicted_window = int(round(predicted_window))\n",
        "\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            prediction_record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'forecast_success': forecast_metrics.get('forecast_success', False)\n",
        "            }\n",
        "\n",
        "            if forecast_metrics.get('forecast_success', False):\n",
        "                self.mse_history.append(forecast_metrics['mse'])\n",
        "                self.mae_history.append(forecast_metrics['mae'])\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = np.mean(self.mse_history)\n",
        "                self.performance_stats['avg_mae'] = np.mean(self.mae_history)\n",
        "\n",
        "                event_type = self._check_for_event()\n",
        "                prediction_record['event_type'] = event_type\n",
        "            else:\n",
        "                self.consecutive_poor_predictions += 1\n",
        "                prediction_record['event_type'] = None\n",
        "\n",
        "            self.prediction_history.append(prediction_record)\n",
        "            self.retraining_data['x_buffer'].append(feature_vector.flatten())\n",
        "            self.retraining_data['y_buffer'].append(predicted_window)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'confidence': self._calculate_confidence(prediction_record),\n",
        "                'performance_stats': self.get_recent_performance(),\n",
        "                'event_type': prediction_record['event_type'],\n",
        "                'prediction_id': len(self.prediction_history)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'confidence': 0.0, 'error': str(e)}\n",
        "\n",
        "    def _check_for_event(self) -> str:\n",
        "        \"\"\"Detect anomaly vs drift using persistence and error spread.\"\"\"\n",
        "        if len(self.mse_history) < self.drift_detection_window:\n",
        "            return None\n",
        "        try:\n",
        "            def ema(values, alpha=0.3):\n",
        "                ema_val = values[0]\n",
        "                for v in values[1:]:\n",
        "                    ema_val = alpha * v + (1 - alpha) * ema_val\n",
        "                return ema_val\n",
        "\n",
        "            mse_vals = list(self.mse_history)[-self.drift_detection_window:]\n",
        "            mae_vals = list(self.mae_history)[-self.drift_detection_window:]\n",
        "            ema_mse, ema_mae = ema(mse_vals), ema(mae_vals)\n",
        "            baseline_mse, baseline_mae = np.median(mse_vals), np.median(mae_vals)\n",
        "\n",
        "            mse_ratio = ema_mse / max(baseline_mse, 1e-5)\n",
        "            mae_ratio = ema_mae / max(baseline_mae, 1e-5)\n",
        "\n",
        "            # Absolute thresholds\n",
        "            if ema_mse < 0.02 and ema_mae < 0.08:\n",
        "                self.consecutive_poor_predictions = 0\n",
        "                return None\n",
        "\n",
        "            event_condition = (mse_ratio > (1 + self.drift_threshold_mse) and\n",
        "                               mae_ratio > (1 + self.drift_threshold_mae))\n",
        "\n",
        "            if event_condition:\n",
        "                self.consecutive_poor_predictions += 1\n",
        "            else:\n",
        "                if 0 < self.consecutive_poor_predictions < 3:\n",
        "                    self.performance_stats['anomaly_events'] += 1\n",
        "                    logger.warning(f\"ANOMALY detected: EMA_MSE={ema_mse:.4f}, EMA_MAE={ema_mae:.4f}\")\n",
        "                    self.consecutive_poor_predictions = 0\n",
        "                    return \"ANOMALY\"\n",
        "                self.consecutive_poor_predictions = 0\n",
        "\n",
        "            if self.consecutive_poor_predictions >= 5:\n",
        "                self.performance_stats['drift_events'] += 1\n",
        "                logger.warning(f\"DRIFT detected: MSE ratio={mse_ratio:.3f}, MAE ratio={mae_ratio:.3f}, \"\n",
        "                               f\"EMA_MSE={ema_mse:.4f}, EMA_MAE={ema_mae:.4f}\")\n",
        "                self.consecutive_poor_predictions = 0\n",
        "                return \"DRIFT\"\n",
        "\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Event detection error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _calculate_confidence(self, prediction_record: Dict) -> float:\n",
        "        if len(self.mse_history) < 10: return 0.5\n",
        "        recent_mse = np.mean(list(self.mse_history)[-10:])\n",
        "        recent_mae = np.mean(list(self.mae_history)[-10:])\n",
        "        mse_conf = max(0, 1 - (recent_mse / (np.percentile(list(self.mse_history), 25) * 4)))\n",
        "        mae_conf = max(0, 1 - (recent_mae / (np.percentile(list(self.mae_history), 25) * 4)))\n",
        "        return min(1.0, max(0.1, (mse_conf + mae_conf) / 2))\n",
        "\n",
        "    def get_recent_performance(self) -> Dict[str, Any]:\n",
        "        successful_predictions = [p for p in list(self.prediction_history)[-50:] if p.get('forecast_success', False)]\n",
        "        return {\n",
        "            'total_predictions': len(self.prediction_history),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(self.prediction_history), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': np.mean(list(self.mse_history)[-10:]) if self.mse_history else 0,\n",
        "            'avg_mse': np.mean(self.mse_history) if self.mse_history else 0,\n",
        "            'recent_mae': np.mean(list(self.mae_history)[-10:]) if self.mae_history else 0,\n",
        "            'avg_mae': np.mean(self.mae_history) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        state = {\n",
        "            'performance_stats': self.performance_stats.copy(),\n",
        "            'prediction_history': list(self.prediction_history)[-100:],\n",
        "            'mse_history': list(self.mse_history),\n",
        "            'mae_history': list(self.mae_history),\n",
        "            'transformer_fitted': self.transformer_fitted\n",
        "        }\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(state, f, default=str, indent=2)\n",
        "\n",
        "# (optional) plotting helper\n",
        "\n",
        "\n",
        "# ==================== PLOTTING ====================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_all_features_forecasts(test_sequences, agent, max_features=12):\n",
        "    all_actual, all_forecast, event_points = [], [], []\n",
        "    for i, seq in enumerate(test_sequences):\n",
        "        result = agent.predict_window_size(seq.flatten(), seq)\n",
        "        fm = result.get(\"forecast_metrics\", {})\n",
        "        if not fm.get(\"forecast_success\", False): continue\n",
        "        actual_row = [np.nan] * max_features\n",
        "        forecast_row = [np.nan] * max_features\n",
        "        for j, col in enumerate(fm.get(\"used_columns\", [])):\n",
        "            idx = int(col.replace(\"V\", \"\")) - 1\n",
        "            if idx < max_features:\n",
        "                actual_row[idx] = fm[\"actual_values\"][j]\n",
        "                forecast_row[idx] = fm[\"predicted_values\"][j]\n",
        "        all_actual.append(actual_row)\n",
        "        all_forecast.append(forecast_row)\n",
        "        if result.get(\"event_type\", None):\n",
        "            event_points.append((i, result[\"event_type\"]))\n",
        "    if not all_actual: return\n",
        "    actual_matrix = np.array(all_actual)\n",
        "    forecast_matrix = np.array(all_forecast)\n",
        "    timestamps = np.arange(len(actual_matrix))\n",
        "    fig, axes = plt.subplots(max_features, 1, figsize=(12, 2.5 * max_features), sharex=True)\n",
        "    for idx, ax in enumerate(axes):\n",
        "        ax.plot(timestamps, actual_matrix[:, idx], label=\"Actual\", marker=\"o\", alpha=0.6)\n",
        "        ax.plot(timestamps, forecast_matrix[:, idx], label=\"Forecast\", marker=\"x\", alpha=0.6)\n",
        "        for (t, etype) in event_points:\n",
        "            ax.scatter(t, actual_matrix[t, idx], color=\"red\" if etype==\"ANOMALY\" else \"orange\",\n",
        "                       marker=\"D\", label=etype if idx==0 else \"\")\n",
        "        ax.set_ylabel(f\"V{idx+1}\"); ax.grid(True, alpha=0.3)\n",
        "        if idx == 0: ax.legend()\n",
        "    axes[-1].set_xlabel(\"Sequence index\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ====== CoordinatorAgent ======\n",
        "\n",
        "class CoordinatorAgent:\n",
        "    \"\"\"\n",
        "    Coordinator Agent - orchestrates outputs from:\n",
        "      1. RobustMasterAgent (sensor-level aggregation)\n",
        "      2. AdaptiveWindowAgent (global window-level perspective)\n",
        "\n",
        "    Combines them into a final system decision:\n",
        "      - True anomalies (local + global agreement)\n",
        "      - Concept drift (global window instability)\n",
        "      - Retraining triggers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, anomaly_weight: float = 0.6, window_weight: float = 0.4):\n",
        "        self.anomaly_weight = anomaly_weight\n",
        "        self.window_weight = window_weight\n",
        "        self.history = []\n",
        "\n",
        "    def fuse(self, master_output: dict, window_output: dict) -> dict:\n",
        "        timestamp = datetime.now()\n",
        "\n",
        "        # Master outputs\n",
        "        sys_dec = master_output.get(\"system_decisions\", {})\n",
        "        sys_anomaly = sys_dec.get(\"system_anomaly\", False)\n",
        "        sys_drift = sys_dec.get(\"system_drift\", False)\n",
        "        sys_retrain = sys_dec.get(\"system_needs_retrain\", False)\n",
        "        anomaly_rate = sys_dec.get(\"anomaly_rate\", 0.0)\n",
        "        drift_rate = sys_dec.get(\"drift_rate\", 0.0)\n",
        "\n",
        "        # Window outputs\n",
        "        win_anomaly = window_output.get(\"window_anomaly_flag\", False) \\\n",
        "                      or (window_output.get(\"event_type\") == \"ANOMALY\")\n",
        "        win_drift = window_output.get(\"window_drift_flag\", False) \\\n",
        "                    or (window_output.get(\"event_type\") == \"DRIFT\")\n",
        "        predicted_window = window_output.get(\"predicted_window\", None)\n",
        "\n",
        "        # Fusion logic\n",
        "        anomaly_score = (\n",
        "            self.anomaly_weight * anomaly_rate +\n",
        "            self.window_weight * (1.0 if win_anomaly else 0.0)\n",
        "        )\n",
        "        drift_score = (\n",
        "            self.anomaly_weight * drift_rate +\n",
        "            self.window_weight * (1.0 if win_drift else 0.0)\n",
        "        )\n",
        "\n",
        "        final_anomaly = anomaly_score >= 0.3 or (sys_anomaly and win_anomaly)\n",
        "        final_drift = drift_score >= 0.2 or (sys_drift or win_drift)\n",
        "        final_retrain = sys_retrain or (win_drift and anomaly_score > 0.2)\n",
        "\n",
        "        if final_anomaly and final_drift:\n",
        "            alert = \"CRITICAL\"\n",
        "        elif final_anomaly:\n",
        "            alert = \"HIGH\"\n",
        "        elif final_drift:\n",
        "            alert = \"MEDIUM\"\n",
        "        else:\n",
        "            alert = \"NORMAL\"\n",
        "\n",
        "        decision = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"final_anomaly\": final_anomaly,\n",
        "            \"final_drift\": final_drift,\n",
        "            \"final_retrain\": final_retrain,\n",
        "            \"alert_level\": alert,\n",
        "            \"scores\": {\n",
        "                \"anomaly_score\": anomaly_score,\n",
        "                \"drift_score\": drift_score,\n",
        "                \"sensor_anomaly_rate\": anomaly_rate,\n",
        "                \"sensor_drift_rate\": drift_rate\n",
        "            },\n",
        "            \"window_agent\": {\n",
        "                \"predicted_window\": predicted_window,\n",
        "                \"window_anomaly\": win_anomaly,\n",
        "                \"window_drift\": win_drift\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.history.append(decision)\n",
        "        return decision\n",
        "\n",
        "##################################################\n",
        "#GROUND TRUTH ANOMALY VS WHICH AGENT AGENT IS RIGHT\n",
        "############################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_agent_vs_groundtruth(results, anomaly_labels, pred_labels_h1=None, pred_labels_h5=None, max_samples=200):\n",
        "    \"\"\"\n",
        "    Compare Master, Window, Coordinator outputs vs ground truth labels.\n",
        "\n",
        "    Args:\n",
        "        results: list of dicts from your demo loop (each has master_out, window_out, coordinator).\n",
        "        anomaly_labels: np.ndarray of shape [N] (0/1 ground truth detection).\n",
        "        pred_labels_h1: np.ndarray of shape [N] (0/1 ground truth prediction 1h).\n",
        "        pred_labels_h5: np.ndarray of shape [N] (0/1 ground truth prediction 5h).\n",
        "        max_samples: number of samples to visualize.\n",
        "    \"\"\"\n",
        "\n",
        "    n = min(len(results), max_samples, len(anomaly_labels))\n",
        "    x = np.arange(n)\n",
        "\n",
        "    # Extract agent outputs\n",
        "    master_anom_rate = [r['master']['system_decisions']['anomaly_rate'] for r in results[:n]]\n",
        "    window_events = [1 if r['window'].get('event_type') == \"DRIFT\" else\n",
        "                     (0.5 if r['window'].get('event_type') == \"ANOMALY\" else 0)\n",
        "                     for r in results[:n]]\n",
        "    coord_alert = []\n",
        "    for r in results[:n]:\n",
        "        lvl = r['coordinator']['alert_level']\n",
        "        if lvl == \"NORMAL\": coord_alert.append(0)\n",
        "        elif lvl == \"MEDIUM\": coord_alert.append(1)\n",
        "        elif lvl == \"HIGH\": coord_alert.append(2)\n",
        "        elif lvl == \"CRITICAL\": coord_alert.append(3)\n",
        "\n",
        "    # Ground truth\n",
        "    gt_detect = anomaly_labels[:n]\n",
        "    gt_h1 = pred_labels_h1[:n] if pred_labels_h1 is not None else None\n",
        "    gt_h5 = pred_labels_h5[:n] if pred_labels_h5 is not None else None\n",
        "\n",
        "    # --- Plot ---\n",
        "    fig, axes = plt.subplots(4 if gt_h1 is not None else 3, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "    axes[0].plot(x, master_anom_rate, label=\"Master anomaly rate\", color=\"blue\")\n",
        "    axes[0].scatter(x, window_events, label=\"Window events (0.5=ANOM,1=DRIFT)\", color=\"orange\", marker=\"x\")\n",
        "    axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1].plot(x, coord_alert, label=\"Coordinator Alert Level (0-3)\", color=\"red\")\n",
        "    axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[2].step(x, gt_detect, where=\"mid\", label=\"GT Anomaly Detection\", color=\"green\")\n",
        "    if gt_h1 is None and gt_h5 is None:\n",
        "        axes[2].legend(); axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    if gt_h1 is not None:\n",
        "        axes[2].step(x, gt_h1, where=\"mid\", label=\"GT 1h Prediction\", color=\"purple\", linestyle=\"--\")\n",
        "    if gt_h5 is not None:\n",
        "        axes[2].step(x, gt_h5, where=\"mid\", label=\"GT 5h Prediction\", color=\"brown\", linestyle=\":\")\n",
        "    axes[2].legend(); axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[-1].set_xlabel(\"Sequence index\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =========================\n",
        "# DEMO LOOP\n",
        "# =========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "    mlp_path = \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_Daily.keras\"\n",
        "    data_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\"\n",
        "\n",
        "\n",
        "\n",
        "    # create agents\n",
        "    sensor_agents, master = create_robust_system(num_sensors=12, models_dir=models_dir)\n",
        "    window_agent = AdaptiveWindowAgent(model_path=mlp_path)\n",
        "    coordinator = CoordinatorAgent()\n",
        "\n",
        "    # load test data\n",
        "    subsequences = np.load(data_path)\n",
        "    holdout = subsequences[-1000:]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, seq in enumerate(holdout, 1):\n",
        "        features = seq.flatten()\n",
        "        master_out = master.process_system_input(seq)\n",
        "        window_out = window_agent.predict_window_size(features, seq)\n",
        "        final = coordinator.fuse(master_out, window_out)\n",
        "        results.append({\n",
        "        \"master\": master_out,\n",
        "        \"window\": window_out,\n",
        "        \"coordinator\": final\n",
        "        })\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Sample {i}\")\n",
        "        print(\"Master:\", master_out[\"system_decisions\"])\n",
        "        print(\"Window:\", {\"predicted_window\": window_out.get(\"predicted_window\"),\n",
        "                          \"event\": window_out.get(\"event_type\")})\n",
        "        print(\"Coordinator:\", final[\"alert_level\"], final[\"scores\"])\n",
        "\n",
        "    anomaly_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_detection.npy\")\n",
        "    h1_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H1.npy\")\n",
        "    h5_labels = np.load(\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/anomaly_labels_H5.npy\")\n",
        "\n",
        "    plot_agent_vs_groundtruth(results, anomaly_labels, h1_labels, h5_labels, max_samples=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-COMBINED-AGENTS.ipynb",
      "authorship_tag": "ABX9TyNnMMDELGAYRwe8V1bmzxP4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}