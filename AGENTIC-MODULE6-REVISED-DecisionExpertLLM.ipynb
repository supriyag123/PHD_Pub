{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-REVISED-DecisionExpertLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ff4d12-bc53-4b6f-8e2e-9fd9c9718f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== HOLDOUT SET PERFORMANCE (FINAL) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9996    0.9896    0.9946    344234\n",
            "           1     0.1739    0.9408    0.2936       726\n",
            "           2     0.7771    0.8832    0.8268      1721\n",
            "\n",
            "    accuracy                         0.9890    346681\n",
            "   macro avg     0.6502    0.9379    0.7050    346681\n",
            "weighted avg     0.9968    0.9890    0.9923    346681\n",
            "\n",
            "Confusion matrix:\n",
            " [[340667   3131    436]\n",
            " [    43    683      0]\n",
            " [    88    113   1520]]\n",
            "{'p_normal': 0.5116674900054932, 'early_fault_score': 0.43738940358161926, 'severe_fault_score': 0.05094306915998459, 'anomaly_score': 0.48833250999450684}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# Fault Classification Pipeline\n",
        "# ============================================================\n",
        "\n",
        "############PASTE ADAPTIVE WINDOW HERE - so everything is in one file - later, we can import as a package#####################\n",
        "\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "# =====================================================\n",
        "# AdaptiveWindowAgent (improved version)\n",
        "# =====================================================\n",
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor  # ✅ MOVED TO TOP\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "class EventStore:\n",
        "    def __init__(self, db_path=\"event_store.db\"):\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self._init_tables()\n",
        "\n",
        "    def _init_tables(self):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS events (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT,\n",
        "                event_type TEXT,\n",
        "                packet_json TEXT,\n",
        "                expert_json TEXT,\n",
        "                human_json TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.conn.commit()\n",
        "\n",
        "    def save_event(self, event_type, packet, expert=None, human=None):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\n",
        "            \"INSERT INTO events(timestamp,event_type,packet_json,expert_json,human_json) VALUES (?,?,?,?,?)\",\n",
        "            (\n",
        "                datetime.now().isoformat(),\n",
        "                event_type,\n",
        "                json.dumps(packet, default=str),\n",
        "                json.dumps(expert, default=str) if expert else None,\n",
        "                json.dumps(human, default=str) if human else None,\n",
        "            )\n",
        "        )\n",
        "        self.conn.commit()\n",
        "\n",
        "    def fetch_recent(self, limit=100):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"SELECT packet_json, expert_json, human_json FROM events ORDER BY id DESC LIMIT ?\", (limit,))\n",
        "        return [json.loads(row[0]) for row in c.fetchall()]\n",
        "\n",
        "#########################################################################\n",
        "# Window Agent - Global Context or Global Predictive Context\n",
        "#########################################################################\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Adaptive Window Agent:\n",
        "    - Predicts window size using MLP\n",
        "    - Evaluates forecast with RF/persistence\n",
        "    - Computes:\n",
        "        * FDS: Forecast Deviation Score (normalized error)\n",
        "        * FDI: Forecast Drift Index (JSD over FDS distribution)\n",
        "        * WSS: Window Shift Score (normalized window size)\n",
        "        * WDI: Window Drift Index (JSD over window size distribution)\n",
        "    - Detects anomaly (local) + drift (regime) events.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id=\"adaptive_window_agent\",\n",
        "                 model_path=None, checkpoint_path=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        "\n",
        "        self.baseline_path = \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_baseline.pkl\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Core model\n",
        "        # --------------------------------------------------\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Baseline error stats (median / MAD) – filled from baseline file\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Metrics memory\n",
        "        # --------------------------------------------------\n",
        "        # Raw error\n",
        "        self.error_memory = deque(maxlen=300)     # long-term errors\n",
        "        self.recent_errors = deque(maxlen=50)     # kept for backward compat (not central now)\n",
        "\n",
        "        # FDS (Forecast Deviation Score) history\n",
        "        self.fds_memory = deque(maxlen=300)\n",
        "        self.recent_fds = deque(maxlen=50)\n",
        "\n",
        "        # Window history\n",
        "        self.window_memory = deque(maxlen=300)\n",
        "        self.recent_windows = deque(maxlen=50)\n",
        "\n",
        "        # Last-step metrics (for returning)\n",
        "        self.last_fds = 0.0\n",
        "        self.last_fdi = 0.0\n",
        "        self.last_wss = 0.0\n",
        "        self.last_wdi = 0.0\n",
        "\n",
        "        # Baseline distributions\n",
        "        self.baseline_errors = None\n",
        "        self.baseline_fds = None\n",
        "        self.baseline_windows = None\n",
        "        self.window_mean = 50.0    # a reasonable mid value\n",
        "        self.window_std = 10.0     # non-zero, avoids div-by-zero\n",
        "\n",
        "        # Optional histogram bins stored in baseline\n",
        "        self.window_hist_bins = None\n",
        "        self.window_hist_counts = None\n",
        "\n",
        "        # Debug flag (OFF by default)\n",
        "        self.debug = False\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Anomaly / drift settings\n",
        "        # --------------------------------------------------\n",
        "        self.threshold_k = 3.0\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.anomaly_cooldown_steps = 5\n",
        "\n",
        "        # Drift detection\n",
        "        self.drift_threshold = 0.25          # for FDI (JSD over FDS)\n",
        "        self.window_drift_threshold = 0.20   # for WDI (JSD over window sizes)\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.drift_cooldown = 0\n",
        "        self.drift_votes_required = 10\n",
        "        self.drift_cooldown_steps = 100\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Retraining buffer (unchanged)\n",
        "        # --------------------------------------------------\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Prediction history (for reporting)\n",
        "        # --------------------------------------------------\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load baseline (errors + windows)\n",
        "        # --------------------------------------------------\n",
        "        self._load_baseline()\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load model last\n",
        "        # --------------------------------------------------\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load NSP (Next-Step Predictor)\n",
        "        # --------------------------------------------------\n",
        "        self.nsp_model_path = \"/content/drive/MyDrive/PHD/2025/NSP_LSTM_next_step.keras\"\n",
        "        self.nsp_model = keras.models.load_model(self.nsp_model_path)\n",
        "        print(\"✅ Loaded NSP LSTM next-step predictor\")\n",
        "\n",
        "    # =================== BASELINE LOADING ===================\n",
        "\n",
        "    def _load_baseline(self):\n",
        "        \"\"\"\n",
        "        Load baseline stats:\n",
        "          - baseline_errors, median, mad\n",
        "          - baseline_windows, window_mean, window_std\n",
        "          - optional histogram bins/counts for windows\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.baseline_path):\n",
        "            with open(self.baseline_path, \"rb\") as f:\n",
        "                base = pickle.load(f)\n",
        "\n",
        "            # Error baseline\n",
        "            self.baseline_errors = np.array(base[\"baseline_errors\"])\n",
        "            self.rolling_stats[\"median\"] = base[\"median\"]\n",
        "            self.rolling_stats[\"mad\"] = base[\"mad\"]\n",
        "\n",
        "            # Precompute baseline FDS distribution\n",
        "            med = self.rolling_stats[\"median\"]\n",
        "            mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "            self.baseline_fds = (self.baseline_errors - med) / (mad + 1e-8)\n",
        "\n",
        "            # Window baseline (may or may not exist)\n",
        "            if \"baseline_windows\" in base:\n",
        "                self.baseline_windows = np.array(base[\"baseline_windows\"])\n",
        "                self.window_mean = float(base.get(\"window_mean\", np.mean(self.baseline_windows)))\n",
        "                self.window_std = float(base.get(\"window_std\", np.std(self.baseline_windows) + 1e-8))\n",
        "                self.window_hist_bins = np.array(base.get(\"window_hist_bins\", [])) if \"window_hist_bins\" in base else None\n",
        "                self.window_hist_counts = np.array(base.get(\"window_hist_counts\", [])) if \"window_hist_counts\" in base else None\n",
        "            else:\n",
        "                self.baseline_windows = None\n",
        "                self.window_mean = 0.0\n",
        "                self.window_std = 1.0\n",
        "                self.window_hist_bins = None\n",
        "                self.window_hist_counts = None\n",
        "\n",
        "            print(\"✅ Loaded baseline error + window distribution.\")\n",
        "            print(f\"   Error median={self.rolling_stats['median']:.6f}, MAD={self.rolling_stats['mad']:.6f}\")\n",
        "            if self.baseline_windows is not None:\n",
        "                print(f\"   Window mean={self.window_mean:.3f}, std={self.window_std:.3f}\")\n",
        "        else:\n",
        "            print(\"⚠️ No baseline found. Using live history only.\")\n",
        "            self.baseline_errors = None\n",
        "            self.baseline_fds = None\n",
        "            self.baseline_windows = None\n",
        "\n",
        "    # =================== MODEL LOADING ===================\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"✅ Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                # Try to load transformer\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                else:\n",
        "                    # Fit transformer from true window labels\n",
        "                    y_original = np.load(\n",
        "                        \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy\"\n",
        "                    )\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(\"⚠️ No transformer found, fitted a new one.\")\n",
        "            else:\n",
        "                print(f\"❌ Model not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {e}\")\n",
        "\n",
        "    # =================== FORECAST EVALUATION ===================\n",
        "\n",
        "    def evaluate_forecast_performance(self, sequence_3d, predicted_window, n_future=1):\n",
        "        try:\n",
        "            seq = np.asarray(sequence_3d)\n",
        "            T, F = seq.shape\n",
        "\n",
        "            W = int(predicted_window)\n",
        "            if W < 2:\n",
        "                W = 2\n",
        "            if W > T - n_future - 1:\n",
        "                W = max(2, T - n_future - 1)\n",
        "\n",
        "            # --- Prepare NSP input ---\n",
        "            window = seq[-W:-n_future, :]   # shape (W-1, F)\n",
        "            x = window[np.newaxis, ...]      # shape (1, W-1, F)\n",
        "\n",
        "            # --- NSP prediction ---\n",
        "            y_pred = self.nsp_model.predict(x, verbose=0)[0]  # (F,)\n",
        "            y_true = seq[-n_future, :]                        # (F,)\n",
        "\n",
        "            mse = float(np.mean((y_true - y_pred) ** 2))\n",
        "            mae = float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "            return {\n",
        "                \"mse\": mse,\n",
        "                \"mae\": mae,\n",
        "                \"forecast_success\": True,\n",
        "                \"actual_values\": y_true.tolist(),\n",
        "                \"predicted_values\": y_pred.tolist(),\n",
        "                \"window_size_used\": W,\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"mse\": 9999.0,\n",
        "                \"mae\": 9999.0,\n",
        "                \"forecast_success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "    # =================== PERSISTENCE FALLBACK ===================\n",
        "\n",
        "    def _persistence_forecast(self, seq, target_sensor_index, n_future):\n",
        "        \"\"\"\n",
        "        Persistence fallback for RF evaluation.\n",
        "        Last-value-carried-forward for target sensor.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            seq = np.asarray(seq)\n",
        "            if len(seq) < 2:\n",
        "                return {\n",
        "                    'mse': 9999,\n",
        "                    'mae': 9999,\n",
        "                    'forecast_success': False,\n",
        "                    'error': 'Sequence too short',\n",
        "                    'method': 'Persistence'\n",
        "                }\n",
        "\n",
        "            last_value = seq[-1, target_sensor_index]\n",
        "            predicted_vals = [last_value]\n",
        "            actual = [seq[-1, target_sensor_index]]\n",
        "\n",
        "            mse = 0.0\n",
        "            mae = 0.0\n",
        "\n",
        "            return {\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual,\n",
        "                'predicted_values': predicted_vals,\n",
        "                'target_sensor_index': target_sensor_index,\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'mse': 9999,\n",
        "                'mae': 9999,\n",
        "                'forecast_success': False,\n",
        "                'error': str(e),\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback_failed'\n",
        "            }\n",
        "\n",
        "    # =================== PREDICTION PIPELINE ===================\n",
        "\n",
        "    def predict_window_size(self, feature_vector, sequence_3d):\n",
        "        \"\"\"\n",
        "        Main entrypoint:\n",
        "          - Predict window W_t\n",
        "          - Evaluate forecast error e_t\n",
        "          - Compute FDS (S_t) and WSS (Z_t)\n",
        "          - Update drift/anomaly logic (FDI, WDI, events)\n",
        "          - Return full metrics packet\n",
        "        \"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'error': \"Model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            pred_raw = self.model.predict(feature_vector, verbose=0)\n",
        "\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = int(round(self.transformer.inverse_transform(pred_raw)[0, 0]))\n",
        "            else:\n",
        "                predicted_window = int(round(pred_raw[0, 0]))\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # WINDOW CLAMP — HARD SAFETY FIX\n",
        "            # ----------------------------------------\n",
        "            # Prevent negative, zero, or extreme window sizes\n",
        "            predicted_window = max(2, predicted_window)        # lower bound\n",
        "\n",
        "            # Forecast evaluation\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            fds = None\n",
        "            wss = None\n",
        "\n",
        "            if forecast_metrics.get(\"forecast_success\", False):\n",
        "                mse = forecast_metrics[\"mse\"]\n",
        "                mae = forecast_metrics[\"mae\"]\n",
        "\n",
        "                # Basic stats\n",
        "                self.mse_history.append(mse)\n",
        "                self.mae_history.append(mae)\n",
        "                self.error_memory.append(mse)\n",
        "\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = float(np.mean(self.mse_history))\n",
        "                self.performance_stats['avg_mae'] = float(np.mean(self.mae_history))\n",
        "\n",
        "                # ---------- Forecast Deviation Score (FDS) ----------\n",
        "                baseline_median = self.rolling_stats[\"median\"]\n",
        "                baseline_mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "                fds = (mse - baseline_median) / (baseline_mad + 1e-8)\n",
        "                fds = float(fds) if fds is not None and not np.isnan(fds) else 0.0\n",
        "\n",
        "\n",
        "                self.last_fds = fds\n",
        "                self.fds_memory.append(fds)\n",
        "                self.recent_fds.append(fds)\n",
        "\n",
        "                # ---------- Window Shift Score (WSS) ----------\n",
        "                if self.baseline_windows is not None and self.window_std > 0:\n",
        "                    wss = (predicted_window - self.window_mean) / (self.window_std + 1e-8)\n",
        "                else:\n",
        "                    wss = 0.0\n",
        "\n",
        "                wss = float(wss)\n",
        "                self.last_wss = wss\n",
        "                self.window_memory.append(predicted_window)\n",
        "                self.recent_windows.append(predicted_window)\n",
        "\n",
        "            # Event (ANOMALY / DRIFT) + Drift indices\n",
        "            event, sev, fdi, wdi = self._check_for_event()\n",
        "            self.last_fdi = fdi\n",
        "            self.last_wdi = wdi\n",
        "\n",
        "            # Save history for reporting\n",
        "            record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'wss': self.last_wss,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev\n",
        "            }\n",
        "            self.prediction_history.append(record)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wss': self.last_wss,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev,\n",
        "                'performance_stats': self.get_recent_performance()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'error': str(e)}\n",
        "\n",
        "    # =================== EVENT LOGIC (ANOMALY + DRIFT) ===================\n",
        "\n",
        "    def _check_for_event(self):\n",
        "        \"\"\"\n",
        "        Event detection for the Adaptive Window Agent.\n",
        "\n",
        "        - ANOMALY: deviation of last MSE from baseline (median + k * MAD)\n",
        "        - DRIFT:\n",
        "            * FDI: JSD between recent FDS distribution and baseline FDS\n",
        "            * WDI: JSD between recent window distribution and baseline window distribution\n",
        "        \"\"\"\n",
        "        # Require enough history\n",
        "        if len(self.error_memory) < 30:\n",
        "            return None, 0.0, None, None\n",
        "\n",
        "        last_mse = float(self.error_memory[-1])\n",
        "        live_errors = np.array(self.error_memory)\n",
        "\n",
        "        # ---------- BASELINE STATS ----------\n",
        "        if self.baseline_errors is not None and len(self.baseline_errors) > 10:\n",
        "            base_errors = np.array(self.baseline_errors)\n",
        "            baseline_median = np.median(base_errors)\n",
        "            baseline_mad = np.median(np.abs(base_errors - baseline_median)) + 1e-8\n",
        "        else:\n",
        "            baseline_median = np.median(live_errors)\n",
        "            baseline_mad = np.median(np.abs(live_errors - baseline_median)) + 1e-8\n",
        "\n",
        "        # Update rolling_stats so other components can see latest baseline-ish values\n",
        "        self.rolling_stats[\"median\"] = baseline_median\n",
        "        self.rolling_stats[\"mad\"] = baseline_mad\n",
        "\n",
        "        # ---------- LIVE STATS ----------\n",
        "        live_median = np.median(live_errors)\n",
        "        live_mad = np.median(np.abs(live_errors - live_median)) + 1e-8\n",
        "\n",
        "        # ---------- ANOMALY THRESHOLD ----------\n",
        "        baseline_threshold = baseline_median + self.threshold_k * baseline_mad\n",
        "        live_threshold = live_median + self.threshold_k * live_mad\n",
        "\n",
        "        anomaly_threshold = 0.8 * baseline_threshold + 0.2 * live_threshold\n",
        "\n",
        "        is_anomaly = last_mse > anomaly_threshold\n",
        "\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            is_anomaly = False\n",
        "        elif is_anomaly:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "\n",
        "        if is_anomaly:\n",
        "            severity = (last_mse - anomaly_threshold) / (baseline_mad + 1e-6)\n",
        "            severity = float(severity)\n",
        "            self.performance_stats[\"anomaly_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[ANOMALY] mse={last_mse:.6f}, thr={anomaly_threshold:.6f}, sev={severity:.3f}\")\n",
        "            return \"ANOMALY\", severity, 0.0, 0.0\n",
        "\n",
        "        # ---------- DRIFT (FDI + WDI) ----------\n",
        "        fdi = None\n",
        "        wdi = None\n",
        "\n",
        "        # FDI: JSD over FDS distribution\n",
        "        if self.baseline_fds is not None and len(self.recent_fds) >= 30:\n",
        "            base_fds = np.asarray(self.baseline_fds)\n",
        "            recent_fds = np.asarray(self.recent_fds)\n",
        "\n",
        "            hist_base, bins = np.histogram(base_fds, bins=25, density=True)\n",
        "            hist_recent, _ = np.histogram(recent_fds, bins=bins, density=True)\n",
        "\n",
        "            hist_base = hist_base / (hist_base.sum() + 1e-12)\n",
        "            hist_recent = hist_recent / (hist_recent.sum() + 1e-12)\n",
        "\n",
        "            fdi = float(jensenshannon(hist_base + 1e-12, hist_recent + 1e-12))\n",
        "\n",
        "        # WDI: JSD over window-size distribution\n",
        "        if self.baseline_windows is not None and len(self.recent_windows) >= 30:\n",
        "            base_win = np.asarray(self.baseline_windows)\n",
        "            recent_win = np.asarray(self.recent_windows)\n",
        "\n",
        "            hist_w_base, bins_w = np.histogram(base_win, bins=20, density=True)\n",
        "            hist_w_recent, _ = np.histogram(recent_win, bins=bins_w, density=True)\n",
        "\n",
        "            hist_w_base = hist_w_base / (hist_w_base.sum() + 1e-12)\n",
        "            hist_w_recent = hist_w_recent / (hist_w_recent.sum() + 1e-12)\n",
        "\n",
        "            wdi = float(jensenshannon(hist_w_base + 1e-12, hist_w_recent + 1e-12))\n",
        "\n",
        "        # Decide drift if either index is high\n",
        "        is_drift_fdi = fdi is not None and fdi > self.drift_threshold\n",
        "        is_drift_wdi = wdi is not None and wdi > self.window_drift_threshold\n",
        "\n",
        "        is_drift = is_drift_fdi or is_drift_wdi\n",
        "\n",
        "        if self.drift_cooldown > 0:\n",
        "            self.drift_cooldown -= 1\n",
        "            is_drift = False\n",
        "        else:\n",
        "            if is_drift:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "        if self.consecutive_drift_votes >= self.drift_votes_required:\n",
        "            self.consecutive_drift_votes = 0\n",
        "            self.drift_cooldown = self.drift_cooldown_steps\n",
        "            self.performance_stats[\"drift_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[DRIFT] FDI={fdi:.4f} WDI={wdi:.4f}\")\n",
        "            fdi = float(fdi) if fdi is not None else 0.0\n",
        "            wdi = float(wdi) if wdi is not None else 0.0\n",
        "            return \"DRIFT\", fdi, fdi, wdi\n",
        "\n",
        "        # Make safe for printing\n",
        "        fdi = float(fdi) if fdi is not None else 0.0\n",
        "        wdi = float(wdi) if wdi is not None else 0.0\n",
        "\n",
        "        return None, 0.0, fdi, wdi\n",
        "\n",
        "\n",
        "    # =================== HELPERS ===================\n",
        "\n",
        "    def get_recent_performance(self):\n",
        "        all_preds = list(self.prediction_history)\n",
        "\n",
        "        successful_predictions = [\n",
        "            p for p in all_preds\n",
        "            if p.get('forecast_metrics', {}).get('forecast_success', False)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'total_predictions': len(all_preds),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(all_preds), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': float(np.mean(list(self.mse_history)[-10:])) if self.mse_history else 0,\n",
        "            'avg_mse': float(np.mean(self.mse_history)) if self.mse_history else 0,\n",
        "            'recent_mae': float(np.mean(list(self.mae_history)[-10:])) if self.mae_history else 0,\n",
        "            'avg_mae': float(np.mean(self.mae_history)) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted,\n",
        "            'last_fdi': self.last_fdi,\n",
        "            'last_wdi': self.last_wdi,\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save performance statistics + prediction history to JSON\"\"\"\n",
        "        try:\n",
        "            state = {\n",
        "                'performance_stats': self.performance_stats.copy(),\n",
        "                'prediction_history': list(self.prediction_history)[-100:],\n",
        "                'mse_history': list(self.mse_history),\n",
        "                'mae_history': list(self.mae_history),\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, indent=2, default=str)\n",
        "            print(f\"✅ Performance state saved to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save performance state: {e}\")\n",
        "\n",
        "\n",
        "#===============================================================================================================================================\n",
        "###  SENSOR AGENTS - INDIVIDUAL AND MASTER\n",
        "#--------------------------------------==========================================================================================================\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 10, #K\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1,\n",
        "                warmup_steps: int = 100):    # <── NEW PARAM\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "            'mean': 0.0,   # backward compatibility\n",
        "            'std': 1.0,    # backward compatibility\n",
        "            'q95': 0.0,\n",
        "            'q99': 0.0\n",
        "        }\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.drift_cooldown = 0\n",
        "\n",
        "        self.anomaly_cooldown_steps = 5    # you can tune\n",
        "        self.drift_cooldown_steps = 10     # you can tune\n",
        "\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.consecutive_anomaly_votes = 0\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Correct metadata file\n",
        "                metadata_path = model_path.replace('_model.h5', '_metadata.pkl')\n",
        "\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "\n",
        "                    baseline = metadata.get('baseline_stats', None)\n",
        "\n",
        "                    if baseline is not None:\n",
        "                        # Initialize rolling stats from training\n",
        "                        # Load robust baseline stats\n",
        "                        self.rolling_stats['median'] = baseline.get('median')\n",
        "                        self.rolling_stats['mad']    = baseline.get('mad')\n",
        "\n",
        "                        # Backward compatibility for other parts of system\n",
        "                        self.rolling_stats['mean'] = self.rolling_stats['median']\n",
        "                        self.rolling_stats['std']  = self.rolling_stats['mad']\n",
        "\n",
        "                        self.rolling_stats['q95']  = baseline['q95']\n",
        "                        self.rolling_stats['q99']  = baseline['q99']\n",
        "\n",
        "                        # Save baseline distribution for drift detection\n",
        "                        self.baseline_errors = np.array(baseline['baseline_errors'])\n",
        "\n",
        "                # AE was trained on raw, NOT scaled\n",
        "                self.scaler = None\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format – expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"✅ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3\n",
        "\n",
        "        # --------------- WARM-UP PHASE -----------------\n",
        "        # During warm-up, rolling stats ignore live data and stay fixed\n",
        "        if self.total_processed < self.warmup_steps:\n",
        "            med = np.median(self.baseline_errors)\n",
        "            mad = np.median(np.abs(self.baseline_errors - med)) + 1e-8\n",
        "\n",
        "            self.rolling_stats['median'] = med\n",
        "            self.rolling_stats['mad'] = mad\n",
        "            self.rolling_stats['mean'] = med     # backward compatibility\n",
        "            self.rolling_stats['std'] = mad\n",
        "        else:\n",
        "            # After warm-up, rolling stats evolve normally\n",
        "            if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "                self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['median'] + self.threshold_k * self.rolling_stats['mad']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model on RAW values.\"\"\"\n",
        "        try:\n",
        "            # Ensure shape: [1, window_length, 1]\n",
        "            X = subsequence.reshape(1, self.window_length, 1)\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "\n",
        "            error = mean_squared_error(\n",
        "                subsequence.flatten(),\n",
        "                reconstruction.flatten()\n",
        "            )\n",
        "            return max(0.0, error)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            # Fallback: variance of raw subsequence\n",
        "            return float(np.var(subsequence))\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "\n",
        "        median = np.median(errors_array)\n",
        "        mad = np.median(np.abs(errors_array - median)) + 1e-8  # avoid zero\n",
        "\n",
        "        # Store\n",
        "        self.rolling_stats['median'] = median\n",
        "        self.rolling_stats['mad'] = mad\n",
        "\n",
        "        # Backward compatibility fields (for plotting)\n",
        "        self.rolling_stats['mean'] = median\n",
        "        self.rolling_stats['std'] = mad\n",
        "\n",
        "        # Percentile bands (unchanged; good for drift & visualization)\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        median = self.rolling_stats.get('median', self.rolling_stats['mean'])\n",
        "        mad = self.rolling_stats.get('mad', self.rolling_stats['std'])\n",
        "        threshold = median + self.threshold_k * mad\n",
        "        is_anomaly_now = score > threshold\n",
        "\n",
        "        # Cooldown active → suppress anomaly\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            return False\n",
        "\n",
        "        # No cooldown and anomaly happened → activate cooldown\n",
        "        if is_anomaly_now:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            is_drift_now = js_divergence > self.drift_threshold\n",
        "\n",
        "            # Cooldown active → suppress\n",
        "            if self.drift_cooldown > 0:\n",
        "                self.drift_cooldown -= 1\n",
        "                return False\n",
        "\n",
        "            # Multi-step confirmation: require 3 drift votes in last few steps\n",
        "            if is_drift_now:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "            if self.consecutive_drift_votes >= 3:\n",
        "                self.drift_cooldown = self.drift_cooldown_steps\n",
        "                self.consecutive_drift_votes = 0\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        median = self.rolling_stats.get('median')\n",
        "        mad = self.rolling_stats.get('mad')\n",
        "\n",
        "        if mad == 0:\n",
        "            return 0.5\n",
        "\n",
        "        threshold = median + self.threshold_k * mad\n",
        "\n",
        "        z = (score - threshold) / mad  # how far beyond threshold?\n",
        "\n",
        "        # Smooth probability-like mapping\n",
        "        confidence = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        return float(np.clip(confidence, 0.0, 1.0))\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str, win_length: int, warmup_steps: int = 100) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"🚀 Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=win_length,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1,\n",
        "                                  warmup_steps=warmup_steps)       # <── NEW\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"✅ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "\n",
        "    return sensor_agents, master\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# DECISION AGENT (replaces / extends CoordinatorAgent)\n",
        "# =====================================================\n",
        "from typing import Optional\n",
        "\n",
        "# =====================================================\n",
        "# DecisionAgent V2-PURE (Prediction-focused fusion)\n",
        "# =====================================================\n",
        "from typing import Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionAgent:\n",
        "    \"\"\"\n",
        "    DecisionAgent V2-PURE:\n",
        "\n",
        "    - INPUTS (same call signature as before):\n",
        "        * master_output: RobustMasterAgent.process_system_input(...)\n",
        "        * window_output: AdaptiveWindowAgent.predict_window_size(...)\n",
        "        * model_outputs: optional dict, expected keys:\n",
        "              - \"failure_prob\": float in [0,1]  (ML failure prediction)\n",
        "              - \"prototype_score\": float (optional, diagnostic only)\n",
        "              - \"transformer_prob\": float (optional, diagnostic only)\n",
        "        * metadata: optional dict (e.g. index, timestamps)\n",
        "\n",
        "    - LOGIC:\n",
        "        * Detection layer (ANOMALY / DRIFT / RETRAIN)\n",
        "              -> derived ONLY from sensor + window signals\n",
        "        * Prediction layer (FAILURE PREDICTION)\n",
        "              -> derived ONLY from ML failure_prob\n",
        "        * Fusion:\n",
        "              -> prediction dominates alert_level / severity\n",
        "              -> detection still influences severity but has lower weight\n",
        "\n",
        "    - OUTPUT (unified decision packet):\n",
        "        {\n",
        "          \"timestamp\": ...\n",
        "          \"final_failure\": bool               # prediction-driven\n",
        "          \"final_anomaly\": bool               # sensor-driven\n",
        "          \"final_drift\": bool                 # sensor/window-driven\n",
        "          \"final_retrain\": bool               # retrain suggestion\n",
        "          \"alert_level\": \"NORMAL\"/\"LOW\"/\"MEDIUM\"/\"HIGH\"/\"CRITICAL\",\n",
        "          \"scores\": {\n",
        "              \"prediction_failure_prob\": float,\n",
        "              \"prediction_fused_score\": float,      # main score for alerting\n",
        "              \"sensor_anomaly_rate\": float,\n",
        "              \"sensor_drift_rate\": float,\n",
        "              \"sensor_retrain_rate\": float,\n",
        "              \"window_event_is_anomaly\": 0/1,\n",
        "              \"window_event_is_drift\": 0/1,\n",
        "              \"window_mse\": float or None,\n",
        "              \"prototype_score\": float or None,\n",
        "              \"transformer_prob\": float or None,\n",
        "          },\n",
        "          \"window_agent\": {\n",
        "              \"predicted_window\": int or None,\n",
        "              \"window_anomaly\": bool,\n",
        "              \"window_drift\": bool,\n",
        "          },\n",
        "          \"raw\": {\n",
        "              \"master_output\": ...,\n",
        "              \"window_output\": ...,\n",
        "              \"model_outputs\": ...,\n",
        "              \"metadata\": ...,\n",
        "          },\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        # weights for FUSION (prediction vs detection)\n",
        "        prediction_weight: float = 0.7,     # ML failure prediction dominates\n",
        "        sensor_weight: float = 0.2,         # sensor anomaly contribution\n",
        "        drift_weight: float = 0.1,          # drift contribution\n",
        "\n",
        "        # thresholds\n",
        "        failure_threshold: float = 0.5,     # base failure decision\n",
        "        failure_critical_threshold: float = 0.8,  # high-risk cutoff\n",
        "\n",
        "        anomaly_rate_threshold: float = 0.3,    # for final_anomaly\n",
        "        drift_rate_threshold: float = 0.2,      # for final_drift\n",
        "        retrain_rate_threshold: float = 0.15,   # for final_retrain\n",
        "    ):\n",
        "        # Fusion weights (normalised later)\n",
        "        self.prediction_weight = prediction_weight\n",
        "        self.sensor_weight = sensor_weight\n",
        "        self.drift_weight = drift_weight\n",
        "\n",
        "        self.failure_threshold = failure_threshold\n",
        "        self.failure_critical_threshold = failure_critical_threshold\n",
        "\n",
        "        self.anomaly_rate_threshold = anomaly_rate_threshold\n",
        "        self.drift_rate_threshold = drift_rate_threshold\n",
        "        self.retrain_rate_threshold = retrain_rate_threshold\n",
        "\n",
        "        # history buffer (optional)\n",
        "        self.history = []\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Main decision function\n",
        "    # -------------------------------------------------\n",
        "    def decide(\n",
        "        self,\n",
        "        master_output: Dict[str, Any],\n",
        "        window_output: Dict[str, Any],\n",
        "        model_outputs: Optional[Dict[str, Any]] = None,\n",
        "        metadata: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        timestamp = datetime.now()\n",
        "\n",
        "        # -----------------------------\n",
        "        # 1) Extract master/system info\n",
        "        # -----------------------------\n",
        "        sys_dec = master_output.get(\"system_decisions\", {}) if master_output else {}\n",
        "        sys_anomaly = bool(sys_dec.get(\"system_anomaly\", False))\n",
        "        sys_drift = bool(sys_dec.get(\"system_drift\", False))\n",
        "        sys_retrain = bool(sys_dec.get(\"system_needs_retrain\", False))\n",
        "\n",
        "        anomaly_rate = float(sys_dec.get(\"anomaly_rate\", 0.0))\n",
        "        drift_rate = float(sys_dec.get(\"drift_rate\", 0.0))\n",
        "        retrain_rate = float(sys_dec.get(\"retrain_rate\", 0.0))\n",
        "\n",
        "        # -----------------------------\n",
        "        # 2) Extract window agent info\n",
        "        # -----------------------------\n",
        "        window_event_type = window_output.get(\"event_type\", None) if window_output else None\n",
        "        win_anomaly = (window_event_type == \"ANOMALY\")\n",
        "        win_drift = (window_event_type == \"DRIFT\")\n",
        "\n",
        "        predicted_window = window_output.get(\"predicted_window\", None) if window_output else None\n",
        "        window_mse = None\n",
        "        if window_output and \"forecast_metrics\" in window_output:\n",
        "            window_mse = window_output[\"forecast_metrics\"].get(\"mse\", None)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 3) Extract ML prediction info\n",
        "        # -----------------------------\n",
        "        failure_prob = None\n",
        "        prototype_score = None\n",
        "        transformer_prob = None\n",
        "\n",
        "        if model_outputs is not None:\n",
        "            # failure prediction only – this is the ML “truth” in this layer\n",
        "            failure_prob = model_outputs.get(\"failure_prob\", None)\n",
        "            # keep these as diagnostic/extra info, NOT for anomaly\n",
        "            prototype_score = model_outputs.get(\"prototype_score\", None)\n",
        "            transformer_prob = model_outputs.get(\"transformer_prob\", None)\n",
        "\n",
        "        # Guard: normalise missing failure_prob -> 0.0\n",
        "        if failure_prob is None:\n",
        "            failure_prob = 0.0\n",
        "        failure_prob = float(failure_prob)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 4) Detection layer (ANOMALY / DRIFT / RETRAIN)\n",
        "        #    -> purely sensor + window based\n",
        "        # -----------------------------\n",
        "        # final_anomaly: system anomaly rate OR strong window anomaly\n",
        "        final_anomaly = (\n",
        "            anomaly_rate >= self.anomaly_rate_threshold\n",
        "            or sys_anomaly\n",
        "            or win_anomaly\n",
        "        )\n",
        "\n",
        "        # final_drift: drift rate OR window drift\n",
        "        final_drift = (\n",
        "            drift_rate >= self.drift_rate_threshold\n",
        "            or sys_drift\n",
        "            or win_drift\n",
        "        )\n",
        "\n",
        "        # final_retrain: explicit retrain + high retrain rate\n",
        "        final_retrain = (\n",
        "            sys_retrain\n",
        "            or retrain_rate >= self.retrain_rate_threshold\n",
        "            or (final_drift and anomaly_rate > 0.1)\n",
        "        )\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5) Prediction layer (FAILURE)\n",
        "        #    -> failure_prob from ML ONLY\n",
        "        # -----------------------------\n",
        "        # Base failure flag purely from probability\n",
        "        base_failure = failure_prob >= self.failure_threshold\n",
        "\n",
        "        # Context-aware adjustment:\n",
        "        # if failure_prob is moderate but sensor anomalies are strong,\n",
        "        # we can gently promote final_failure = True\n",
        "        strong_sensor_context = (\n",
        "            anomaly_rate > (self.anomaly_rate_threshold * 1.2)\n",
        "            or (win_anomaly and anomaly_rate > 0.1)\n",
        "        )\n",
        "\n",
        "        if not base_failure and failure_prob >= (self.failure_threshold * 0.7) and strong_sensor_context:\n",
        "            final_failure = True\n",
        "        else:\n",
        "            final_failure = base_failure\n",
        "\n",
        "        # -----------------------------\n",
        "        # 6) Fused severity / alert scoring\n",
        "        # -----------------------------\n",
        "        # normalise weights\n",
        "        w_sum = self.prediction_weight + self.sensor_weight + self.drift_weight\n",
        "        if w_sum <= 0:\n",
        "            w_pred = 1.0\n",
        "            w_sens = 0.0\n",
        "            w_drift = 0.0\n",
        "        else:\n",
        "            w_pred = self.prediction_weight / w_sum\n",
        "            w_sens = self.sensor_weight / w_sum\n",
        "            w_drift = self.drift_weight / w_sum\n",
        "\n",
        "        # detection “intensity” – bounded in [0,1]\n",
        "        detection_intensity = np.clip(anomaly_rate, 0.0, 1.0)\n",
        "        drift_intensity = np.clip(drift_rate, 0.0, 1.0)\n",
        "\n",
        "        # fused prediction-driven risk score\n",
        "        prediction_fused_score = (\n",
        "            w_pred * failure_prob +\n",
        "            w_sens * detection_intensity +\n",
        "            w_drift * drift_intensity +\n",
        "            (0.05 if win_anomaly else 0.0) +\n",
        "            (0.03 if win_drift else 0.0)\n",
        "        )\n",
        "        prediction_fused_score = float(np.clip(prediction_fused_score, 0.0, 1.0))\n",
        "\n",
        "        # -----------------------------\n",
        "        # 7) Map fused score + flags → alert_level\n",
        "        # -----------------------------\n",
        "        alert_level = self._compute_alert_level(\n",
        "            failure_prob=failure_prob,\n",
        "            fused_score=prediction_fused_score,\n",
        "            final_failure=final_failure,\n",
        "            final_anomaly=final_anomaly,\n",
        "            final_drift=final_drift,\n",
        "        )\n",
        "\n",
        "        # -----------------------------\n",
        "        # 8) Build decision packet\n",
        "        # -----------------------------\n",
        "        decision = {\n",
        "            \"timestamp\": timestamp.isoformat(),\n",
        "            # prediction-focused\n",
        "            \"final_failure\": bool(final_failure),\n",
        "            # detection-focused\n",
        "            \"final_anomaly\": bool(final_anomaly),\n",
        "            \"final_drift\": bool(final_drift),\n",
        "            \"final_retrain\": bool(final_retrain),\n",
        "            # human-facing\n",
        "            \"alert_level\": alert_level,\n",
        "            # structured scores\n",
        "            \"scores\": {\n",
        "                # Prediction side\n",
        "                \"prediction_failure_prob\": float(failure_prob),\n",
        "                \"prediction_fused_score\": float(prediction_fused_score),\n",
        "\n",
        "                # Detection side\n",
        "                \"sensor_anomaly_rate\": float(anomaly_rate),\n",
        "                \"sensor_drift_rate\": float(drift_rate),\n",
        "                \"sensor_retrain_rate\": float(retrain_rate),\n",
        "\n",
        "                # Window side\n",
        "                \"window_event_type\": window_event_type,\n",
        "                \"window_event_is_anomaly\": 1.0 if win_anomaly else 0.0,\n",
        "                \"window_event_is_drift\": 1.0 if win_drift else 0.0,\n",
        "                \"window_mse\": float(window_mse) if window_mse is not None else None,\n",
        "\n",
        "                # Extra ML diagnostics (not used for detection)\n",
        "                \"prototype_score\": float(prototype_score) if prototype_score is not None else None,\n",
        "                \"transformer_prob\": float(transformer_prob) if transformer_prob is not None else None,\n",
        "            },\n",
        "            \"window_agent\": {\n",
        "                \"predicted_window\": predicted_window,\n",
        "                \"window_anomaly\": bool(win_anomaly),\n",
        "                \"window_drift\": bool(win_drift),\n",
        "            },\n",
        "            \"raw\": {\n",
        "                \"master_output\": master_output,\n",
        "                \"window_output\": window_output,\n",
        "                \"model_outputs\": model_outputs,\n",
        "                \"metadata\": metadata,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        self.history.append(decision)\n",
        "        return decision\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Helper: alert level mapping\n",
        "    # -------------------------------------------------\n",
        "    def _compute_alert_level(\n",
        "        self,\n",
        "        failure_prob: float,\n",
        "        fused_score: float,\n",
        "        final_failure: bool,\n",
        "        final_anomaly: bool,\n",
        "        final_drift: bool,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Map scores + flags into a discrete alert level.\n",
        "        ML failure prediction dominates; sensor anomalies modulate.\n",
        "        \"\"\"\n",
        "        # Hard overrides for critical cases\n",
        "        if failure_prob >= self.failure_critical_threshold and (final_anomaly or final_drift):\n",
        "            return \"CRITICAL\"\n",
        "\n",
        "        if final_failure and fused_score >= 0.9:\n",
        "            return \"CRITICAL\"\n",
        "\n",
        "        # General mapping based on fused_score and flags\n",
        "        if not final_failure and not final_anomaly and not final_drift:\n",
        "            if fused_score < 0.3:\n",
        "                return \"NORMAL\"\n",
        "            if fused_score < 0.4:\n",
        "                return \"LOW\"\n",
        "\n",
        "        # moderate region\n",
        "        if fused_score < 0.5:\n",
        "            return \"LOW\"\n",
        "        if fused_score < 0.7:\n",
        "            return \"MEDIUM\"\n",
        "        if fused_score < 0.9:\n",
        "            return \"HIGH\"\n",
        "\n",
        "        return \"CRITICAL\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# UPGRADED EXPERT AGENT (SOTA-STYLE FOR METROPT)\n",
        "# =====================================================\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, Any, List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExpertAgent:\n",
        "    \"\"\"\n",
        "    ExpertAgent (Upgraded):\n",
        "      - Invoked when DecisionAgent flags an anomaly (or when you choose).\n",
        "      - Uses rich context:\n",
        "          * decision_packet (DecisionAgent output)\n",
        "          * raw master_output (sensor-level results)\n",
        "          * raw window_output (AdaptiveWindowAgent)\n",
        "          * prototype scores, etc. via model_outputs in decision_packet\n",
        "          * recent multivariate sensor window (numeric snapshot)\n",
        "          * recent history from EventStore\n",
        "          * static domain knowledge (sensor metadata, fault patterns)\n",
        "      - Calls an LLM to produce:\n",
        "          * summary\n",
        "          * explanation\n",
        "          * likely_fault\n",
        "          * recommended_action\n",
        "          * severity\n",
        "      - Validates JSON and falls back gracefully on errors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        event_store,\n",
        "        system_description: str = \"MetroPT Air Production Unit (APU)\",\n",
        "        llm_client: Optional[object] = None,\n",
        "        history_limit: int = 100,\n",
        "        max_history_for_prompt: int = 10,\n",
        "        window_preview_len: int = 10,\n",
        "        sensor_metadata: Optional[Dict[str, str]] = None,\n",
        "        fault_knowledge: Optional[List[str]] = None,\n",
        "        max_retries: int = 2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        event_store         : EventStore instance (for past decisions)\n",
        "        system_description  : short description of the system\n",
        "        llm_client          : OpenAI client (OpenAI())\n",
        "        history_limit       : how many past events to fetch from DB\n",
        "        max_history_for_prompt : how many to actually show to LLM\n",
        "        window_preview_len  : last N timesteps per sensor to show\n",
        "        sensor_metadata     : mapping sensor_index -> human-readable name\n",
        "        fault_knowledge     : list of domain fault patterns (strings)\n",
        "        max_retries         : number of LLM retry attempts on failure\n",
        "        \"\"\"\n",
        "        self.store = event_store\n",
        "        self.system_description = system_description\n",
        "        self.llm_client = llm_client\n",
        "        self.history_limit = history_limit\n",
        "        self.max_history_for_prompt = max_history_for_prompt\n",
        "        self.window_preview_len = window_preview_len\n",
        "        self.max_retries = max_retries\n",
        "\n",
        "        # Default MetroPT-style sensor metadata (can be overridden)\n",
        "        if sensor_metadata is None:\n",
        "            self.sensor_metadata = {\n",
        "                0: \"TP2 (bar): the measure of the pressure on the compressor\",\n",
        "                1: \"TP3 (bar): the measure of the pressure generated at the pneumatic panel\",\n",
        "                2: \"H1 (bar) – the measure of the pressure generated due to pressure drop when the discharge of the cyclonic separator filter occurs\",\n",
        "                3: \"DV pressure (bar): the measure of the pressure drop generated when the towers discharge air dryers; a zero reading indicates that the compressor is operating under load\",\n",
        "                4: \"Reservoirs (bar): the measure of the downstream pressure of the reservoirs, which should be close to the pneumatic panel pressure (TP3)\",\n",
        "                5: \"Oil Temperature (ºC) :  the measure of the oil temperature on the compressor\",\n",
        "                6: \"Motor Current (A) –  the measure of the current of one phase of the three-phase motor; it presents values close to 0A - when it turns off, 4A - when working offloaded, 7A - when working under load, and 9A - when it starts working\",\n",
        "                7: \"COMP - the electrical signal of the air intake valve on the compressor; it is active when there is no air intake, indicating that the compressor is either turned off or operating in an offloaded state.\",\n",
        "                8: \"DV electric – the electrical signal that controls the compressor outlet valve; it is active when the compressor is functioning under load and inactive when the compressor is either off or operating in an offloaded state.\",\n",
        "                9: \"TOWERS – the electrical signal that defines the tower responsible for drying the air and the tower responsible for draining the humidity removed from the air; when not active, it indicates that tower one is functioning; when active, it indicates that tower two is in operation.\",\n",
        "                10: \"MPG – the electrical signal responsible for starting the compressor under load by activating the intake valve when the pressure in the air production unit (APU) falls below 8.2 bar; it activates the COMP sensor, which assumes the same behaviour as the MPG sensor\",\n",
        "                11: \"LPS – the electrical signal that detects and activates when the pressure drops below 7 bars\",\n",
        "\n",
        "            }\n",
        "        else:\n",
        "            self.sensor_metadata = sensor_metadata\n",
        "\n",
        "        # Default MetroPT-style fault patterns (can be overridden)\n",
        "        if fault_knowledge is None:\n",
        "            self.fault_knowledge = [\n",
        "                \"Air Leak: gradual pressure decay + increased compressor runtime.\",\n",
        "                \"Blockage: oscillatory or unstable pressure + abnormal valve cycling.\",\n",
        "                \"Overheating: rising oil temperature + increased motor current.\",\n",
        "                \"Valve Stuck: valve digital state frozen while pressure behaviour is abnormal.\",\n",
        "                \"Short Cycling: frequent compressor start/stop in short intervals.\",\n",
        "                \"Sensor Failure: flat-line, impossible values, or inconsistent readings.\",\n",
        "            ]\n",
        "        else:\n",
        "            self.fault_knowledge = fault_knowledge\n",
        "\n",
        "    # =====================================================\n",
        "    # PUBLIC ENTRYPOINT\n",
        "    # =====================================================\n",
        "    def analyse_anomaly(\n",
        "        self,\n",
        "        decision_packet: dict,\n",
        "        system_subsequence: np.ndarray,\n",
        "        extra_context: Optional[dict] = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Main entry point:\n",
        "          - decision_packet: from DecisionAgent.decide(...)\n",
        "          - system_subsequence: [window_length, num_sensors] np.ndarray\n",
        "          - extra_context: anything else (index, timestamps, etc.)\n",
        "\n",
        "        Returns:\n",
        "          expert_packet dict with:\n",
        "            - timestamp\n",
        "            - decision_packet\n",
        "            - prompt_used\n",
        "            - llm_result (JSON from model or fallback)\n",
        "        \"\"\"\n",
        "        # 1) Fetch recent past decisions from EventStore\n",
        "        recent_raw_decisions = self.store.fetch_recent(limit=self.history_limit)\n",
        "\n",
        "        # 2) Build rich prompt\n",
        "        prompt = self._build_prompt(\n",
        "            decision_packet=decision_packet,\n",
        "            system_subsequence=system_subsequence,\n",
        "            extra_context=extra_context,\n",
        "            recent_events=recent_raw_decisions,\n",
        "        )\n",
        "\n",
        "        # 3) Call LLM with JSON-only contract\n",
        "        llm_result = self._call_llm_with_json(prompt)\n",
        "\n",
        "        expert_packet = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"decision_packet\": decision_packet,\n",
        "            \"prompt_used\": prompt,\n",
        "            \"llm_result\": llm_result,\n",
        "        }\n",
        "\n",
        "        return expert_packet\n",
        "\n",
        "    # =====================================================\n",
        "    # PROMPT CONSTRUCTION\n",
        "    # =====================================================\n",
        "    def _build_prompt(\n",
        "        self,\n",
        "        decision_packet: dict,\n",
        "        system_subsequence: np.ndarray,\n",
        "        extra_context: Optional[dict],\n",
        "        recent_events: List[dict],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Build a rich natural-language + structured prompt for the LLM.\n",
        "        Includes:\n",
        "          - system description\n",
        "          - sensor metadata\n",
        "          - known fault patterns\n",
        "          - current decision scores + flags\n",
        "          - per-sensor anomaly info (if available)\n",
        "          - small numeric snapshot of current window\n",
        "          - compressed recent history\n",
        "          - explicit JSON response schema\n",
        "        \"\"\"\n",
        "\n",
        "        # -------------------------------\n",
        "        # 1) Extract top-level scores\n",
        "        # -------------------------------\n",
        "        scores = decision_packet.get(\"scores\", {})\n",
        "        window_info = decision_packet.get(\"window_agent\", {})\n",
        "        alert_level = decision_packet.get(\"alert_level\", \"UNKNOWN\")\n",
        "        final_anomaly = decision_packet.get(\"final_anomaly\", False)\n",
        "        final_drift = decision_packet.get(\"final_drift\", False)\n",
        "        final_retrain = decision_packet.get(\"final_retrain\", False)\n",
        "\n",
        "        # -------------------------------\n",
        "        # 2) Extract raw master/window/model data (if available)\n",
        "        # -------------------------------\n",
        "        raw_block = decision_packet.get(\"raw\", {})\n",
        "        master_output = raw_block.get(\"master_output\", None)\n",
        "        window_output = raw_block.get(\"window_output\", None)\n",
        "        model_outputs = raw_block.get(\"model_outputs\", {})\n",
        "        metadata = raw_block.get(\"metadata\", {})\n",
        "\n",
        "        prototype_score = model_outputs.get(\"prototype_score\", None)\n",
        "        transformer_prob = model_outputs.get(\"transformer_prob\", None)\n",
        "\n",
        "        # -------------------------------\n",
        "        # 3) Per-sensor anomaly stats from master_output\n",
        "        # -------------------------------\n",
        "        per_sensor_summary = []\n",
        "        if master_output is not None:\n",
        "            sensor_results = master_output.get(\"sensor_results\", [])\n",
        "            for i, res in enumerate(sensor_results):\n",
        "                name = self.sensor_metadata.get(i, f\"Sensor_{i}\")\n",
        "                per_sensor_summary.append({\n",
        "                    \"sensor_index\": i,\n",
        "                    \"name\": name,\n",
        "                    \"is_anomaly\": bool(res.get(\"is_anomaly\", False)),\n",
        "                    \"drift_flag\": bool(res.get(\"drift_flag\", False)),\n",
        "                    \"needs_retrain\": bool(res.get(\"needs_retrain_flag\", False)),\n",
        "                    \"anomaly_score\": float(res.get(\"anomaly_score\", 0.0)),\n",
        "                    \"confidence\": float(res.get(\"confidence\", 0.0)),\n",
        "                })\n",
        "\n",
        "        # -------------------------------\n",
        "        # 4) Numeric snapshot of current window\n",
        "        # -------------------------------\n",
        "        # system_subsequence: [T, F]\n",
        "        try:\n",
        "            seq = np.asarray(system_subsequence)\n",
        "            T, F = seq.shape\n",
        "        except Exception:\n",
        "            seq = np.array(system_subsequence)\n",
        "            if seq.ndim == 1:\n",
        "                seq = seq.reshape(-1, 1)\n",
        "            T, F = seq.shape\n",
        "\n",
        "        preview_len = min(self.window_preview_len, T)\n",
        "        window_preview = seq[-preview_len:]  # shape [preview_len, F]\n",
        "\n",
        "        # Represent as {sensor_name: [values...]}\n",
        "        sensor_window_dict = {}\n",
        "        for j in range(F):\n",
        "            name = self.sensor_metadata.get(j, f\"Sensor_{j}\")\n",
        "            sensor_window_dict[name] = window_preview[:, j].round(4).tolist()\n",
        "\n",
        "        # -------------------------------\n",
        "        # 5) Compressed recent history\n",
        "        # -------------------------------\n",
        "        history_summaries = []\n",
        "        for ev in recent_events[: self.max_history_for_prompt]:\n",
        "            try:\n",
        "                # ev is a past decision_packet (because we stored 'packet=decision_packet')\n",
        "                scores_ev = ev.get(\"scores\", {})\n",
        "                history_summaries.append({\n",
        "                    \"timestamp\": ev.get(\"timestamp\", \"\"),\n",
        "                    \"alert_level\": ev.get(\"alert_level\", \"UNKNOWN\"),\n",
        "                    \"anomaly_score\": scores_ev.get(\"anomaly_score\", None),\n",
        "                    \"drift_score\": scores_ev.get(\"drift_score\", None),\n",
        "                })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # -------------------------------\n",
        "        # 6) Sensor metadata & fault patterns as text\n",
        "        # -------------------------------\n",
        "        sensor_meta_list = [f\"{idx}: {desc}\" for idx, desc in self.sensor_metadata.items()]\n",
        "\n",
        "        fault_knowledge_text = \"\\n\".join(\n",
        "            [f\"- {fk}\" for fk in self.fault_knowledge]\n",
        "        )\n",
        "\n",
        "        # -------------------------------\n",
        "        # 7) Build final instruction with JSON schema\n",
        "        # -------------------------------\n",
        "        schema_instruction = \"\"\"\n",
        "You MUST respond with ONLY a single valid JSON object, no extra text.\n",
        "The JSON MUST have exactly the following keys at the top level:\n",
        "\n",
        "- \"summary\": short 1-2 sentence description of what is happening.\n",
        "- \"explanation\": 2-6 sentences, clear reasoning in industrial / physical terms.\n",
        "- \"likely_fault\": short label of the most likely fault type, or \"Unknown\".\n",
        "- \"recommended_action\": one of:\n",
        "    - \"IGNORE\"\n",
        "    - \"MONITOR\"\n",
        "    - \"ACK_AND_INVESTIGATE\"\n",
        "    - \"SCHEDULE_MAINTENANCE\"\n",
        "    - \"IMMEDIATE_SHUTDOWN\"\n",
        "- \"severity\": one of: \"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\".\n",
        "\n",
        "Do NOT include any extra keys outside these five.\n",
        "Do NOT include any surrounding text, markdown, or commentary.\n",
        "Return ONLY the JSON object.\n",
        "\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an expert industrial fault diagnosis assistant for: {self.system_description}.\n",
        "\n",
        "System context:\n",
        "- This is a compressed-air production unit (APU) of a metro train.\n",
        "- Sensors include analog (pressure, current, temperature) and digital (valves, states).\n",
        "\n",
        "Sensor metadata (index -> description):\n",
        "{json.dumps(sensor_meta_list, indent=2)}\n",
        "\n",
        "Known fault patterns (domain knowledge):\n",
        "{fault_knowledge_text}\n",
        "\n",
        "CURRENT DECISION PACKET:\n",
        "\n",
        "- Alert level: {alert_level}\n",
        "\n",
        "PREDICTION SIGNALS (from ML failure model):\n",
        "- final_failure (prediction-based): {decision_packet.get(\"final_failure\")}\n",
        "- prediction_failure_prob: {scores.get(\"prediction_failure_prob\")}\n",
        "- prediction_fused_score: {scores.get(\"prediction_fused_score\")}\n",
        "\n",
        "DETECTION SIGNALS (from sensor & window agents):\n",
        "- final_anomaly (sensor-based): {final_anomaly}\n",
        "- final_drift (sensor/window-based): {final_drift}\n",
        "- final_retrain: {final_retrain}\n",
        "\n",
        "All prediction and detection signals are separate:\n",
        "- Prediction = future failure likelihood (ML)\n",
        "- Detection = current abnormalities (sensor + drift + window)\n",
        "\n",
        "Window agent info:\n",
        "{json.dumps(window_info)}\n",
        "\n",
        "Prototype / transformer outputs (if any):\n",
        "- prototype_score: {prototype_score}\n",
        "- transformer_prob: {transformer_prob}\n",
        "\n",
        "Per-sensor anomaly summary:\n",
        "{json.dumps(per_sensor_summary, indent=2)}\n",
        "\n",
        "Current sensor window snapshot (last {preview_len} timesteps for each sensor):\n",
        "{json.dumps(sensor_window_dict, indent=2)}\n",
        "\n",
        "Recent history of decisions (compressed):\n",
        "{json.dumps(history_summaries, indent=2)}\n",
        "\n",
        "Extra context:\n",
        "{json.dumps(extra_context, default=str)}\n",
        "\n",
        "Your tasks:\n",
        "1. Decide whether this anomaly is likely REAL or a FALSE POSITIVE.\n",
        "2. Infer which fault pattern (if any) best matches the evidence.\n",
        "3. Explain the reasoning in terms of sensor behaviour (pressure, current, valves, temperature).\n",
        "4. Recommend the next action for the human operator, considering safety and cost.\n",
        "5. Assign a severity level: LOW, MEDIUM, HIGH, or CRITICAL.\n",
        "\n",
        "{schema_instruction}\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    # =====================================================\n",
        "    # LLM CALL + JSON HANDLING\n",
        "    # =====================================================\n",
        "    def _call_llm_with_json(self, prompt: str) -> dict:\n",
        "        \"\"\"\n",
        "        Calls the LLM via self.llm_client and returns a validated JSON dict.\n",
        "        Uses:\n",
        "          - prompt-based JSON contract\n",
        "          - retry with minimal fallback\n",
        "          - schema validation\n",
        "        \"\"\"\n",
        "\n",
        "        # If no client configured, return fallback\n",
        "        if self.llm_client is None:\n",
        "            return {\n",
        "                \"summary\": \"Anomaly detected (fallback, no LLM client configured).\",\n",
        "                \"explanation\": \"ExpertAgent has no LLM client; returning default recommendation.\",\n",
        "                \"likely_fault\": \"Unknown\",\n",
        "                \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "                \"severity\": \"MEDIUM\",\n",
        "            }\n",
        "\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = self.llm_client.responses.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    input=prompt,\n",
        "                    max_output_tokens=400,\n",
        "                )\n",
        "\n",
        "                # Using unified Responses API: easiest is output_text\n",
        "                raw_text = getattr(response, \"output_text\", None)\n",
        "                if raw_text is None:\n",
        "                    # fallback to explicit extraction\n",
        "                    raw_text = response.output[0].content[0].text\n",
        "\n",
        "                parsed = self._robust_json_parse(raw_text)\n",
        "                validated = self._validate_llm_json(parsed)\n",
        "                return validated\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ ExpertAgent LLM error (attempt {attempt+1}): {e}\")\n",
        "                last_error = e\n",
        "\n",
        "        # Final fallback if everything fails\n",
        "        return {\n",
        "            \"summary\": \"Anomaly detected (LLM fallback).\",\n",
        "            \"explanation\": f\"LLM failed or returned invalid JSON. Last error: {last_error}\",\n",
        "            \"likely_fault\": \"Unknown\",\n",
        "            \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "            \"severity\": \"MEDIUM\",\n",
        "        }\n",
        "\n",
        "    def _robust_json_parse(self, text: str) -> dict:\n",
        "        \"\"\"\n",
        "        Best-effort JSON parsing:\n",
        "          - First try direct json.loads\n",
        "          - If fails, try to extract the first {...} block\n",
        "        \"\"\"\n",
        "        text = text.strip()\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except Exception:\n",
        "            # Try to find a JSON object substring\n",
        "            start = text.find(\"{\")\n",
        "            end = text.rfind(\"}\")\n",
        "            if start != -1 and end != -1 and end > start:\n",
        "                snippet = text[start : end + 1]\n",
        "                return json.loads(snippet)\n",
        "            # If still failing, raise\n",
        "            raise\n",
        "\n",
        "    def _validate_llm_json(self, obj: Any) -> dict:\n",
        "        \"\"\"\n",
        "        Enforce a minimal schema:\n",
        "          - must be a dict\n",
        "          - must contain keys: summary, explanation, likely_fault, recommended_action, severity\n",
        "        If keys are missing, fill with defaults.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(obj, dict):\n",
        "            raise ValueError(\"LLM output is not a JSON object\")\n",
        "\n",
        "        required_keys = [\"summary\", \"explanation\", \"likely_fault\", \"recommended_action\", \"severity\"]\n",
        "        defaults = {\n",
        "            \"summary\": \"No summary provided by LLM.\",\n",
        "            \"explanation\": \"No explanation provided by LLM.\",\n",
        "            \"likely_fault\": \"Unknown\",\n",
        "            \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "            \"severity\": \"MEDIUM\",\n",
        "        }\n",
        "\n",
        "        cleaned = {}\n",
        "        for k in required_keys:\n",
        "            v = obj.get(k, defaults[k])\n",
        "            # ensure string\n",
        "            cleaned[k] = str(v)\n",
        "\n",
        "        # Optionally normalise recommended_action/severity to known values\n",
        "        cleaned[\"recommended_action\"] = cleaned[\"recommended_action\"].upper().strip()\n",
        "        cleaned[\"severity\"] = cleaned[\"severity\"].upper().strip()\n",
        "\n",
        "        # Clamp to allowed sets if desired\n",
        "        allowed_actions = {\n",
        "            \"IGNORE\",\n",
        "            \"MONITOR\",\n",
        "            \"ACK_AND_INVESTIGATE\",\n",
        "            \"SCHEDULE_MAINTENANCE\",\n",
        "            \"IMMEDIATE_SHUTDOWN\",\n",
        "        }\n",
        "        if cleaned[\"recommended_action\"] not in allowed_actions:\n",
        "            cleaned[\"recommended_action\"] = \"ACK_AND_INVESTIGATE\"\n",
        "\n",
        "        allowed_severity = {\"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\"}\n",
        "        if cleaned[\"severity\"] not in allowed_severity:\n",
        "            cleaned[\"severity\"] = \"MEDIUM\"\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SIMPLE HUMAN LOOP + ALERT STUB\n",
        "# =====================================================\n",
        "\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "\n",
        "def send_alert_to_human(expert_packet: dict):\n",
        "    \"\"\"\n",
        "    Sends an email alert to a human operator using SMTP.\n",
        "    Includes summary, recommended action, severity, and JSON dump.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Extract ExpertAgent output\n",
        "    # ----------------------\n",
        "    result = expert_packet.get(\"llm_result\", {})\n",
        "    summary = result.get(\"summary\", \"No summary\")\n",
        "    action = result.get(\"recommended_action\", \"N/A\")\n",
        "    severity = result.get(\"severity\", \"N/A\")\n",
        "\n",
        "    decision_packet_json = json.dumps(\n",
        "        expert_packet.get(\"decision_packet\", {}),\n",
        "        indent=2\n",
        "    )\n",
        "    llm_json = json.dumps(result, indent=2)\n",
        "\n",
        "    # ----------------------\n",
        "    # Email content\n",
        "    # ----------------------\n",
        "    subject = f\"⚠️ APU Alert – Severity: {severity}\"\n",
        "\n",
        "    body = f\"\"\"\n",
        "Human Operator,\n",
        "\n",
        "An alert has been generated by the APU Monitoring System.\n",
        "\n",
        "-------------------\n",
        "SUMMARY\n",
        "-------------------\n",
        "{summary}\n",
        "\n",
        "-------------------\n",
        "RECOMMENDED ACTION\n",
        "-------------------\n",
        "{action}\n",
        "\n",
        "-------------------\n",
        "SEVERITY\n",
        "-------------------\n",
        "{severity}\n",
        "\n",
        "-------------------\n",
        "FULL LLM RESULT\n",
        "-------------------\n",
        "{llm_json}\n",
        "\n",
        "-------------------\n",
        "RAW DECISION PACKET\n",
        "-------------------\n",
        "{decision_packet_json}\n",
        "\n",
        "Timestamp: {expert_packet.get('timestamp')}\n",
        "\"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Create email object\n",
        "    # ----------------------\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = EMAIL_SENDER\n",
        "    msg[\"To\"] = EMAIL_RECIPIENT\n",
        "    msg[\"Subject\"] = subject\n",
        "\n",
        "    msg.attach(MIMEText(body, \"plain\"))\n",
        "\n",
        "    # ----------------------\n",
        "    # Send email via SMTP\n",
        "    # ----------------------\n",
        "    try:\n",
        "        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
        "        server.starttls()\n",
        "        server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
        "        server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, msg.as_string())\n",
        "        server.quit()\n",
        "\n",
        "        print(\"\\n=== EMAIL ALERT SENT SUCCESSFULLY ===\")\n",
        "        print(f\"To: {EMAIL_RECIPIENT}\")\n",
        "        print(f\"Summary: {summary}\")\n",
        "        print(\"======================================\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to send alert email:\", e)\n",
        "\n",
        "\n",
        "\n",
        "def get_human_feedback_stub(expert_packet: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Stub: in a real deployment, this would come from UI / operator.\n",
        "    Here we just echo back a synthetic 'ACK'.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"final_decision\": \"ACK_AND_LOG\",\n",
        "        \"notes\": \"Stub human feedback – replace with real operator input.\",\n",
        "    }\n",
        "##################################################\n",
        "#GROUND TRUTH ANOMALY VS WHICH AGENT AGENT IS RIGHT\n",
        "############################################################\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_agent_vs_groundtruth(\n",
        "    results,\n",
        "    detection_labels=None,\n",
        "    h1_labels=None,\n",
        "    h3_labels=None,\n",
        "    h12_labels=None,\n",
        "    max_samples=200\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot comparison between agent decisions, ground-truth labels,\n",
        "    coordinator outputs, and AdaptiveWindowAgent predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    n = min(max_samples, len(results))\n",
        "    t = np.arange(n)\n",
        "\n",
        "    # Extract coordinator anomaly/drift\n",
        "    agent_anomaly = [1 if r[\"coordinator\"][\"final_anomaly\"] else 0 for r in results[:n]]\n",
        "    agent_drift   = [1 if r[\"coordinator\"][\"final_drift\"] else 0 for r in results[:n]]\n",
        "\n",
        "    # New: extract ML prediction\n",
        "    failure_prob = [\n",
        "        results[i][\"coordinator\"][\"scores\"].get(\"prediction_failure_prob\", 0.0)\n",
        "        for i in range(n)\n",
        "    ]\n",
        "    final_failure = [\n",
        "        1 if results[i][\"coordinator\"].get(\"final_failure\") else 0\n",
        "        for i in range(n)\n",
        "    ]\n",
        "\n",
        "    # Ground-truth labels\n",
        "    det = detection_labels[:n] if detection_labels is not None else None\n",
        "    h1  = h1_labels[:n] if h1_labels is not None else None\n",
        "    h3  = h3_labels[:n] if h3_labels is not None else None\n",
        "    h12 = h12_labels[:n] if h12_labels is not None else None\n",
        "\n",
        "    # Window agent outputs\n",
        "    window_sizes  = [r[\"window\"][\"predicted_window\"] for r in results[:n]]\n",
        "    window_events = [r[\"window\"].get(\"event_type\") for r in results[:n]]\n",
        "\n",
        "    # ---- Create subplots ----\n",
        "    fig, axes = plt.subplots(6, 1, figsize=(15, 17), sharex=True)\n",
        "\n",
        "    # Agent outputs\n",
        "    axes[0].plot(t, agent_anomaly, label=\"Agent Anomaly\", color=\"red\")\n",
        "    axes[0].plot(t, agent_drift,   label=\"Agent Drift\", color=\"orange\")\n",
        "    axes[0].set_ylabel(\"Agent\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Detection labels\n",
        "    if det is not None:\n",
        "        axes[1].plot(t, det, label=\"Detection Labels\", color=\"blue\")\n",
        "        axes[1].set_ylabel(\"Detect\")\n",
        "        axes[1].legend()\n",
        "\n",
        "    # Prediction labels (1h, 3h, 12h)\n",
        "    if h1 is not None:\n",
        "        axes[2].plot(t, h1, label=\"H1\", color=\"green\")\n",
        "    if h3 is not None:\n",
        "        axes[2].plot(t, h3, label=\"H3\", color=\"purple\")\n",
        "    if h12 is not None:\n",
        "        axes[2].plot(t, h12, label=\"H12\", color=\"brown\")\n",
        "    axes[2].set_ylabel(\"Prediction\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    # Coordinator alert level\n",
        "    alert_map = {\"NORMAL\": 0, \"MEDIUM\": 1, \"HIGH\": 2, \"CRITICAL\": 3}\n",
        "    alerts = [alert_map.get(r[\"coordinator\"][\"alert_level\"], 0) for r in results[:n]]\n",
        "    axes[3].plot(t, alerts, label=\"Alert Level\", color=\"black\")\n",
        "    axes[3].set_yticks([0,1,2,3])\n",
        "    axes[3].set_yticklabels([\"NORMAL\",\"MED\",\"HIGH\",\"CRIT\"])\n",
        "    axes[3].set_ylabel(\"Coordinator\")\n",
        "    axes[3].legend()\n",
        "\n",
        "    # AdaptiveWindowAgent subplot\n",
        "    axes[4].plot(t, window_sizes, label=\"Predicted Window\", color=\"blue\")\n",
        "\n",
        "    # Mark drift/anomaly events\n",
        "    for i, evt in enumerate(window_events):\n",
        "        if evt == \"DRIFT\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"orange\", marker=\"x\", label=\"Window Drift\" if i==0 else \"\")\n",
        "        elif evt == \"ANOMALY\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"red\", marker=\"o\", label=\"Window Anomaly\" if i==0 else \"\")\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "    # ⭐ Row 6: ML Failure Prediction Probability\n",
        "    # ----------------------------------------------------\n",
        "    axes[5].plot(t, failure_prob, label=\"Failure Prob (ML)\", color=\"green\")\n",
        "    axes[5].plot(t, final_failure, label=\"Final Failure Decision\", color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    axes[5].set_ylabel(\"Fail Prob\")\n",
        "    axes[5].set_ylim([-0.1, 1.1])\n",
        "    axes[5].legend()\n",
        "\n",
        "    # Overlay ground-truth fault events (vertical lines)\n",
        "    if det is not None:\n",
        "        for i, val in enumerate(det):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Fault (Detection)\" if i==0 else \"\")\n",
        "    if h1 is not None:\n",
        "        for i, val in enumerate(h1):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"green\", linestyle=\"--\", alpha=0.2, label=\"Fault (H1)\" if i==0 else \"\")\n",
        "    if h3 is not None:\n",
        "        for i, val in enumerate(h3):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"purple\", linestyle=\"--\", alpha=0.2, label=\"Fault (H3)\" if i==0 else \"\")\n",
        "    if h12 is not None:\n",
        "        for i, val in enumerate(h12):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"brown\", linestyle=\"--\", alpha=0.2, label=\"Fault (H12)\" if i==0 else \"\")\n",
        "\n",
        "    axes[4].set_ylabel(\"Window Size\")\n",
        "    axes[4].legend()\n",
        "\n",
        "    axes[-1].set_xlabel(\"Sample index\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#=========================================================================================================================================\n",
        "#================= TESTING MAIN CODE========================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#############################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 1. LOAD DATA + MASK + LABELS\n",
        "    # -------------------------------------------------\n",
        "data_path       = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy\"\n",
        "label_path      = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/window_labels_3class.npy\"\n",
        "train_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/train_mask.npy\"\n",
        "test_mask_path  = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/test_mask.npy\"\n",
        "holdout_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/holdout_mask.npy\"\n",
        "\n",
        "X = np.load(data_path)        # (N, W, S)\n",
        "y = np.load(label_path)       # (N,) ∈ {0,1,2}\n",
        "\n",
        "train_mask   = np.load(train_mask_path).astype(bool)\n",
        "test_mask    = np.load(test_mask_path).astype(bool)\n",
        "holdout_mask = np.load(holdout_mask_path).astype(bool)\n",
        "\n",
        "# Sanity check (VERY important) -- This assertion enforces strict mutual exclusivity between evaluation and holdout subsets, preventing data leakage and ensuring unbiased performance estimation.\n",
        "assert not np.any(train_mask & test_mask)\n",
        "assert not np.any(train_mask & holdout_mask)\n",
        "assert not np.any(test_mask & holdout_mask)\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "X_hold,  y_hold  = X[holdout_mask], y[holdout_mask]\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "print(\"Hold :\", X_hold.shape)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# -------------TRansformer for 3-class classification - NO CONTRASTIVE LEARNING\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "def transformer_encoder(x, head_size, num_heads, ff_dim, dropout=0.2):\n",
        "    # --- Self-attention block ---\n",
        "    attn_output = layers.MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout\n",
        "    )(x, x)\n",
        "\n",
        "    attn_output = layers.Dropout(dropout)(attn_output)\n",
        "    x = layers.Add()([x, attn_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # --- Feed-forward block ---\n",
        "    ff_output = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff_output = layers.Dropout(dropout)(ff_output)\n",
        "    ff_output = layers.Dense(x.shape[-1])(ff_output)\n",
        "\n",
        "    x = layers.Add()([x, ff_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_transformer(window_len, n_features, n_classes=3):\n",
        "    inputs = keras.Input(shape=(window_len, n_features))\n",
        "\n",
        "    # Project features to model dimension\n",
        "    x = layers.Dense(64)(inputs)\n",
        "\n",
        "    # Transformer blocks\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "\n",
        "    # Pooling + classifier\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "window_len  = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "n_classes = 3\n",
        "model = build_transformer(window_len, n_features, n_classes)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.SparseTopKCategoricalAccuracy(k=2, name=\"top2_acc\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Class weights (IMBALANCE HANDLING)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "class_weight = {\n",
        "    i: w for i, w in zip(np.unique(y_train), class_weights)\n",
        "}\n",
        "\n",
        "print(\"Class weights:\", class_weight)\n",
        "model.summary()\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "#####------------------------Train and Save----------------------------\n",
        "classifier = model\n",
        "history = classifier.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),   # TEST = offline validation\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "classifier.save(\n",
        "    os.path.join(save_dir, \"transformer_classifier.keras\"),\n",
        "    include_optimizer=False   # important for clean reload\n",
        ")\n",
        "\n",
        "print(\"✅ Transformer classifier saved\")\n",
        "\n",
        "####-----------------------------Load and test-----------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        modulating_factor = K.pow(1.0 - p_t, gamma)\n",
        "\n",
        "        return K.mean(alpha_factor * modulating_factor * bce)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class/transformer_classifier.keras\"\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "classifier = load_model(\n",
        "    model_path,\n",
        "    custom_objects={\"focal_loss\": focal_loss}  # only if you actually used it\n",
        ")\n",
        "\n",
        "classifier.trainable = False  # safety\n",
        "\n",
        "\n",
        "print(\"✅ Transformer loaded for TEST\")\n",
        "probs_test = classifier.predict(X_test)\n",
        "y_pred_test = np.argmax(probs_test, axis=1)\n",
        "\n",
        "print(\"\\n=== TEST SET PERFORMANCE ===\")\n",
        "print(classification_report(y_test, y_pred_test, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "###-------------------------HOLDOUT-------------------\n",
        "############################################################\n",
        "probs_hold = classifier.predict(X_hold)\n",
        "y_pred_hold = np.argmax(probs_hold, axis=1)\n",
        "\n",
        "print(\"\\n=== HOLDOUT SET PERFORMANCE (FINAL) ===\")\n",
        "print(classification_report(y_hold, y_pred_hold, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_hold, y_pred_hold))\n",
        "\n",
        "\n",
        "\n",
        "def transformer_signals(classifier, X_window):\n",
        "    probs = classifier.predict(X_window, verbose=0)[0]  # shape (3,)\n",
        "\n",
        "    p0, p1, p2 = probs\n",
        "\n",
        "    return {\n",
        "        \"p_normal\": float(p0),\n",
        "        \"early_fault_score\": float(p1),\n",
        "        \"severe_fault_score\": float(p2),\n",
        "        \"anomaly_score\": float(1.0 - p0)\n",
        "    }\n",
        "\n",
        "signals = transformer_signals(classifier, X_hold) #Get actual prob distribution now\n",
        "print(signals)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#  MAIN ORCHESTRATION: DECISION + EXPERT + EVENT STORE\n",
        "# =====================================================\n",
        "\n",
        "# Reuse your existing:\n",
        "# - window_agent  (AdaptiveWindowAgent)\n",
        "# - sensor_agents, master (RobustMasterAgent)\n",
        "# - scores        (prototype_scores on emb_holdout)\n",
        "# - X_holdout, y_holdout\n",
        "\n",
        "print(\"\\n🚀 Running DecisionAgent + ExpertAgent pipeline on HOLDOUT...\\n\")\n",
        "\n",
        "# 1) Initialise storage + agents\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "from openai import OpenAI\n",
        "openai_client = OpenAI()\n",
        "\n",
        "event_store = EventStore(db_path=\"event_store.db\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1) Decision + Expert agents\n",
        "# -----------------------------------------------------\n",
        "decision_agent = DecisionAgent(\n",
        "    prediction_weight=0.7,   # transformer dominates\n",
        "    sensor_weight=0.2,\n",
        "    drift_weight=0.1,\n",
        "    failure_threshold=0.5,\n",
        "    failure_critical_threshold=0.8,\n",
        ")\n",
        "\n",
        "expert_agent = ExpertAgent(\n",
        "    event_store=event_store,\n",
        "    system_description=\"MetroPT multivariate APU system\",\n",
        "    llm_client=openai_client,\n",
        "    history_limit=100,\n",
        "    max_history_for_prompt=10,\n",
        "    window_preview_len=30,\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2) Run HOLDOUT\n",
        "# -----------------------------------------------------\n",
        "print(\"\\n🚀 Running Decision + Expert pipeline on HOLDOUT...\\n\")\n",
        "\n",
        "decision_results = []\n",
        "decision_flags = []\n",
        "\n",
        "for i, seq in enumerate(X_hold):\n",
        "    try:\n",
        "        # ---------------------------------------------\n",
        "        # a) Sensor-level detection\n",
        "        # ---------------------------------------------\n",
        "        master_out = master.process_system_input(seq)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # b) Window agent\n",
        "        # ---------------------------------------------\n",
        "        features = seq.flatten()\n",
        "        window_out = window_agent.predict_window_size(features, seq)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # c) Transformer prediction (ML truth)\n",
        "        # ---------------------------------------------\n",
        "        probs = classifier.predict(seq[np.newaxis, ...], verbose=0)[0]\n",
        "\n",
        "        model_outputs = {\n",
        "            \"failure_prob\": float(probs[2]),   # class 2 = FAULT\n",
        "            \"transformer_prob\": float(probs[1]),  # class 1 = WARNING\n",
        "            \"prototype_score\": None,\n",
        "        }\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # d) Decision fusion\n",
        "        # ---------------------------------------------\n",
        "        decision_packet = decision_agent.decide(\n",
        "            master_output=master_out,\n",
        "            window_output=window_out,\n",
        "            model_outputs=model_outputs,\n",
        "            metadata={\"index\": int(i)},\n",
        "        )\n",
        "\n",
        "        expert_packet = None\n",
        "        human_feedback = None\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # e) LLM only for HIGH / CRITICAL\n",
        "        # ---------------------------------------------\n",
        "        if decision_packet[\"alert_level\"] in [\"HIGH\", \"CRITICAL\"]:\n",
        "            expert_packet = expert_agent.analyse_anomaly(\n",
        "                decision_packet=decision_packet,\n",
        "                system_subsequence=seq,\n",
        "                extra_context={\"index\": int(i)},\n",
        "            )\n",
        "\n",
        "            human_feedback = get_human_feedback_stub(expert_packet)\n",
        "\n",
        "            event_store.save_event(\n",
        "                event_type=\"ALERT\",\n",
        "                packet=decision_packet,\n",
        "                expert=expert_packet,\n",
        "                human=human_feedback,\n",
        "            )\n",
        "        else:\n",
        "            event_store.save_event(\n",
        "                event_type=\"NORMAL\",\n",
        "                packet=decision_packet,\n",
        "                expert=None,\n",
        "                human=None,\n",
        "            )\n",
        "\n",
        "        decision_results.append({\n",
        "            \"master\": master_out,\n",
        "            \"window\": window_out,\n",
        "            \"decision\": decision_packet,\n",
        "            \"expert\": expert_packet,\n",
        "            \"human_feedback\": human_feedback,\n",
        "        })\n",
        "\n",
        "        decision_flags.append(1 if decision_packet[\"final_failure\"] else 0)\n",
        "\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Processed {i+1}/{len(X_hold)} samples...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Sample {i} failed: {e}\")\n",
        "        decision_results.append({\n",
        "            \"master\": None,\n",
        "            \"window\": None,\n",
        "            \"decision\": {\"final_failure\": False},\n",
        "            \"expert\": None,\n",
        "            \"human_feedback\": None,\n",
        "        })\n",
        "        decision_flags.append(0)\n",
        "\n",
        "decision_flags = np.array(decision_flags)\n",
        "\n",
        "print(\"\\n✅ Pipeline completed.\")\n",
        "print(\"Failure flag distribution:\", np.unique(decision_flags, return_counts=True))\n",
        "print(\"Anomaly flags distribution:\", np.unique(decision_flags, return_counts=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-REVISED-DecisionExpertLLM.ipynb",
      "authorship_tag": "ABX9TyOMehK3Yoi/gLAILcN9zUTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}