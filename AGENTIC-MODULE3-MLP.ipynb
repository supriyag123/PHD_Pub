{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "034157aa-39a7-439d-c6c7-cb2e31f68c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1346822617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create line chart: True vs Predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Enhanced Feature Engineering MLP\n",
        "# Based on correlation analysis showing Feature 7 √ó Feature 8 interaction\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.regularizers import l1_l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedFeatureEngineeringMLP:\n",
        "    \"\"\"\n",
        "    MLP with explicit feature engineering based on correlation analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.y_scaler = StandardScaler()\n",
        "        self.x_scaler = StandardScaler()  # For engineered features\n",
        "        self.model = None\n",
        "\n",
        "        # Create directory for enhanced model\n",
        "        self.enhanced_dir = f\"{output_dir}enhanced_mlp/\"\n",
        "        os.makedirs(self.enhanced_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üìÅ Enhanced MLP directory: {self.enhanced_dir}\")\n",
        "\n",
        "    def load_original_data(self, data_filename, windows_filename):\n",
        "        \"\"\"Load original data for feature engineering\"\"\"\n",
        "        print(\"üìä Loading original data for feature engineering...\")\n",
        "\n",
        "        data_path = os.path.join(self.output_dir, data_filename)\n",
        "        windows_path = os.path.join(self.output_dir, windows_filename)\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        y = np.load(windows_path)\n",
        "\n",
        "        print(f\"‚úÖ Original data loaded: X={x.shape}, y={y.shape}\")\n",
        "        return x, y\n",
        "\n",
        "    def create_enhanced_features(self, x):\n",
        "        \"\"\"\n",
        "        Create enhanced features based on correlation analysis findings\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Creating enhanced features based on correlation analysis...\")\n",
        "        print(f\"   Starting with {x.shape[1]} original features\")\n",
        "\n",
        "        enhanced_features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # 1. Original features (scaled)\n",
        "        enhanced_features.append(x)\n",
        "        feature_names.extend([f\"orig_{i}\" for i in range(x.shape[1])])\n",
        "        print(f\"   ‚úÖ Original features: {x.shape[1]}\")\n",
        "\n",
        "        # 2. Best interaction found (Feature 7 √ó Feature 8)\n",
        "        if x.shape[1] > 8:\n",
        "            best_interaction = x[:, 7] * x[:, 8]\n",
        "            enhanced_features.append(best_interaction.reshape(-1, 1))\n",
        "            feature_names.append(\"feat_7_x_feat_8\")\n",
        "            print(f\"   ‚úÖ Best interaction (7√ó8): 1 feature\")\n",
        "\n",
        "        # 3. Top feature interactions (systematic)\n",
        "        print(\"   Creating systematic feature interactions...\")\n",
        "        interaction_features = []\n",
        "        interaction_names = []\n",
        "\n",
        "        # Focus on top features that showed some correlation\n",
        "        top_features = min(15, x.shape[1])  # Use top 15 features\n",
        "        interaction_count = 0\n",
        "\n",
        "        for i in range(top_features):\n",
        "            for j in range(i+1, top_features):\n",
        "                if interaction_count < 50:  # Limit interactions\n",
        "                    # Multiplication\n",
        "                    mult_feat = x[:, i] * x[:, j]\n",
        "                    interaction_features.append(mult_feat)\n",
        "                    interaction_names.append(f\"feat_{i}_x_feat_{j}\")\n",
        "\n",
        "                    # Division (safe)\n",
        "                    if np.all(np.abs(x[:, j]) > 1e-8):\n",
        "                        div_feat = x[:, i] / (x[:, j] + 1e-8)\n",
        "                        interaction_features.append(div_feat)\n",
        "                        interaction_names.append(f\"feat_{i}_div_feat_{j}\")\n",
        "\n",
        "                    interaction_count += 2\n",
        "\n",
        "                    if interaction_count >= 50:\n",
        "                        break\n",
        "            if interaction_count >= 50:\n",
        "                break\n",
        "\n",
        "        if interaction_features:\n",
        "            interaction_matrix = np.column_stack(interaction_features)\n",
        "            enhanced_features.append(interaction_matrix)\n",
        "            feature_names.extend(interaction_names)\n",
        "            print(f\"   ‚úÖ Feature interactions: {len(interaction_features)}\")\n",
        "\n",
        "        # 4. Polynomial features (degree 2) for top features\n",
        "        print(\"   Creating polynomial features...\")\n",
        "        poly_features = []\n",
        "        poly_names = []\n",
        "\n",
        "        top_poly_features = min(10, x.shape[1])\n",
        "        for i in range(top_poly_features):\n",
        "            # Quadratic terms\n",
        "            quad_feat = x[:, i] ** 2\n",
        "            poly_features.append(quad_feat)\n",
        "            poly_names.append(f\"feat_{i}_squared\")\n",
        "\n",
        "            # Cubic terms (selective)\n",
        "            if i < 5:  # Only for top 5\n",
        "                cube_feat = x[:, i] ** 3\n",
        "                poly_features.append(cube_feat)\n",
        "                poly_names.append(f\"feat_{i}_cubed\")\n",
        "\n",
        "        if poly_features:\n",
        "            poly_matrix = np.column_stack(poly_features)\n",
        "            enhanced_features.append(poly_matrix)\n",
        "            feature_names.extend(poly_names)\n",
        "            print(f\"   ‚úÖ Polynomial features: {len(poly_features)}\")\n",
        "\n",
        "        # 5. Statistical features (rolling-like operations)\n",
        "        print(\"   Creating statistical features...\")\n",
        "        stat_features = []\n",
        "        stat_names = []\n",
        "\n",
        "        if x.shape[1] >= 5:\n",
        "            # Moving averages across features (treating features as sequence)\n",
        "            for window in [3, 5]:\n",
        "                if x.shape[1] >= window:\n",
        "                    for start in range(0, min(20, x.shape[1] - window + 1), window):\n",
        "                        end = start + window\n",
        "                        mean_feat = np.mean(x[:, start:end], axis=1)\n",
        "                        std_feat = np.std(x[:, start:end], axis=1)\n",
        "\n",
        "                        stat_features.extend([mean_feat, std_feat])\n",
        "                        stat_names.extend([f\"mean_{start}_{end}\", f\"std_{start}_{end}\"])\n",
        "\n",
        "        if stat_features:\n",
        "            stat_matrix = np.column_stack(stat_features)\n",
        "            enhanced_features.append(stat_matrix)\n",
        "            feature_names.extend(stat_names)\n",
        "            print(f\"   ‚úÖ Statistical features: {len(stat_features)}\")\n",
        "\n",
        "        # 6. Combine all features\n",
        "        X_enhanced = np.hstack(enhanced_features)\n",
        "\n",
        "        print(f\"\\nüìà Feature Engineering Summary:\")\n",
        "        print(f\"   Original features: {x.shape[1]}\")\n",
        "        print(f\"   Enhanced features: {X_enhanced.shape[1]}\")\n",
        "        print(f\"   Enhancement factor: {X_enhanced.shape[1] / x.shape[1]:.1f}x\")\n",
        "\n",
        "        return X_enhanced, feature_names\n",
        "\n",
        "    def select_best_features(self, X_enhanced, y, max_features=200):\n",
        "        \"\"\"Select best features using multiple criteria\"\"\"\n",
        "        print(f\"\\nüéØ Selecting best {max_features} features from {X_enhanced.shape[1]}...\")\n",
        "\n",
        "        # Use mutual information for non-linear feature selection\n",
        "        selector = SelectKBest(mutual_info_regression, k=min(max_features, X_enhanced.shape[1]))\n",
        "        X_selected = selector.fit_transform(X_enhanced, y)\n",
        "\n",
        "        # Get selected feature indices\n",
        "        selected_indices = selector.get_support(indices=True)\n",
        "        selected_scores = selector.scores_[selected_indices]\n",
        "\n",
        "        print(f\"   Selected {X_selected.shape[1]} features\")\n",
        "        print(f\"   Score range: [{np.min(selected_scores):.6f}, {np.max(selected_scores):.6f}]\")\n",
        "\n",
        "        return X_selected, selected_indices, selector\n",
        "\n",
        "    def build_interaction_focused_mlp(self, input_dim):\n",
        "        \"\"\"Build MLP optimized for learning feature interactions\"\"\"\n",
        "        print(f\"\\nüèóÔ∏è Building interaction-focused MLP for {input_dim} features...\")\n",
        "\n",
        "        model = Sequential([\n",
        "            # Very wide first layer to capture many interactions\n",
        "            Dense(4096, input_dim=input_dim, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            # Wide second layer for interaction combinations\n",
        "            Dense(2048, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            # Compression layers\n",
        "            Dense(1024, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(512, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(256, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "\n",
        "            Dense(32, activation='relu'),\n",
        "\n",
        "            # Output\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        # Optimizer tuned for weak signals\n",
        "        optimizer = keras.optimizers.Adam(\n",
        "            learning_rate=0.002,  # Higher LR for weak signals\n",
        "            clipnorm=1.0,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999\n",
        "        )\n",
        "\n",
        "        # MAE loss often better for weak signals\n",
        "        model.compile(\n",
        "            loss='mae',  # Mean Absolute Error\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mse', 'mae']\n",
        "        )\n",
        "\n",
        "        print(f\"   Model parameters: {model.count_params():,}\")\n",
        "        print(f\"   Loss function: MAE (better for weak signals)\")\n",
        "        print(f\"   Learning rate: 0.002 (higher for weak relationships)\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_enhanced_model(self, x_train, y_train, x_val, y_val):\n",
        "        \"\"\"Train the enhanced model with optimized settings\"\"\"\n",
        "        print(f\"\\nüöÄ Training enhanced feature model...\")\n",
        "\n",
        "        # Build model\n",
        "        self.model = self.build_interaction_focused_mlp(x_train.shape[1])\n",
        "\n",
        "        # Enhanced callbacks\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                f\"{self.enhanced_dir}best_enhanced_model.weights.h5\",\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=150,  # More patience for weak signals\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.0001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=50,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=1500,  # More epochs for weak signals\n",
        "            batch_size=64,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Enhanced model training complete!\")\n",
        "        return history\n",
        "\n",
        "    def evaluate_enhanced_model(self, x_test, y_test, history):\n",
        "        \"\"\"Evaluate the enhanced model\"\"\"\n",
        "        print(\"\\nüìä Evaluating enhanced model...\")\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_scaled = self.model.predict(x_test, verbose=0)\n",
        "        y_pred = self.y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "        y_true = self.y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Accuracy metrics\n",
        "        acc_05 = np.mean(np.abs(y_true - y_pred) <= 0.5) * 100\n",
        "        acc_1 = np.mean(np.abs(y_true - y_pred) <= 1) * 100\n",
        "        acc_15 = np.mean(np.abs(y_true - y_pred) <= 1.5) * 100\n",
        "        acc_2 = np.mean(np.abs(y_true - y_pred) <= 2) * 100\n",
        "\n",
        "        results = {\n",
        "            'r2': r2, 'mse': mse, 'mae': mae, 'rmse': rmse,\n",
        "            'acc_05': acc_05, 'acc_1': acc_1, 'acc_15': acc_15, 'acc_2': acc_2,\n",
        "            'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        print(f\"üìà Enhanced Model Results:\")\n",
        "        print(f\"   R¬≤: {r2:.6f}\")\n",
        "        print(f\"   MAE: {mae:.4f}\")\n",
        "        print(f\"   RMSE: {rmse:.4f}\")\n",
        "        print(f\"   Accuracy ¬±0.5: {acc_05:.1f}%\")\n",
        "        print(f\"   Accuracy ¬±1.0: {acc_1:.1f}%\")\n",
        "        print(f\"   Accuracy ¬±1.5: {acc_15:.1f}%\")\n",
        "        print(f\"   Accuracy ¬±2.0: {acc_2:.1f}%\")\n",
        "\n",
        "        # Plot enhanced results\n",
        "        self.plot_enhanced_results(y_true, y_pred, history, r2, mae)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_enhanced_results(self, y_true, y_pred, history, r2, mae):\n",
        "        \"\"\"Plot comprehensive results for enhanced model\"\"\"\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
        "\n",
        "        # Training history - Loss\n",
        "        axes[0,0].plot(history.history['loss'], label='Training', linewidth=2)\n",
        "        axes[0,0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "        axes[0,0].set_title('Enhanced Model Loss')\n",
        "        axes[0,0].set_yscale('log')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Training history - MAE\n",
        "        axes[0,1].plot(history.history['mae'], label='Training', linewidth=2)\n",
        "        axes[0,1].plot(history.history['val_mae'], label='Validation', linewidth=2)\n",
        "        axes[0,1].set_title('Enhanced Model MAE')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Predictions scatter\n",
        "        axes[0,2].scatter(y_true, y_pred, alpha=0.6, s=3, color='darkblue')\n",
        "        axes[0,2].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "        axes[0,2].set_xlabel('True Values')\n",
        "        axes[0,2].set_ylabel('Predictions')\n",
        "        axes[0,2].set_title(f'Enhanced Predictions\\n(R¬≤={r2:.6f})')\n",
        "        axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate (if available)\n",
        "        if 'lr' in history.history:\n",
        "            axes[0,3].plot(history.history['lr'], linewidth=2)\n",
        "            axes[0,3].set_title('Learning Rate Schedule')\n",
        "            axes[0,3].set_yscale('log')\n",
        "        else:\n",
        "            # Show improvement over epochs\n",
        "            val_loss = history.history['val_loss']\n",
        "            best_val_loss = np.minimum.accumulate(val_loss)\n",
        "            axes[0,3].plot(best_val_loss, linewidth=2, color='green')\n",
        "            axes[0,3].set_title('Best Validation Loss')\n",
        "            axes[0,3].set_yscale('log')\n",
        "        axes[0,3].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residuals\n",
        "        residuals = y_true - y_pred\n",
        "        axes[1,0].scatter(y_pred, residuals, alpha=0.6, s=3, color='green')\n",
        "        axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
        "        axes[1,0].set_xlabel('Predictions')\n",
        "        axes[1,0].set_ylabel('Residuals')\n",
        "        axes[1,0].set_title('Enhanced Model Residuals')\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Error distribution\n",
        "        axes[1,1].hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[1,1].set_xlabel('Residuals')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "        axes[1,1].set_title('Error Distribution')\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy comparison\n",
        "        tolerances = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
        "        accuracies = [np.mean(np.abs(residuals) <= tol) * 100 for tol in tolerances]\n",
        "        bars = axes[1,2].bar(tolerances, accuracies, alpha=0.7, color='lightgreen')\n",
        "        axes[1,2].set_xlabel('Error Tolerance')\n",
        "        axes[1,2].set_ylabel('Accuracy (%)')\n",
        "        axes[1,2].set_title('Enhanced Accuracy vs Tolerance')\n",
        "        axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add accuracy labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            axes[1,2].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                          f'{acc:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "        # Sample predictions line chart\n",
        "        sample_size = min(300, len(y_true))\n",
        "        indices = np.random.choice(len(y_true), sample_size, replace=False)\n",
        "        indices = np.sort(indices)\n",
        "        axes[1,3].plot(indices, y_true[indices], 'b-', label='True', linewidth=1.5, alpha=0.8)\n",
        "        axes[1,3].plot(indices, y_pred[indices], 'r--', label='Enhanced Pred', linewidth=1.5, alpha=0.8)\n",
        "        axes[1,3].set_xlabel('Sample Index')\n",
        "        axes[1,3].set_ylabel('VAR Window Size')\n",
        "        axes[1,3].set_title('Enhanced Sample Predictions')\n",
        "        axes[1,3].legend()\n",
        "        axes[1,3].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def run_enhanced_pipeline(self, data_filename, windows_filename):\n",
        "        \"\"\"Run complete enhanced feature engineering pipeline\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"üöÄ ENHANCED FEATURE ENGINEERING MLP PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # 1. Load original data\n",
        "            x, y = self.load_original_data(data_filename, windows_filename)\n",
        "\n",
        "            # 2. Create enhanced features\n",
        "            X_enhanced, feature_names = self.create_enhanced_features(x)\n",
        "\n",
        "            # 3. Select best features\n",
        "            X_selected, selected_indices, selector = self.select_best_features(X_enhanced, y)\n",
        "\n",
        "            # 4. Split data\n",
        "            print(f\"\\nüìä Splitting enhanced data...\")\n",
        "            x_temp, x_test, y_temp, y_test = train_test_split(\n",
        "                X_selected, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "            x_train, x_val, y_train, y_val = train_test_split(\n",
        "                x_temp, y_temp, test_size=0.25, random_state=42\n",
        "            )\n",
        "\n",
        "            # 5. Scale features and targets\n",
        "            x_train_scaled = self.x_scaler.fit_transform(x_train)\n",
        "            x_val_scaled = self.x_scaler.transform(x_val)\n",
        "            x_test_scaled = self.x_scaler.transform(x_test)\n",
        "\n",
        "            y_train_scaled = self.y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "            y_val_scaled = self.y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
        "            y_test_scaled = self.y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "            print(f\"   Train: {x_train_scaled.shape[0]} samples, {x_train_scaled.shape[1]} features\")\n",
        "            print(f\"   Validation: {x_val_scaled.shape[0]} samples\")\n",
        "            print(f\"   Test: {x_test_scaled.shape[0]} samples\")\n",
        "\n",
        "            # 6. Train enhanced model\n",
        "            history = self.train_enhanced_model(\n",
        "                x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled\n",
        "            )\n",
        "\n",
        "            # 7. Evaluate\n",
        "            results = self.evaluate_enhanced_model(x_test_scaled, y_test_scaled, history)\n",
        "\n",
        "            # 8. Save enhanced model\n",
        "            model_file = f\"{self.enhanced_dir}enhanced_feature_model.keras\"\n",
        "            self.model.save(model_file)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"üéâ ENHANCED FEATURE ENGINEERING COMPLETE!\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Original max correlation: ~0.025\")\n",
        "            print(f\"Enhanced interaction correlation: 0.037\")\n",
        "            print(f\"Final R¬≤: {results['r2']:.6f}\")\n",
        "            print(f\"Final MAE: {results['mae']:.4f}\")\n",
        "            print(f\"Accuracy ¬±2: {results['acc_2']:.1f}%\")\n",
        "            print(f\"Model saved: {model_file}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            return {\n",
        "                'model': self.model,\n",
        "                'results': results,\n",
        "                'history': history,\n",
        "                'feature_names': feature_names,\n",
        "                'selected_indices': selected_indices,\n",
        "                'x_scaler': self.x_scaler,\n",
        "                'y_scaler': self.y_scaler\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Enhanced pipeline failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "# Simple interface function\n",
        "def run_enhanced_feature_mlp(data_filename, windows_filename):\n",
        "    \"\"\"\n",
        "    Run enhanced feature engineering MLP\n",
        "\n",
        "    Args:\n",
        "        data_filename: e.g., 'generated-data-OPTIMIZED.npy'\n",
        "        windows_filename: e.g., 'generated-data-true-window-OPTIMIZED.npy'\n",
        "    \"\"\"\n",
        "    enhanced_mlp = EnhancedFeatureEngineeringMLP()\n",
        "    return enhanced_mlp.run_enhanced_pipeline(data_filename, windows_filename)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Enhanced Feature Engineering MLP\")\n",
        "\n",
        "    # SPECIFY YOUR FILES\n",
        "    data_file = 'generated-data-OPTIMIZED.npy'\n",
        "    windows_file = 'generated-data-true-window-OPTIMIZED.npy'\n",
        "\n",
        "    print(f\"Running enhanced pipeline on: {data_file}, {windows_file}\")\n",
        "\n",
        "    results = run_enhanced_feature_mlp(data_file, windows_file)\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\nüéä Enhanced model complete!\")\n",
        "        print(f\"R¬≤ improvement: {results['results']['r2']:.6f}\")\n",
        "        print(f\"Accuracy ¬±2: {results['results']['acc_2']:.1f}%\")\n",
        "    else:\n",
        "        print(\"‚ùå Enhanced pipeline failed\")\n",
        "\n",
        "# Usage example\n",
        "\"\"\"\n",
        "# Run enhanced feature engineering MLP\n",
        "results = run_enhanced_feature_mlp(\n",
        "    'generated-data-OPTIMIZED.npy',\n",
        "    'generated-data-true-window-OPTIMIZED.npy'\n",
        ")\n",
        "\n",
        "# Check improvement\n",
        "if results:\n",
        "    r2_enhanced = results['results']['r2']\n",
        "    acc_2_enhanced = results['results']['acc_2']\n",
        "    print(f\"Enhanced R¬≤: {r2_enhanced:.6f}\")\n",
        "    print(f\"Enhanced Accuracy ¬±2: {acc_2_enhanced:.1f}%\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyOPxU4rSi42y3jrXvNzIr89",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}