{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/Multiprocessing%20-%20Final_Paper_PART1_Multivariate_sliding_window_labelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-BJGPTX7XS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f45ea0f-db34-43a8-d257-6ca1180a90e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Collecting statsmodels\n",
            "  Downloading statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Installing collected packages: statsmodels\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.1\n",
            "    Uninstalling statsmodels-0.14.1:\n",
            "      Successfully uninstalled statsmodels-0.14.1\n",
            "Successfully installed statsmodels-0.14.2\n",
            "Collecting lingam\n",
            "  Downloading lingam-1.8.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.7/95.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lingam) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lingam) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lingam) (1.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from lingam) (0.20.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from lingam) (0.14.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lingam) (3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lingam) (2.0.3)\n",
            "Collecting pygam (from lingam)\n",
            "  Downloading pygam-0.9.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lingam) (3.7.1)\n",
            "Collecting psy (from lingam)\n",
            "  Downloading psy-0.0.1-py2.py3-none-any.whl (38 kB)\n",
            "Collecting semopy (from lingam)\n",
            "  Downloading semopy-2.3.11.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lingam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lingam) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lingam) (2024.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from psy->lingam) (4.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lingam) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lingam) (3.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from semopy->lingam) (1.12)\n",
            "Collecting numdifftools (from semopy->lingam)\n",
            "  Downloading numdifftools-0.9.41-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->lingam) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->lingam) (1.16.0)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->psy->lingam) (3.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->semopy->lingam) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.0.0->progressbar2->psy->lingam) (4.11.0)\n",
            "Building wheels for collected packages: semopy\n",
            "  Building wheel for semopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for semopy: filename=semopy-2.3.11-py3-none-any.whl size=1659681 sha256=ac4ea2b49db825e88d8471ec525fb8b2f97a97b835553b5e37681065792cb9e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/ec/0d/0b294c02d8c4e9e80afea58839f2c1b4706770594bc99ec045\n",
            "Successfully built semopy\n",
            "Installing collected packages: numdifftools, pygam, psy, semopy, lingam\n",
            "Successfully installed lingam-1.8.3 numdifftools-0.9.41 psy-0.0.1 pygam-0.9.1 semopy-2.3.11\n"
          ]
        }
      ],
      "source": [
        "!pip install statsmodels --upgrade\n",
        "!pip install -U lingam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tools.eval_measures import rmse, aic\n",
        "import ast\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from numpy import arange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "\n",
        "\n",
        "df=pd.read_csv(r'/content/drive/MyDrive/PHD/2021/household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
        "\n",
        "df.head()\n",
        "\n",
        "#df = df[['Global_active_power','Global_reactive_power','Global_intensity']]\n",
        "#Imputing NULL\n",
        "df = df.replace('?', np.nan)\n",
        "df.isnull().sum()\n",
        "\n",
        "#IMputing missing value\n",
        "\n",
        "def fill_missing(values):\n",
        "    one_day = 60*24\n",
        "    for row in range(df.shape[0]):\n",
        "        for col in range(df.shape[1]):\n",
        "            if np.isnan(values[row][col]):\n",
        "                values[row,col] = values[row-one_day,col]\n",
        "df = df.astype('float32')\n",
        "fill_missing(df.values)\n",
        "df.isnull().sum()\n",
        "\n",
        "#Downsampling to Days initially --------------------------- to perform granger causality on the entire time series history------------------------------to reduce computation--------------####\n",
        "daily_df = df.resample('D').sum()\n",
        "daily_df.head()\n",
        "#daily_df =df\n",
        "ts_len = daily_df.shape[0]\n",
        "\n",
        "#Now convert index to column\n",
        "daily_df['datetime']=daily_df.index\n",
        "\n",
        "#VISUALISE THE TIMESERIES\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=daily_df['datetime'], y=daily_df['Global_reactive_power'], name='Global_reactive_power'))\n",
        "\n",
        "fig.update_layout(showlegend=True, title='Household electricity consumption')\n",
        "fig.show()\n",
        "\n",
        "#remove index column unless required.\n",
        "\n",
        "daily_df.drop(daily_df.columns[7], axis=1, inplace=True)\n",
        "#daily_df.drop(daily_df.columns[2], axis=1, inplace=True) #dropped Voltage\n",
        "#Scaling the values\n",
        "whole_series = daily_df\n",
        "scalers={}\n",
        "for i in daily_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(whole_series[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    whole_series[i]=s_s\n",
        "\n",
        "\n",
        "\n",
        "#Stationarity check for each time series ---- This module can be run manually for now\n",
        "\n",
        "def augmented_dickey_fuller_statistics(time_series):\n",
        "  result = adfuller(time_series.values)\n",
        "  ADF_Statistic = result[0]\n",
        "  p_value = result[1]\n",
        "  Critical_values_1 = result[4][\"1%\"]\n",
        "  Critical_values_5 = result[4][\"5%\"]\n",
        "  Critical_values_10 = result[4][\"10%\"]\n",
        "\n",
        "  # We take that p-value should be less than 0.05 and ADF_statistic should be less than critical value at 5% confidence Critical_value_5\n",
        "  if p_value <0.05 and ADF_Statistic < Critical_values_5:\n",
        "    return \"stationary\"\n",
        "  else:\n",
        "    return \"non-stationary\"\n",
        "\n",
        "for i in range(0,whole_series.shape[1]):\n",
        "  print('Augmented Dickey-Fuller Test Result:', whole_series.iloc[:,i].name)\n",
        "  x= augmented_dickey_fuller_statistics(whole_series.iloc[:,i])\n",
        "  print(x)\n",
        "  #if any of the x is \"non-stationary\": df_difference = whole_series.diff() -- and then deal with df_difference\n",
        "\n",
        "#Granger Causality\n",
        "\n",
        "max_lag_GC = 50\n",
        "\n",
        "def granger_causation_matrix(data, variables, p, test = 'ssr_chi2test', verbose=False):\n",
        "    \"\"\"Check Granger Causality of all possible combinations of the time series.\n",
        "    The rows are the response variables, columns are predictors. The values in the table\n",
        "    are the P-Values. P-Values lesser than the significance level (0.05), implies\n",
        "    the Null Hypothesis that the coefficients of the corresponding past values is\n",
        "    zero, that is, the X does not cause Y can be rejected.\n",
        "\n",
        "    data      : pandas dataframe containing the time series variables\n",
        "    variables : list containing names of the time series variables.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            test_result = grangercausalitytests(data[[r, c]], p, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(p)]\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "            min_p_value = np.min(p_values)\n",
        "            df.loc[r, c] = min_p_value\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "granger_causation_matrix(whole_series, whole_series.columns, max_lag_GC)\n",
        "\n",
        "#### GRANGER CAUSALITY PROVES that Voltage is strongly related with most - so we are removing voltage############\n",
        "\n",
        "whole_series.drop(whole_series.columns[2], axis=1, inplace=True)\n",
        "\n",
        "#Downsampling to Hours now  --------------------------- ----------------------------------------------------------------------------------------------------------------------------------####\n",
        "daily_df = df.resample('H').sum()\n",
        "daily_df.head()\n",
        "#daily_df =df\n",
        "ts_len = daily_df.shape[0]\n",
        "\n",
        "#Now convert index to column\n",
        "daily_df['datetime']=daily_df.index\n",
        "\n",
        "#VISUALISE THE TIMESERIES\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=daily_df['datetime'], y=daily_df['Global_reactive_power'], name='Global_reactive_power'))\n",
        "\n",
        "fig.update_layout(showlegend=True, title='Household electricity consumption')\n",
        "fig.show()\n",
        "\n",
        "#remove index column unless required.\n",
        "\n",
        "daily_df.drop(daily_df.columns[7], axis=1, inplace=True)\n",
        "#daily_df.drop(daily_df.columns[2], axis=1, inplace=True) #dropped Voltage\n",
        "#Scaling the values\n",
        "whole_series = daily_df\n",
        "scalers={}\n",
        "for i in daily_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(whole_series[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    whole_series[i]=s_s\n",
        "\n",
        "\n",
        "\n",
        "#### GRANGER CAUSALITY PROVES that Voltage is strongly related with most - so we are removing voltage############\n",
        "\n",
        "whole_series.drop(whole_series.columns[2], axis=1, inplace=True)\n",
        "######## SPLIT THE TIME SERIES INTO LONG SEQUENCES OF LENGTH K\n",
        "\n",
        "#Creating subsequences\n",
        "\n",
        "\n",
        "def extract_windows_vectorized(array, large_seq_size):\n",
        "    start = 0\n",
        "    last_index = len(array)-1\n",
        "    max_time =  last_index - large_seq_size +1  ##last index upto which sliding windoe begining can go\n",
        "\n",
        "    sub_windows = (\n",
        "        start +\n",
        "        # expand_dims are used to convert a 1D array to 2D array.\n",
        "        np.expand_dims(np.arange(large_seq_size), 0) +\n",
        "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
        "    ).astype(int)\n",
        "\n",
        "    return array[sub_windows]\n",
        "\n",
        "\n",
        "#DEFINE K - User parameter - length of the LONG time series sequence.\n",
        "K = 120  #taking 5 days of hourly data\n",
        "\n",
        "large_seq_size = K\n",
        "n_features = whole_series.shape[1]\n",
        "\n",
        "Long = extract_windows_vectorized(whole_series.values,large_seq_size) #Shape: (1263, 180, 7) - 902 long sequences, each 180 long, 7 variables\n",
        "\n",
        "maxval = Long.shape[0]\n",
        "count_train = int(math.ceil(0.9*maxval))\n",
        "Long_train = Long[0:count_train]\n",
        "Long_test = Long[count_train:]\n",
        "\n",
        "#First, define a function that can generate small subsequences for all the large K sequences\n",
        "\n",
        "def generate_small_seq(series, n_past, n_future):\n",
        "  #\n",
        "  # n_past ==> no of past observations -- OR -- sliding window\n",
        "  #\n",
        "  # n_future ==> no of future observations -- prediction variable y\n",
        "  #\n",
        "  X, y = list(), list()\n",
        "  for window_start in range(len(series)):\n",
        "    past_end = window_start + n_past\n",
        "    future_end = past_end + n_future\n",
        "    if future_end > len(series):\n",
        "      break\n",
        "    # slicing the past and future parts of the window\n",
        "    past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
        "    X.append(past)\n",
        "    y.append(future)\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "n_future=1\n",
        "predictions = list()\n",
        "\n",
        "rmse_list = list()\n",
        "min_window_list = list()\n",
        "best_window_for_long_seq = list()\n",
        "AIC = list()\n",
        "n_fold = 6 # 5 fold plus 1 for test)\n",
        "# evaluate a logistic regression model using k-fold cross-validation\n",
        "\n",
        "#-------------------Multiprocessing----------------------------------------------------------\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "import itertools\n",
        "import multiprocessing\n",
        "\n",
        "#A function which will process a tuple of parameters\n",
        "def iosw_gen(params):\n",
        "  i = params[0]\n",
        "  j = params[1]\n",
        "  k = params[2]\n",
        "  best_window_for_long_seq.clear()\n",
        "  Total_small_seq_X, Total_small_seq_y = generate_small_seq(Long_train[i,:,:],k, n_future)\n",
        "  Total_small_seq_X_reshaped = Total_small_seq_X[:,:,j].reshape(Total_small_seq_X.shape[0],Total_small_seq_X.shape[1])\n",
        "  Total_small_seq_y_reshaped = Total_small_seq_y[:,:,j].reshape(Total_small_seq_y.shape[0],Total_small_seq_y.shape[1])\n",
        "  maxval2 = Total_small_seq_X_reshaped.shape[0]\n",
        "  count_train2 = int(math.ceil(0.7*maxval2))\n",
        "  Small_train_X = Total_small_seq_X_reshaped[0:count_train2]\n",
        "  Small_train_y = Total_small_seq_y_reshaped[0:count_train2]\n",
        "  Small_test_X =  Total_small_seq_X_reshaped[count_train2:]\n",
        "  Small_test_y = Total_small_seq_y_reshaped[count_train2:]\n",
        "  tscv = TimeSeriesSplit(n_splits=n_fold-1)\n",
        "  regressor = LassoCV(alphas=arange(0, 1, 0.01), cv=tscv, n_jobs=-1)\n",
        "  try:\n",
        "    regressor.fit(Small_train_X, Small_train_y) #get jth time series only\n",
        "    y_predj = regressor.predict(Small_test_X)\n",
        "       #print('alpha: %f' % regressor.alpha_)\n",
        "    rmse = sqrt(mean_squared_error(Small_test_y, y_predj))\n",
        "  except:\n",
        "    rmse = 9999\n",
        "  print(\"done this\")\n",
        "  return i,j,k,rmse\n",
        "\n",
        "\n",
        "\n",
        "#Generate values for each parameter\n",
        "i = range(1001,3001)\n",
        "j = range(Long_train.shape[2])\n",
        "#j = range(5)\n",
        "k = range(2,(round(K)-n_fold-1))\n",
        "\n",
        "#Generate a list of tuples where each tuple is a combination of parameters.\n",
        "#The list will contain all possible combinations of parameters.\n",
        "paramlist = list(itertools.product(i,j,k))\n",
        "\n",
        "#Generate processes equal to the number of cores\n",
        "pool = multiprocessing.Pool(16)\n",
        "\n",
        "#cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "#cores\n",
        "\n",
        "#Distribute the parameter sets evenly across the cores\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "res  = pool.map(iosw_gen,paramlist)\n",
        "res_df = pd.DataFrame(res, columns=[\"i\",\"j\",\"k\",\"rmse\"])\n",
        "res_df_iosw = res_df.loc[res_df.groupby([\"i\",\"j\"]).rmse.idxmin()]\n",
        "print(\"done\")\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/iosw_df-1001-3000-S.npy',res_df_iosw)\n",
        "print(\"saved\")\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "for i in range(Long_train.shape[0]) :\n",
        "#for i in range(2):\n",
        "  #iterate over the entire 759 long sequences of multivariate time series\n",
        "  min_window_list.clear()\n",
        "  for j in range(Long_train.shape[2]):\n",
        "    #iterate over each variable time series\n",
        "    rmse_list.clear()\n",
        "\n",
        "    for k in range(2,(round(K)-n_fold-1)):\n",
        "\n",
        "\n",
        "\n",
        "    #find minimum rmse for all sliding window, and corresponding sw size\n",
        "    min_index = rmse_list.index(min(rmse_list))\n",
        "    min_sw = min_index + 2\n",
        "    print('i=', i,'j=', j,'SW =', min_sw, rmse_list)\n",
        "    min_window_list.append(min_sw)\n",
        "\n",
        "  #exited for internal loops -- now operating for each i\n",
        "\n",
        "  cur_seq = Long_train[i,:,:]\n",
        "  AIC.clear()\n",
        "  model = VAR(cur_seq)\n",
        "\n",
        "  for m in (range(min(min_window_list),max(min_window_list)+1)):\n",
        "    #print(Long_train)\n",
        "    try:\n",
        "      results = model.fit(m)\n",
        "      print('Order =', m)\n",
        "      print('AIC: ', results.aic)\n",
        "      print('BIC: ', results.bic)\n",
        "      AIC.append(results.aic)\n",
        "    except:\n",
        "      AIC.append(99999)\n",
        "      print('VAR could not solve row number')\n",
        "      print(i, m)\n",
        "\n",
        "\n",
        "  minAIC_index = AIC.index(min(AIC))+min(min_window_list)\n",
        "  print(AIC)\n",
        "  print('Minimum lag = ', minAIC_index)\n",
        "  best_window_for_long_seq.append(minAIC_index)\n",
        "\n",
        "#OUT OF ALL LOOPS NOW\n",
        "#best_window_for_long_seq now contains the best multivariate window size for each of the long sequence i\n",
        "print(best_window_for_long_seq)\n",
        "Window_train = np.array(best_window_for_long_seq)\n",
        "\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy',Long_train)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-TRAIN_hourly.npy',Window_train)\n",
        "\n",
        "##NOW CREATE LABELLED DATA FOR TEST DATASET- TO BE USED FOR FINAL TESTING----------------------------------------------------------------------------------------\n",
        "predictions = list()\n",
        "rmse_list = list()\n",
        "min_window_list = list()\n",
        "best_window_for_long_seq = list()\n",
        "AIC = list()\n",
        "n_fold = 6 # 5 fold plus 1 for test)\n",
        "\n",
        "for i in range(Long_test.shape[0]) :\n",
        "  #iterate over the entire 759 long sequences of multivariate time series\n",
        "  min_window_list.clear()\n",
        "  for j in range(Long_test.shape[2]):\n",
        "    #iterate over each variable time series\n",
        "    rmse_list.clear()\n",
        "\n",
        "    for k in range(2,(round(K)-n_fold-1)):\n",
        "      Total_small_seq_X, Total_small_seq_y = generate_small_seq(Long_test[i,:,:],k, n_future)\n",
        "      Total_small_seq_X_reshaped = Total_small_seq_X[:,:,j].reshape(Total_small_seq_X.shape[0],Total_small_seq_X.shape[1])\n",
        "      Total_small_seq_y_reshaped = Total_small_seq_y[:,:,j].reshape(Total_small_seq_y.shape[0],Total_small_seq_y.shape[1])\n",
        "      maxval2 = Total_small_seq_X_reshaped.shape[0]\n",
        "      count_train2 = int(math.ceil(0.7*maxval2))\n",
        "      Small_train_X = Total_small_seq_X_reshaped[0:count_train2]\n",
        "      Small_train_y = Total_small_seq_y_reshaped[0:count_train2]\n",
        "      Small_test_X =  Total_small_seq_X_reshaped[count_train2:]\n",
        "      Small_test_y = Total_small_seq_y_reshaped[count_train2:]\n",
        "      #cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "      #regressor  = LinearRegression()\n",
        "      tscv = TimeSeriesSplit(n_splits=n_fold-1)\n",
        "      regressor = LassoCV(alphas=arange(0, 1, 0.01), cv=tscv, n_jobs=-1)\n",
        "\n",
        "      #scores = cross_val_score(regressor, Total_small_seq_X_reshaped, Total_small_seq_y_reshaped, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
        "      try:\n",
        "       regressor.fit(Small_train_X, Small_train_y) #get jth time series only\n",
        "       if Small_test_X.shape[0] == 0:\n",
        "        break\n",
        "       y_predj = regressor.predict(Small_test_X)\n",
        "       print('alpha: %f' % regressor.alpha_)\n",
        "       rmse = sqrt(mean_squared_error(Small_test_y, y_predj))\n",
        "       print(rmse)\n",
        "       rmse_list.append(rmse)\n",
        "      except:\n",
        "        rmse_list.append(9999)\n",
        "        print('error')\n",
        "\n",
        "\n",
        "\n",
        "    #find minimum rmse for all sliding window, and corresponding sw size\n",
        "    min_index = rmse_list.index(min(rmse_list))\n",
        "    min_sw = min_index + 2\n",
        "    print('i=', i,'j=', j,'SW =', min_sw, rmse_list)\n",
        "    min_window_list.append(min_sw)\n",
        "\n",
        "  #exited for internal loops -- now operating for each i\n",
        "\n",
        "  cur_seq = Long_train[i,:,:]\n",
        "  AIC.clear()\n",
        "  model = VAR(cur_seq)\n",
        "\n",
        "  for m in (range(min(min_window_list),max(min_window_list)+1)):\n",
        "    #print(Long_train)\n",
        "    try:\n",
        "      results = model.fit(m)\n",
        "      print('Order =', m)\n",
        "      print('AIC: ', results.aic)\n",
        "      print('BIC: ', results.bic)\n",
        "      AIC.append(results.aic)\n",
        "    except:\n",
        "      AIC.append(99999)\n",
        "      print('VAR could not solve row number')\n",
        "      print(i, m)\n",
        "\n",
        "\n",
        "  minAIC_index_test = AIC.index(min(AIC))+min(min_window_list)\n",
        "  print(AIC)\n",
        "  print('Minimum lag = ', minAIC_index_test)\n",
        "  best_window_for_long_seq.append(minAIC_index_test)\n",
        "\n",
        "Window_test = np.array(best_window_for_long_seq)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TEST.npy',Long_test)\n",
        "np.save(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-TEST.npy',Window_test)\n",
        "#np.save(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy',Long_test)\n",
        "print('saving completed')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xp-Mv3Ksamxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zEUESMBM_Eo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6ZgIvrdRNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/Final_Paper_PART1_Multivariate_sliding_window_labelling.ipynb",
      "authorship_tag": "ABX9TyMImCsEOWEufcVbnhHFlRJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}