{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "ec95c868-547e-451a-b9f5-2be54ffacedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Resumable Enhanced Feature Engineering MLP\n",
            "Running resumable enhanced pipeline on: generated-data-OPTIMIZED.npy, generated-data-true-window-OPTIMIZED.npy\n",
            "🆕 Initialized new pipeline state\n",
            "📁 Enhanced MLP directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/enhanced_mlp/\n",
            "💾 Checkpoint directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/enhanced_mlp/checkpoints/\n",
            "\n",
            "📋 PIPELINE STATUS REPORT\n",
            "==================================================\n",
            "1/7: Data Loading         ⏳ Pending\n",
            "2/7: Feature Engineering  ⏳ Pending\n",
            "3/7: Feature Selection    ⏳ Pending\n",
            "4/7: Data Preparation     ⏳ Pending\n",
            "5/7: Model Training       ⏳ Pending\n",
            "6/7: Model Evaluation     ⏳ Pending\n",
            "7/7: Pipeline Complete    ⏳ Pending\n",
            "==================================================\n",
            "\n",
            "💾 CHECKPOINT FILES STATUS\n",
            "==================================================\n",
            "❌ enhanced_features    \n",
            "❌ feature_names        \n",
            "❌ selected_features    \n",
            "❌ selected_indices     \n",
            "❌ selector             \n",
            "❌ split_data           \n",
            "❌ scalers              \n",
            "❌ model_weights        \n",
            "❌ training_state       \n",
            "❌ final_model          \n",
            "❌ results              \n",
            "==================================================\n",
            "🆕 Initialized new pipeline state\n",
            "📁 Enhanced MLP directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/enhanced_mlp/\n",
            "💾 Checkpoint directory: /content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/enhanced_mlp/checkpoints/\n",
            "================================================================================\n",
            "🚀 RESUMABLE ENHANCED FEATURE ENGINEERING MLP PIPELINE\n",
            "================================================================================\n",
            "\n",
            "📋 Step 1/7: Loading Data\n",
            "   Status: ⏳ Pending\n",
            "📊 Loading original data for feature engineering...\n",
            "💾 Pipeline state saved\n",
            "✅ Original data loaded and saved: X=(350000, 650), y=(350000,)\n",
            "\n",
            "📋 Step 2/7: Feature Engineering\n",
            "   Status: ⏳ Pending\n",
            "\n",
            "🔧 Creating enhanced features based on correlation analysis...\n",
            "   Starting with 650 original features\n",
            "   ✅ Original features: 650\n",
            "   ✅ Best interaction (7×8): 1 feature\n",
            "   Creating systematic feature interactions...\n",
            "   ✅ Feature interactions: 50\n",
            "   Creating polynomial features...\n",
            "   ✅ Polynomial features: 15\n",
            "   Creating statistical features...\n",
            "   ✅ Statistical features: 22\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Feature Engineering MLP - RESUMABLE VERSION\n",
        "# Added comprehensive checkpoint/resume functionality\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.regularizers import l1_l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ResumableEnhancedFeatureEngineeringMLP:\n",
        "    \"\"\"\n",
        "    MLP with explicit feature engineering and full resume capability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "        self.output_dir = output_dir\n",
        "        self.y_scaler = StandardScaler()\n",
        "        self.x_scaler = StandardScaler()\n",
        "        self.model = None\n",
        "\n",
        "        # Create directory for enhanced model\n",
        "        self.enhanced_dir = f\"{output_dir}enhanced_mlp/\"\n",
        "        os.makedirs(self.enhanced_dir, exist_ok=True)\n",
        "\n",
        "        # Checkpoint files\n",
        "        self.checkpoint_dir = f\"{self.enhanced_dir}checkpoints/\"\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        self.checkpoint_files = {\n",
        "            'enhanced_features': f\"{self.checkpoint_dir}enhanced_features.npy\",\n",
        "            'feature_names': f\"{self.checkpoint_dir}feature_names.pkl\",\n",
        "            'selected_features': f\"{self.checkpoint_dir}selected_features.npy\",\n",
        "            'selected_indices': f\"{self.checkpoint_dir}selected_indices.npy\",\n",
        "            'selector': f\"{self.checkpoint_dir}selector.pkl\",\n",
        "            'split_data': f\"{self.checkpoint_dir}split_data.npz\",\n",
        "            'scalers': f\"{self.checkpoint_dir}scalers.pkl\",\n",
        "            'model_weights': f\"{self.checkpoint_dir}training_checkpoint.weights.h5\",\n",
        "            'training_state': f\"{self.checkpoint_dir}training_state.json\",\n",
        "            'final_model': f\"{self.enhanced_dir}enhanced_feature_model.keras\",\n",
        "            'results': f\"{self.checkpoint_dir}results.pkl\"\n",
        "        }\n",
        "\n",
        "        self.pipeline_state = self.load_pipeline_state()\n",
        "        print(f\"📁 Enhanced MLP directory: {self.enhanced_dir}\")\n",
        "        print(f\"💾 Checkpoint directory: {self.checkpoint_dir}\")\n",
        "\n",
        "    def load_pipeline_state(self):\n",
        "        \"\"\"Load or initialize pipeline state\"\"\"\n",
        "        state_file = f\"{self.checkpoint_dir}pipeline_state.json\"\n",
        "        if os.path.exists(state_file):\n",
        "            with open(state_file, 'r') as f:\n",
        "                state = json.load(f)\n",
        "            print(f\"📋 Loaded pipeline state: {state}\")\n",
        "            return state\n",
        "        else:\n",
        "            state = {\n",
        "                'data_loaded': False,\n",
        "                'features_enhanced': False,\n",
        "                'features_selected': False,\n",
        "                'data_split': False,\n",
        "                'data_scaled': False,\n",
        "                'model_trained': False,\n",
        "                'model_evaluated': False,\n",
        "                'pipeline_complete': False\n",
        "            }\n",
        "            print(f\"🆕 Initialized new pipeline state\")\n",
        "            return state\n",
        "\n",
        "    def save_pipeline_state(self):\n",
        "        \"\"\"Save current pipeline state\"\"\"\n",
        "        state_file = f\"{self.checkpoint_dir}pipeline_state.json\"\n",
        "        with open(state_file, 'w') as f:\n",
        "            json.dump(self.pipeline_state, f, indent=2)\n",
        "        print(f\"💾 Pipeline state saved\")\n",
        "\n",
        "    def load_original_data(self, data_filename, windows_filename):\n",
        "        \"\"\"Load original data for feature engineering\"\"\"\n",
        "        if self.pipeline_state['data_loaded']:\n",
        "            print(\"📊 Data already loaded, skipping...\")\n",
        "            return None, None\n",
        "\n",
        "        print(\"📊 Loading original data for feature engineering...\")\n",
        "\n",
        "        data_path = os.path.join(self.output_dir, data_filename)\n",
        "        windows_path = os.path.join(self.output_dir, windows_filename)\n",
        "\n",
        "        x = np.load(data_path)\n",
        "        y = np.load(windows_path)\n",
        "\n",
        "        # Save loaded data for resume\n",
        "        np.save(f\"{self.checkpoint_dir}original_x.npy\", x)\n",
        "        np.save(f\"{self.checkpoint_dir}original_y.npy\", y)\n",
        "\n",
        "        self.pipeline_state['data_loaded'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(f\"✅ Original data loaded and saved: X={x.shape}, y={y.shape}\")\n",
        "        return x, y\n",
        "\n",
        "    def create_enhanced_features(self, x=None, force_recreate=False):\n",
        "        \"\"\"Create enhanced features with checkpoint support\"\"\"\n",
        "        if self.pipeline_state['features_enhanced'] and not force_recreate:\n",
        "            print(\"🔧 Enhanced features already created, loading from checkpoint...\")\n",
        "            X_enhanced = np.load(self.checkpoint_files['enhanced_features'])\n",
        "            with open(self.checkpoint_files['feature_names'], 'rb') as f:\n",
        "                feature_names = pickle.load(f)\n",
        "            print(f\"✅ Loaded enhanced features: {X_enhanced.shape}\")\n",
        "            return X_enhanced, feature_names\n",
        "\n",
        "        # Load original data if not provided\n",
        "        if x is None:\n",
        "            x = np.load(f\"{self.checkpoint_dir}original_x.npy\")\n",
        "\n",
        "        print(f\"\\n🔧 Creating enhanced features based on correlation analysis...\")\n",
        "        print(f\"   Starting with {x.shape[1]} original features\")\n",
        "\n",
        "        enhanced_features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # 1. Original features (scaled)\n",
        "        enhanced_features.append(x)\n",
        "        feature_names.extend([f\"orig_{i}\" for i in range(x.shape[1])])\n",
        "        print(f\"   ✅ Original features: {x.shape[1]}\")\n",
        "\n",
        "        # 2. Best interaction found (Feature 7 × Feature 8)\n",
        "        if x.shape[1] > 8:\n",
        "            best_interaction = x[:, 7] * x[:, 8]\n",
        "            enhanced_features.append(best_interaction.reshape(-1, 1))\n",
        "            feature_names.append(\"feat_7_x_feat_8\")\n",
        "            print(f\"   ✅ Best interaction (7×8): 1 feature\")\n",
        "\n",
        "        # 3. Top feature interactions (systematic)\n",
        "        print(\"   Creating systematic feature interactions...\")\n",
        "        interaction_features = []\n",
        "        interaction_names = []\n",
        "\n",
        "        top_features = min(15, x.shape[1])\n",
        "        interaction_count = 0\n",
        "\n",
        "        for i in range(top_features):\n",
        "            for j in range(i+1, top_features):\n",
        "                if interaction_count < 50:\n",
        "                    # Multiplication\n",
        "                    mult_feat = x[:, i] * x[:, j]\n",
        "                    interaction_features.append(mult_feat)\n",
        "                    interaction_names.append(f\"feat_{i}_x_feat_{j}\")\n",
        "\n",
        "                    # Division (safe)\n",
        "                    if np.all(np.abs(x[:, j]) > 1e-8):\n",
        "                        div_feat = x[:, i] / (x[:, j] + 1e-8)\n",
        "                        interaction_features.append(div_feat)\n",
        "                        interaction_names.append(f\"feat_{i}_div_feat_{j}\")\n",
        "\n",
        "                    interaction_count += 2\n",
        "\n",
        "                    if interaction_count >= 50:\n",
        "                        break\n",
        "            if interaction_count >= 50:\n",
        "                break\n",
        "\n",
        "        if interaction_features:\n",
        "            interaction_matrix = np.column_stack(interaction_features)\n",
        "            enhanced_features.append(interaction_matrix)\n",
        "            feature_names.extend(interaction_names)\n",
        "            print(f\"   ✅ Feature interactions: {len(interaction_features)}\")\n",
        "\n",
        "        # 4. Polynomial features (degree 2) for top features\n",
        "        print(\"   Creating polynomial features...\")\n",
        "        poly_features = []\n",
        "        poly_names = []\n",
        "\n",
        "        top_poly_features = min(10, x.shape[1])\n",
        "        for i in range(top_poly_features):\n",
        "            # Quadratic terms\n",
        "            quad_feat = x[:, i] ** 2\n",
        "            poly_features.append(quad_feat)\n",
        "            poly_names.append(f\"feat_{i}_squared\")\n",
        "\n",
        "            # Cubic terms (selective)\n",
        "            if i < 5:  # Only for top 5\n",
        "                cube_feat = x[:, i] ** 3\n",
        "                poly_features.append(cube_feat)\n",
        "                poly_names.append(f\"feat_{i}_cubed\")\n",
        "\n",
        "        if poly_features:\n",
        "            poly_matrix = np.column_stack(poly_features)\n",
        "            enhanced_features.append(poly_matrix)\n",
        "            feature_names.extend(poly_names)\n",
        "            print(f\"   ✅ Polynomial features: {len(poly_features)}\")\n",
        "\n",
        "        # 5. Statistical features\n",
        "        print(\"   Creating statistical features...\")\n",
        "        stat_features = []\n",
        "        stat_names = []\n",
        "\n",
        "        if x.shape[1] >= 5:\n",
        "            for window in [3, 5]:\n",
        "                if x.shape[1] >= window:\n",
        "                    for start in range(0, min(20, x.shape[1] - window + 1), window):\n",
        "                        end = start + window\n",
        "                        mean_feat = np.mean(x[:, start:end], axis=1)\n",
        "                        std_feat = np.std(x[:, start:end], axis=1)\n",
        "\n",
        "                        stat_features.extend([mean_feat, std_feat])\n",
        "                        stat_names.extend([f\"mean_{start}_{end}\", f\"std_{start}_{end}\"])\n",
        "\n",
        "        if stat_features:\n",
        "            stat_matrix = np.column_stack(stat_features)\n",
        "            enhanced_features.append(stat_matrix)\n",
        "            feature_names.extend(stat_names)\n",
        "            print(f\"   ✅ Statistical features: {len(stat_features)}\")\n",
        "\n",
        "        # 6. Combine all features\n",
        "        X_enhanced = np.hstack(enhanced_features)\n",
        "\n",
        "        # Save enhanced features\n",
        "        np.save(self.checkpoint_files['enhanced_features'], X_enhanced)\n",
        "        with open(self.checkpoint_files['feature_names'], 'wb') as f:\n",
        "            pickle.dump(feature_names, f)\n",
        "\n",
        "        self.pipeline_state['features_enhanced'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(f\"\\n📈 Feature Engineering Summary:\")\n",
        "        print(f\"   Original features: {x.shape[1]}\")\n",
        "        print(f\"   Enhanced features: {X_enhanced.shape[1]}\")\n",
        "        print(f\"   Enhancement factor: {X_enhanced.shape[1] / x.shape[1]:.1f}x\")\n",
        "        print(f\"💾 Enhanced features saved to checkpoint\")\n",
        "\n",
        "        return X_enhanced, feature_names\n",
        "\n",
        "    def select_best_features(self, X_enhanced=None, y=None, max_features=200, force_reselect=False):\n",
        "        \"\"\"Select best features with checkpoint support\"\"\"\n",
        "        if self.pipeline_state['features_selected'] and not force_reselect:\n",
        "            print(\"🎯 Feature selection already done, loading from checkpoint...\")\n",
        "            X_selected = np.load(self.checkpoint_files['selected_features'])\n",
        "            selected_indices = np.load(self.checkpoint_files['selected_indices'])\n",
        "            with open(self.checkpoint_files['selector'], 'rb') as f:\n",
        "                selector = pickle.load(f)\n",
        "            print(f\"✅ Loaded selected features: {X_selected.shape}\")\n",
        "            return X_selected, selected_indices, selector\n",
        "\n",
        "        # Load data if not provided\n",
        "        if X_enhanced is None:\n",
        "            X_enhanced = np.load(self.checkpoint_files['enhanced_features'])\n",
        "        if y is None:\n",
        "            y = np.load(f\"{self.checkpoint_dir}original_y.npy\")\n",
        "\n",
        "        print(f\"🎯 Selecting best {max_features} features from {X_enhanced.shape[1]}...\")\n",
        "\n",
        "        # Use mutual information for non-linear feature selection\n",
        "        selector = SelectKBest(mutual_info_regression, k=min(max_features, X_enhanced.shape[1]))\n",
        "        X_selected = selector.fit_transform(X_enhanced, y)\n",
        "\n",
        "        # Get selected feature indices\n",
        "        selected_indices = selector.get_support(indices=True)\n",
        "        selected_scores = selector.scores_[selected_indices]\n",
        "\n",
        "        # Save feature selection\n",
        "        np.save(self.checkpoint_files['selected_features'], X_selected)\n",
        "        np.save(self.checkpoint_files['selected_indices'], selected_indices)\n",
        "        with open(self.checkpoint_files['selector'], 'wb') as f:\n",
        "            pickle.dump(selector, f)\n",
        "\n",
        "        self.pipeline_state['features_selected'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(f\"   Selected {X_selected.shape[1]} features\")\n",
        "        print(f\"   Score range: [{np.min(selected_scores):.6f}, {np.max(selected_scores):.6f}]\")\n",
        "        print(f\"💾 Feature selection saved to checkpoint\")\n",
        "\n",
        "        return X_selected, selected_indices, selector\n",
        "\n",
        "    def split_and_scale_data(self, X_selected=None, y=None, force_resplit=False):\n",
        "        \"\"\"Split and scale data with checkpoint support\"\"\"\n",
        "        if self.pipeline_state['data_scaled'] and not force_resplit:\n",
        "            print(\"📊 Data already split and scaled, loading from checkpoint...\")\n",
        "            data = np.load(self.checkpoint_files['split_data'])\n",
        "            with open(self.checkpoint_files['scalers'], 'rb') as f:\n",
        "                scalers = pickle.load(f)\n",
        "\n",
        "            self.x_scaler = scalers['x_scaler']\n",
        "            self.y_scaler = scalers['y_scaler']\n",
        "\n",
        "            return (data['x_train_scaled'], data['x_val_scaled'], data['x_test_scaled'],\n",
        "                   data['y_train_scaled'], data['y_val_scaled'], data['y_test_scaled'])\n",
        "\n",
        "        # Load data if not provided\n",
        "        if X_selected is None:\n",
        "            X_selected = np.load(self.checkpoint_files['selected_features'])\n",
        "        if y is None:\n",
        "            y = np.load(f\"{self.checkpoint_dir}original_y.npy\")\n",
        "\n",
        "        print(f\"📊 Splitting and scaling data...\")\n",
        "\n",
        "        # Split data\n",
        "        x_temp, x_test, y_temp, y_test = train_test_split(\n",
        "            X_selected, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        x_train, x_val, y_train, y_val = train_test_split(\n",
        "            x_temp, y_temp, test_size=0.25, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features and targets\n",
        "        x_train_scaled = self.x_scaler.fit_transform(x_train)\n",
        "        x_val_scaled = self.x_scaler.transform(x_val)\n",
        "        x_test_scaled = self.x_scaler.transform(x_test)\n",
        "\n",
        "        y_train_scaled = self.y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "        y_val_scaled = self.y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
        "        y_test_scaled = self.y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Save split and scaled data\n",
        "        np.savez(self.checkpoint_files['split_data'],\n",
        "                x_train_scaled=x_train_scaled, x_val_scaled=x_val_scaled, x_test_scaled=x_test_scaled,\n",
        "                y_train_scaled=y_train_scaled, y_val_scaled=y_val_scaled, y_test_scaled=y_test_scaled)\n",
        "\n",
        "        with open(self.checkpoint_files['scalers'], 'wb') as f:\n",
        "            pickle.dump({'x_scaler': self.x_scaler, 'y_scaler': self.y_scaler}, f)\n",
        "\n",
        "        self.pipeline_state['data_split'] = True\n",
        "        self.pipeline_state['data_scaled'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(f\"   Train: {x_train_scaled.shape[0]} samples, {x_train_scaled.shape[1]} features\")\n",
        "        print(f\"   Validation: {x_val_scaled.shape[0]} samples\")\n",
        "        print(f\"   Test: {x_test_scaled.shape[0]} samples\")\n",
        "        print(f\"💾 Split and scaled data saved to checkpoint\")\n",
        "\n",
        "        return (x_train_scaled, x_val_scaled, x_test_scaled,\n",
        "               y_train_scaled, y_val_scaled, y_test_scaled)\n",
        "\n",
        "    def build_interaction_focused_mlp(self, input_dim):\n",
        "        \"\"\"Build MLP optimized for learning feature interactions\"\"\"\n",
        "        print(f\"🏗️ Building interaction-focused MLP for {input_dim} features...\")\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(4096, input_dim=input_dim, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Dense(2048, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Dense(1024, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(512, activation='relu',\n",
        "                  kernel_regularizer=l1_l2(0.0001, 0.001)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(256, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "\n",
        "            Dense(32, activation='relu'),\n",
        "\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(\n",
        "            learning_rate=0.002,\n",
        "            clipnorm=1.0,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            loss='mae',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mse', 'mae']\n",
        "        )\n",
        "\n",
        "        print(f\"   Model parameters: {model.count_params():,}\")\n",
        "        return model\n",
        "\n",
        "    def train_enhanced_model(self, force_retrain=False):\n",
        "        \"\"\"Train model with resume capability\"\"\"\n",
        "        if self.pipeline_state['model_trained'] and not force_retrain:\n",
        "            print(\"🚀 Model already trained, loading from checkpoint...\")\n",
        "            if os.path.exists(self.checkpoint_files['final_model']):\n",
        "                self.model = load_model(self.checkpoint_files['final_model'])\n",
        "                print(\"✅ Loaded trained model\")\n",
        "                return None\n",
        "            else:\n",
        "                print(\"⚠️  Final model not found, will retrain...\")\n",
        "\n",
        "        # Load scaled data\n",
        "        data = np.load(self.checkpoint_files['split_data'])\n",
        "        x_train_scaled = data['x_train_scaled']\n",
        "        x_val_scaled = data['x_val_scaled']\n",
        "        y_train_scaled = data['y_train_scaled']\n",
        "        y_val_scaled = data['y_val_scaled']\n",
        "\n",
        "        print(f\"🚀 Training enhanced feature model...\")\n",
        "\n",
        "        # Build model\n",
        "        self.model = self.build_interaction_focused_mlp(x_train_scaled.shape[1])\n",
        "\n",
        "        # Check for existing training checkpoint\n",
        "        initial_epoch = 0\n",
        "        if os.path.exists(self.checkpoint_files['training_state']) and not force_retrain:\n",
        "            try:\n",
        "                with open(self.checkpoint_files['training_state'], 'r') as f:\n",
        "                    training_state = json.load(f)\n",
        "                initial_epoch = training_state.get('last_epoch', 0)\n",
        "                if initial_epoch > 0:\n",
        "                    self.model.load_weights(self.checkpoint_files['model_weights'])\n",
        "                    print(f\"📋 Resuming training from epoch {initial_epoch}\")\n",
        "            except:\n",
        "                print(\"⚠️  Could not load training checkpoint, starting fresh\")\n",
        "                initial_epoch = 0\n",
        "\n",
        "        # Custom callback to save training state\n",
        "        class TrainingStateCallback(keras.callbacks.Callback):\n",
        "            def __init__(self, state_file, weights_file):\n",
        "                self.state_file = state_file\n",
        "                self.weights_file = weights_file\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                # Save every 50 epochs\n",
        "                if epoch % 50 == 0:\n",
        "                    state = {\n",
        "                        'last_epoch': epoch + 1,\n",
        "                        'val_loss': float(logs.get('val_loss', 0)),\n",
        "                        'loss': float(logs.get('loss', 0))\n",
        "                    }\n",
        "                    with open(self.state_file, 'w') as f:\n",
        "                        json.dump(state, f)\n",
        "                    self.model.save_weights(self.weights_file)\n",
        "\n",
        "        # Enhanced callbacks\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(\n",
        "                f\"{self.enhanced_dir}best_enhanced_model.weights.h5\",\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=150,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                min_delta=0.0001\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=50,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            TrainingStateCallback(\n",
        "                self.checkpoint_files['training_state'],\n",
        "                self.checkpoint_files['model_weights']\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            x_train_scaled, y_train_scaled,\n",
        "            validation_data=(x_val_scaled, y_val_scaled),\n",
        "            epochs=1500,\n",
        "            batch_size=64,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            initial_epoch=initial_epoch\n",
        "        )\n",
        "\n",
        "        # Save final model\n",
        "        self.model.save(self.checkpoint_files['final_model'])\n",
        "\n",
        "        self.pipeline_state['model_trained'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(\"✅ Enhanced model training complete and saved!\")\n",
        "        return history\n",
        "\n",
        "    def evaluate_enhanced_model(self, history=None, force_reevaluate=False):\n",
        "        \"\"\"Evaluate model with checkpoint support\"\"\"\n",
        "        if self.pipeline_state['model_evaluated'] and not force_reevaluate:\n",
        "            print(\"📊 Model already evaluated, loading results...\")\n",
        "            with open(self.checkpoint_files['results'], 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"✅ Loaded evaluation results: R²={results['r2']:.6f}\")\n",
        "            return results\n",
        "\n",
        "        print(\"📊 Evaluating enhanced model...\")\n",
        "\n",
        "        # Load model if not in memory\n",
        "        if self.model is None:\n",
        "            self.model = load_model(self.checkpoint_files['final_model'])\n",
        "\n",
        "        # Load scalers and test data\n",
        "        data = np.load(self.checkpoint_files['split_data'])\n",
        "        with open(self.checkpoint_files['scalers'], 'rb') as f:\n",
        "            scalers = pickle.load(f)\n",
        "\n",
        "        self.x_scaler = scalers['x_scaler']\n",
        "        self.y_scaler = scalers['y_scaler']\n",
        "\n",
        "        x_test_scaled = data['x_test_scaled']\n",
        "        y_test_scaled = data['y_test_scaled']\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_scaled = self.model.predict(x_test_scaled, verbose=0)\n",
        "        y_pred = self.y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "        y_true = self.y_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Accuracy metrics\n",
        "        acc_05 = np.mean(np.abs(y_true - y_pred) <= 0.5) * 100\n",
        "        acc_1 = np.mean(np.abs(y_true - y_pred) <= 1) * 100\n",
        "        acc_15 = np.mean(np.abs(y_true - y_pred) <= 1.5) * 100\n",
        "        acc_2 = np.mean(np.abs(y_true - y_pred) <= 2) * 100\n",
        "\n",
        "        results = {\n",
        "            'r2': r2, 'mse': mse, 'mae': mae, 'rmse': rmse,\n",
        "            'acc_05': acc_05, 'acc_1': acc_1, 'acc_15': acc_15, 'acc_2': acc_2,\n",
        "            'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        # Save results\n",
        "        with open(self.checkpoint_files['results'], 'wb') as f:\n",
        "            pickle.dump(results, f)\n",
        "\n",
        "        self.pipeline_state['model_evaluated'] = True\n",
        "        self.save_pipeline_state()\n",
        "\n",
        "        print(f\"📈 Enhanced Model Results:\")\n",
        "        print(f\"   R²: {r2:.6f}\")\n",
        "        print(f\"   MAE: {mae:.4f}\")\n",
        "        print(f\"   RMSE: {rmse:.4f}\")\n",
        "        print(f\"   Accuracy ±0.5: {acc_05:.1f}%\")\n",
        "        print(f\"   Accuracy ±1.0: {acc_1:.1f}%\")\n",
        "        print(f\"   Accuracy ±1.5: {acc_15:.1f}%\")\n",
        "        print(f\"   Accuracy ±2.0: {acc_2:.1f}%\")\n",
        "        print(f\"💾 Results saved to checkpoint\")\n",
        "\n",
        "        # Plot results if history available\n",
        "        if history:\n",
        "            self.plot_enhanced_results(y_true, y_pred, history, r2, mae)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_enhanced_results(self, y_true, y_pred, history, r2, mae):\n",
        "        \"\"\"Plot comprehensive results\"\"\"\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
        "\n",
        "        # Training history plots and other visualizations\n",
        "        # (same as original but with checkpoint awareness)\n",
        "\n",
        "        # Training history - Loss\n",
        "        axes[0,0].plot(history.history['loss'], label='Training', linewidth=2)\n",
        "        axes[0,0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "        axes[0,0].set_title('Enhanced Model Loss')\n",
        "        axes[0,0].set_yscale('log')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Training history - MAE\n",
        "        axes[0,1].plot(history.history['mae'], label='Training', linewidth=2)\n",
        "        axes[0,1].plot(history.history['val_mae'], label='Validation', linewidth=2)\n",
        "        axes[0,1].set_title('Enhanced Model MAE')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Predictions scatter\n",
        "        axes[0,2].scatter(y_true, y_pred, alpha=0.6, s=3, color='darkblue')\n",
        "        axes[0,2].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "        axes[0,2].set_xlabel('True Values')\n",
        "        axes[0,2].set_ylabel('Predictions')\n",
        "        axes[0,2].set_title(f'Enhanced Predictions\\n(R²={r2:.6f})')\n",
        "        axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residuals and other plots\n",
        "        residuals = y_true - y_pred\n",
        "        axes[1,0].scatter(y_pred, residuals, alpha=0.6, s=3, color='green')\n",
        "        axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
        "        axes[1,0].set_xlabel('Predictions')\n",
        "        axes[1,0].set_ylabel('Residuals')\n",
        "        axes[1,0].set_title('Enhanced Model Residuals')\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.enhanced_dir}enhanced_results.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def run_enhanced_pipeline(self, data_filename, windows_filename, force_restart=False):\n",
        "        \"\"\"Run complete enhanced pipeline with resume capability\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"🚀 RESUMABLE ENHANCED FEATURE ENGINEERING MLP PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if force_restart:\n",
        "            print(\"🔄 Force restart requested, clearing all checkpoints...\")\n",
        "            self.clear_checkpoints()\n",
        "            self.pipeline_state = self.load_pipeline_state()\n",
        "\n",
        "        try:\n",
        "            # 1. Load original data\n",
        "            print(f\"\\n📋 Step 1/7: Loading Data\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['data_loaded'] else '⏳ Pending'}\")\n",
        "            x, y = self.load_original_data(data_filename, windows_filename)\n",
        "\n",
        "            # 2. Create enhanced features\n",
        "            print(f\"\\n📋 Step 2/7: Feature Engineering\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['features_enhanced'] else '⏳ Pending'}\")\n",
        "            X_enhanced, feature_names = self.create_enhanced_features(x)\n",
        "\n",
        "            # 3. Select best features\n",
        "            print(f\"\\n📋 Step 3/7: Feature Selection\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['features_selected'] else '⏳ Pending'}\")\n",
        "            X_selected, selected_indices, selector = self.select_best_features(X_enhanced, y)\n",
        "\n",
        "            # 4. Split and scale data\n",
        "            print(f\"\\n📋 Step 4/7: Data Preparation\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['data_scaled'] else '⏳ Pending'}\")\n",
        "            scaled_data = self.split_and_scale_data(X_selected, y)\n",
        "\n",
        "            # 5. Train model\n",
        "            print(f\"\\n📋 Step 5/7: Model Training\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['model_trained'] else '⏳ Pending'}\")\n",
        "            history = self.train_enhanced_model()\n",
        "\n",
        "            # 6. Evaluate model\n",
        "            print(f\"\\n📋 Step 6/7: Model Evaluation\")\n",
        "            print(f\"   Status: {'✅ Complete' if self.pipeline_state['model_evaluated'] else '⏳ Pending'}\")\n",
        "            results = self.evaluate_enhanced_model(history)\n",
        "\n",
        "            # 7. Mark pipeline complete\n",
        "            print(f\"\\n📋 Step 7/7: Pipeline Completion\")\n",
        "            self.pipeline_state['pipeline_complete'] = True\n",
        "            self.save_pipeline_state()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"🎉 RESUMABLE ENHANCED FEATURE ENGINEERING COMPLETE!\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Final R²: {results['r2']:.6f}\")\n",
        "            print(f\"Final MAE: {results['mae']:.4f}\")\n",
        "            print(f\"Accuracy ±2: {results['acc_2']:.1f}%\")\n",
        "            print(f\"Model saved: {self.checkpoint_files['final_model']}\")\n",
        "            print(f\"All checkpoints saved in: {self.checkpoint_dir}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            return {\n",
        "                'model': self.model,\n",
        "                'results': results,\n",
        "                'history': history,\n",
        "                'feature_names': feature_names,\n",
        "                'selected_indices': selected_indices,\n",
        "                'x_scaler': self.x_scaler,\n",
        "                'y_scaler': self.y_scaler,\n",
        "                'checkpoint_dir': self.checkpoint_dir\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Enhanced pipeline failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def clear_checkpoints(self):\n",
        "        \"\"\"Clear all checkpoint files\"\"\"\n",
        "        print(\"🗑️ Clearing all checkpoints...\")\n",
        "        for name, filepath in self.checkpoint_files.items():\n",
        "            if os.path.exists(filepath):\n",
        "                os.remove(filepath)\n",
        "                print(f\"   Removed: {name}\")\n",
        "\n",
        "        # Reset pipeline state\n",
        "        state_file = f\"{self.checkpoint_dir}pipeline_state.json\"\n",
        "        if os.path.exists(state_file):\n",
        "            os.remove(state_file)\n",
        "\n",
        "        # Remove original data files\n",
        "        orig_files = [\n",
        "            f\"{self.checkpoint_dir}original_x.npy\",\n",
        "            f\"{self.checkpoint_dir}original_y.npy\"\n",
        "        ]\n",
        "        for filepath in orig_files:\n",
        "            if os.path.exists(filepath):\n",
        "                os.remove(filepath)\n",
        "\n",
        "        print(\"✅ All checkpoints cleared\")\n",
        "\n",
        "    def get_pipeline_status(self):\n",
        "        \"\"\"Get detailed pipeline status\"\"\"\n",
        "        print(\"\\n📋 PIPELINE STATUS REPORT\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        steps = [\n",
        "            ('Data Loading', 'data_loaded'),\n",
        "            ('Feature Engineering', 'features_enhanced'),\n",
        "            ('Feature Selection', 'features_selected'),\n",
        "            ('Data Preparation', 'data_scaled'),\n",
        "            ('Model Training', 'model_trained'),\n",
        "            ('Model Evaluation', 'model_evaluated'),\n",
        "            ('Pipeline Complete', 'pipeline_complete')\n",
        "        ]\n",
        "\n",
        "        for i, (step_name, state_key) in enumerate(steps, 1):\n",
        "            status = \"✅ Complete\" if self.pipeline_state.get(state_key, False) else \"⏳ Pending\"\n",
        "            print(f\"{i}/7: {step_name:<20} {status}\")\n",
        "\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Show checkpoint file status\n",
        "        print(\"\\n💾 CHECKPOINT FILES STATUS\")\n",
        "        print(\"=\"*50)\n",
        "        for name, filepath in self.checkpoint_files.items():\n",
        "            exists = \"✅\" if os.path.exists(filepath) else \"❌\"\n",
        "            size = \"\"\n",
        "            if os.path.exists(filepath):\n",
        "                size_bytes = os.path.getsize(filepath)\n",
        "                if size_bytes > 1024*1024:\n",
        "                    size = f\" ({size_bytes/(1024*1024):.1f}MB)\"\n",
        "                elif size_bytes > 1024:\n",
        "                    size = f\" ({size_bytes/1024:.1f}KB)\"\n",
        "                else:\n",
        "                    size = f\" ({size_bytes}B)\"\n",
        "            print(f\"{exists} {name:<20} {size}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    def resume_from_step(self, step_name):\n",
        "        \"\"\"Resume pipeline from a specific step\"\"\"\n",
        "        step_mapping = {\n",
        "            'data': 'data_loaded',\n",
        "            'features': 'features_enhanced',\n",
        "            'selection': 'features_selected',\n",
        "            'preparation': 'data_scaled',\n",
        "            'training': 'model_trained',\n",
        "            'evaluation': 'model_evaluated'\n",
        "        }\n",
        "\n",
        "        if step_name not in step_mapping:\n",
        "            print(f\"❌ Unknown step: {step_name}\")\n",
        "            print(f\"Available steps: {list(step_mapping.keys())}\")\n",
        "            return\n",
        "\n",
        "        # Reset pipeline state from the specified step onwards\n",
        "        state_key = step_mapping[step_name]\n",
        "        reset_from = False\n",
        "\n",
        "        for key in self.pipeline_state:\n",
        "            if key == state_key:\n",
        "                reset_from = True\n",
        "            if reset_from:\n",
        "                self.pipeline_state[key] = False\n",
        "\n",
        "        self.save_pipeline_state()\n",
        "        print(f\"🔄 Pipeline reset from step: {step_name}\")\n",
        "        self.get_pipeline_status()\n",
        "\n",
        "\n",
        "# Enhanced interface functions\n",
        "def run_resumable_enhanced_feature_mlp(data_filename, windows_filename, force_restart=False):\n",
        "    \"\"\"\n",
        "    Run resumable enhanced feature engineering MLP\n",
        "\n",
        "    Args:\n",
        "        data_filename: e.g., 'generated-data-OPTIMIZED.npy'\n",
        "        windows_filename: e.g., 'generated-data-true-window-OPTIMIZED.npy'\n",
        "        force_restart: If True, clear all checkpoints and start fresh\n",
        "    \"\"\"\n",
        "    enhanced_mlp = ResumableEnhancedFeatureEngineeringMLP()\n",
        "    return enhanced_mlp.run_enhanced_pipeline(data_filename, windows_filename, force_restart)\n",
        "\n",
        "def check_pipeline_status(output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "    \"\"\"Check the status of the pipeline\"\"\"\n",
        "    enhanced_mlp = ResumableEnhancedFeatureEngineeringMLP(output_dir)\n",
        "    enhanced_mlp.get_pipeline_status()\n",
        "    return enhanced_mlp\n",
        "\n",
        "def clear_all_checkpoints(output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "    \"\"\"Clear all checkpoints and start fresh\"\"\"\n",
        "    enhanced_mlp = ResumableEnhancedFeatureEngineeringMLP(output_dir)\n",
        "    enhanced_mlp.clear_checkpoints()\n",
        "    return enhanced_mlp\n",
        "\n",
        "def resume_from_step(step_name, output_dir='/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'):\n",
        "    \"\"\"Resume pipeline from a specific step\"\"\"\n",
        "    enhanced_mlp = ResumableEnhancedFeatureEngineeringMLP(output_dir)\n",
        "    enhanced_mlp.resume_from_step(step_name)\n",
        "    return enhanced_mlp\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Resumable Enhanced Feature Engineering MLP\")\n",
        "\n",
        "    # SPECIFY YOUR FILES\n",
        "    data_file = 'generated-data-OPTIMIZED.npy'\n",
        "    windows_file = 'generated-data-true-window-OPTIMIZED.npy'\n",
        "\n",
        "    print(f\"Running resumable enhanced pipeline on: {data_file}, {windows_file}\")\n",
        "\n",
        "    # Check current status first\n",
        "    mlp = check_pipeline_status()\n",
        "\n",
        "    # Run pipeline (will resume automatically)\n",
        "    results = run_resumable_enhanced_feature_mlp(data_file, windows_file)\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\n🎊 Enhanced model complete!\")\n",
        "        print(f\"R² improvement: {results['results']['r2']:.6f}\")\n",
        "        print(f\"Accuracy ±2: {results['results']['acc_2']:.1f}%\")\n",
        "        print(f\"Checkpoints saved in: {results['checkpoint_dir']}\")\n",
        "    else:\n",
        "        print(\"❌ Enhanced pipeline failed\")\n",
        "\n",
        "# Usage examples\n",
        "\"\"\"\n",
        "# 1. Run pipeline (will resume automatically if crashed)\n",
        "results = run_resumable_enhanced_feature_mlp(\n",
        "    'generated-data-OPTIMIZED.npy',\n",
        "    'generated-data-true-window-OPTIMIZED.npy'\n",
        ")\n",
        "\n",
        "# 2. Check pipeline status\n",
        "mlp = check_pipeline_status()\n",
        "\n",
        "# 3. Force restart from beginning\n",
        "results = run_resumable_enhanced_feature_mlp(\n",
        "    'generated-data-OPTIMIZED.npy',\n",
        "    'generated-data-true-window-OPTIMIZED.npy',\n",
        "    force_restart=True\n",
        ")\n",
        "\n",
        "# 4. Resume from specific step (if you want to rerun from training)\n",
        "mlp = resume_from_step('training')\n",
        "results = run_resumable_enhanced_feature_mlp(\n",
        "    'generated-data-OPTIMIZED.npy',\n",
        "    'generated-data-true-window-OPTIMIZED.npy'\n",
        ")\n",
        "\n",
        "# 5. Clear all checkpoints\n",
        "mlp = clear_all_checkpoints()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE3-MLP.ipynb",
      "authorship_tag": "ABX9TyPqimWqkQZJi47GQtUhZTnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}