{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HoP7OuWNxlsJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Sensor Model Pre-Training System\n",
        "================================\n",
        "\n",
        "This script handles the initial training of sensor models using your real dataset.\n",
        "Models are trained once and saved to disk for use in the production agent system.\n",
        "\n",
        "Usage:\n",
        "    python sensor_pretraining.py --data_path /path/to/dataset --models_dir ./trained_models\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ TensorFlow not available. Install with: pip install tensorflow\")\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SensorModelTrainer:\n",
        "    \"\"\"\n",
        "    Handles training and saving of individual sensor models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 window_length: int = 50,\n",
        "                 model_type: str = 'lstm_autoencoder',\n",
        "                 latent_dim: int = 32,\n",
        "                 epochs: int = 100,\n",
        "                 batch_size: int = 32,\n",
        "                 validation_split: float = 0.2):\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.model_type = model_type\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise ImportError(\"TensorFlow required for model training\")\n",
        "\n",
        "    def build_lstm_autoencoder(self) -> Model:\n",
        "        \"\"\"Build LSTM Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1), name='encoder_input')\n",
        "        encoded = LSTM(self.latent_dim, activation='relu', return_sequences=False, name='encoder_lstm')(inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = RepeatVector(self.window_length, name='repeat_vector')(encoded)\n",
        "        decoded = LSTM(self.latent_dim, activation='relu', return_sequences=True, name='decoder_lstm')(decoded)\n",
        "        outputs = TimeDistributed(Dense(1, activation='linear'), name='decoder_output')(decoded)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs, outputs, name='sensor_lstm_autoencoder')\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_vae(self) -> Model:\n",
        "        \"\"\"Build Variational Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1))\n",
        "        x = LSTM(self.latent_dim, return_sequences=False)(inputs)\n",
        "\n",
        "        # Latent space\n",
        "        z_mean = Dense(self.latent_dim // 2, name='z_mean')(x)\n",
        "        z_log_var = Dense(self.latent_dim // 2, name='z_log_var')(x)\n",
        "\n",
        "        # Sampling function\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = tf.keras.layers.Lambda(sampling, name='sampling')([z_mean, z_log_var])\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = RepeatVector(self.window_length)(z)\n",
        "        decoded = LSTM(self.latent_dim, return_sequences=True)(decoder_input)\n",
        "        outputs = TimeDistributed(Dense(1))(decoded)\n",
        "\n",
        "        # VAE model\n",
        "        model = Model(inputs, outputs, name='sensor_vae')\n",
        "\n",
        "        # VAE loss\n",
        "        reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        vae_loss = reconstruction_loss + 0.1 * kl_loss\n",
        "        model.add_loss(vae_loss)\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, sensor_data: np.ndarray, overlap_ratio: float = 0.5) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert time series data into sequences for training.\n",
        "\n",
        "        Args:\n",
        "            sensor_data: 1D time series data for one sensor\n",
        "            overlap_ratio: Overlap between consecutive sequences (0.0 = no overlap, 0.9 = high overlap)\n",
        "\n",
        "        Returns:\n",
        "            Array of sequences [n_sequences, window_length]\n",
        "        \"\"\"\n",
        "\n",
        "        if len(sensor_data) < self.window_length:\n",
        "            raise ValueError(f\"Data length ({len(sensor_data)}) < window_length ({self.window_length})\")\n",
        "\n",
        "        # Calculate step size based on overlap\n",
        "        step_size = max(1, int(self.window_length * (1 - overlap_ratio)))\n",
        "\n",
        "        sequences = []\n",
        "        for i in range(0, len(sensor_data) - self.window_length + 1, step_size):\n",
        "            sequences.append(sensor_data[i:i + self.window_length])\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def train_sensor_model(self, sensor_data: np.ndarray, sensor_id: int) -> Tuple[Model, Dict]:\n",
        "        \"\"\"\n",
        "        Train a model for a single sensor.\n",
        "\n",
        "        Args:\n",
        "            sensor_data: Pre-scaled 1D time series data\n",
        "            sensor_id: Sensor identifier\n",
        "\n",
        "        Returns:\n",
        "            Trained model and training history\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Training model for sensor {sensor_id}...\")\n",
        "        print(f\"  Data shape: {sensor_data.shape}\")\n",
        "\n",
        "        # Create sequences\n",
        "        sequences = self.prepare_sequences(sensor_data, overlap_ratio=0.3)\n",
        "        print(f\"  Created {len(sequences)} sequences of length {self.window_length}\")\n",
        "\n",
        "        if len(sequences) < 100:\n",
        "            raise ValueError(f\"Insufficient sequences ({len(sequences)}) for training. Need at least 100.\")\n",
        "\n",
        "        # Prepare training data\n",
        "        X = sequences.reshape(len(sequences), self.window_length, 1)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val = train_test_split(X, test_size=self.validation_split, random_state=42)\n",
        "        print(f\"  Train sequences: {len(X_train)}, Validation sequences: {len(X_val)}\")\n",
        "\n",
        "        # Build model\n",
        "        if self.model_type == 'lstm_autoencoder':\n",
        "            model = self.build_lstm_autoencoder()\n",
        "        elif self.model_type == 'vae':\n",
        "            model = self.build_vae()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
        "\n",
        "        print(f\"  Model: {self.model_type} with {model.count_params():,} parameters\")\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        if self.model_type == 'vae':\n",
        "            # VAE training (no target needed due to custom loss)\n",
        "            history = model.fit(\n",
        "                X_train, epochs=self.epochs, batch_size=self.batch_size,\n",
        "                validation_data=(X_val,), callbacks=callbacks, verbose=1\n",
        "            )\n",
        "        else:\n",
        "            # Autoencoder training\n",
        "            history = model.fit(\n",
        "                X_train, X_train, epochs=self.epochs, batch_size=self.batch_size,\n",
        "                validation_data=(X_val, X_val), callbacks=callbacks, verbose=1\n",
        "            )\n",
        "\n",
        "        training_time = datetime.now() - start_time\n",
        "        print(f\"  Training completed in {training_time}\")\n",
        "\n",
        "        # Compute baseline errors for anomaly detection\n",
        "        print(\"  Computing baseline error statistics...\")\n",
        "        baseline_errors = []\n",
        "\n",
        "        for i in range(0, len(X_val), self.batch_size):\n",
        "            batch = X_val[i:i + self.batch_size]\n",
        "            predictions = model.predict(batch, verbose=0)\n",
        "\n",
        "            for j, pred in enumerate(predictions):\n",
        "                error = mean_squared_error(batch[j].flatten(), pred.flatten())\n",
        "                baseline_errors.append(error)\n",
        "\n",
        "        baseline_stats = {\n",
        "            'mean': float(np.mean(baseline_errors)),\n",
        "            'std': float(np.std(baseline_errors)) + 1e-8,\n",
        "            'q95': float(np.percentile(baseline_errors, 95)),\n",
        "            'q99': float(np.percentile(baseline_errors, 99)),\n",
        "            'min': float(np.min(baseline_errors)),\n",
        "            'max': float(np.max(baseline_errors))\n",
        "        }\n",
        "\n",
        "        training_info = {\n",
        "            'sensor_id': sensor_id,\n",
        "            'model_type': self.model_type,\n",
        "            'window_length': self.window_length,\n",
        "            'training_sequences': len(X_train),\n",
        "            'validation_sequences': len(X_val),\n",
        "            'training_time': str(training_time),\n",
        "            'final_loss': float(history.history['loss'][-1]),\n",
        "            'final_val_loss': float(history.history['val_loss'][-1]),\n",
        "            'epochs_trained': len(history.history['loss']),\n",
        "            'baseline_errors': baseline_errors[-100:],  # Store last 100 for drift detection\n",
        "            'baseline_stats': baseline_stats,\n",
        "            'trained_at': datetime.now()\n",
        "        }\n",
        "\n",
        "        print(f\"  ✅ Training successful!\")\n",
        "        print(f\"     Final loss: {training_info['final_loss']:.6f}\")\n",
        "        print(f\"     Final val loss: {training_info['final_val_loss']:.6f}\")\n",
        "        print(f\"     Baseline error: {baseline_stats['mean']:.6f} ± {baseline_stats['std']:.6f}\")\n",
        "\n",
        "        return model, training_info\n",
        "\n",
        "    def save_model(self, model: Model, training_info: Dict, models_dir: str):\n",
        "        \"\"\"Save trained model and metadata.\"\"\"\n",
        "\n",
        "        sensor_id = training_info['sensor_id']\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Save model\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        model.save(model_path)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_path = os.path.join(models_dir, f\"sensor_{sensor_id}_metadata.pkl\")\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump(training_info, f)\n",
        "\n",
        "        print(f\"  💾 Saved model and metadata for sensor {sensor_id}\")\n",
        "\n",
        "        return model_path, metadata_path\n",
        "\n",
        "\n",
        "def load_your_dataset(data_path: str) -> Dict[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load your real dataset and return sensor data.\n",
        "\n",
        "    CUSTOMIZE THIS FUNCTION FOR YOUR DATA FORMAT\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to your dataset\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping sensor_id -> pre-scaled time series data\n",
        "\n",
        "    Example formats supported:\n",
        "    - CSV with columns: timestamp, sensor_0, sensor_1, ...\n",
        "    - Multiple CSV files: sensor_0.csv, sensor_1.csv, ...\n",
        "    - HDF5 files\n",
        "    - Numpy arrays\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"📂 Loading dataset from: {data_path}\")\n",
        "\n",
        "    sensor_data = {}\n",
        "\n",
        "    if data_path.endswith('.csv'):\n",
        "        # Single CSV file with multiple sensors\n",
        "        print(\"  Format: Single CSV file\")\n",
        "        df = pd.read_csv(data_path)\n",
        "\n",
        "        # Assume columns are: timestamp, sensor_0, sensor_1, sensor_2, ...\n",
        "        sensor_columns = [col for col in df.columns if col.startswith('sensor_')]\n",
        "\n",
        "        for col in sensor_columns:\n",
        "            sensor_id = int(col.split('_')[1])  # Extract sensor ID\n",
        "            data = df[col].values\n",
        "\n",
        "            # Remove NaNs and infinite values\n",
        "            data = data[np.isfinite(data)]\n",
        "\n",
        "            if len(data) > 0:\n",
        "                sensor_data[sensor_id] = data\n",
        "                print(f\"    Sensor {sensor_id}: {len(data)} samples\")\n",
        "\n",
        "    elif data_path.endswith('.h5') or data_path.endswith('.hdf5'):\n",
        "        # HDF5 file\n",
        "        print(\"  Format: HDF5 file\")\n",
        "        import h5py\n",
        "\n",
        "        with h5py.File(data_path, 'r') as f:\n",
        "            for key in f.keys():\n",
        "                if key.startswith('sensor_'):\n",
        "                    sensor_id = int(key.split('_')[1])\n",
        "                    data = f[key][:]\n",
        "                    data = data[np.isfinite(data)]\n",
        "\n",
        "                    if len(data) > 0:\n",
        "                        sensor_data[sensor_id] = data\n",
        "                        print(f\"    Sensor {sensor_id}: {len(data)} samples\")\n",
        "\n",
        "    elif os.path.isdir(data_path):\n",
        "        # Directory with multiple files\n",
        "        print(\"  Format: Directory with multiple sensor files\")\n",
        "\n",
        "        for filename in os.listdir(data_path):\n",
        "            if filename.startswith('sensor_') and filename.endswith('.csv'):\n",
        "                sensor_id = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(data_path, filename)\n",
        "\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Assume single column or use first numeric column\n",
        "                numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "                if len(numeric_cols) > 0:\n",
        "                    data = df[numeric_cols[0]].values\n",
        "                    data = data[np.isfinite(data)]\n",
        "\n",
        "                    if len(data) > 0:\n",
        "                        sensor_data[sensor_id] = data\n",
        "                        print(f\"    Sensor {sensor_id}: {len(data)} samples\")\n",
        "\n",
        "    elif data_path.endswith('.npy'):\n",
        "        # Numpy array [timesteps, sensors]\n",
        "        print(\"  Format: Numpy array\")\n",
        "        data_array = np.load(data_path)\n",
        "\n",
        "        if data_array.ndim == 2:\n",
        "            for sensor_id in range(data_array.shape[1]):\n",
        "                data = data_array[:, sensor_id]\n",
        "                data = data[np.isfinite(data)]\n",
        "\n",
        "                if len(data) > 0:\n",
        "                    sensor_data[sensor_id] = data\n",
        "                    print(f\"    Sensor {sensor_id}: {len(data)} samples\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported data format: {data_path}\")\n",
        "\n",
        "    if not sensor_data:\n",
        "        raise ValueError(\"No sensor data loaded. Check your data format.\")\n",
        "\n",
        "    print(f\"✅ Loaded {len(sensor_data)} sensors\")\n",
        "    return sensor_data\n",
        "\n",
        "\n",
        "def validate_dataset(sensor_data: Dict[int, np.ndarray], window_length: int) -> Dict[int, np.ndarray]:\n",
        "    \"\"\"Validate and prepare dataset for training.\"\"\"\n",
        "\n",
        "    print(\"🔍 Validating dataset...\")\n",
        "\n",
        "    validated_data = {}\n",
        "\n",
        "    for sensor_id, data in sensor_data.items():\n",
        "        print(f\"  Sensor {sensor_id}:\")\n",
        "\n",
        "        # Check data length\n",
        "        if len(data) < window_length * 10:\n",
        "            print(f\"    ⚠️ Insufficient data ({len(data)} < {window_length * 10}) - SKIPPING\")\n",
        "            continue\n",
        "\n",
        "        # Check for constant values\n",
        "        if np.std(data) < 1e-6:\n",
        "            print(f\"    ⚠️ Constant values detected (std={np.std(data):.2e}) - SKIPPING\")\n",
        "            continue\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"    📊 Length: {len(data)}\")\n",
        "        print(f\"    📊 Range: [{np.min(data):.3f}, {np.max(data):.3f}]\")\n",
        "        print(f\"    📊 Mean: {np.mean(data):.3f}, Std: {np.std(data):.3f}\")\n",
        "        print(f\"    📊 Missing values: {np.sum(~np.isfinite(data))}\")\n",
        "\n",
        "        # Check if data appears to be already scaled\n",
        "        if -5 <= np.min(data) <= 5 and -5 <= np.max(data) <= 5:\n",
        "            print(f\"    ✅ Data appears pre-scaled\")\n",
        "        else:\n",
        "            print(f\"    ⚠️ Data may need scaling (range: [{np.min(data):.2f}, {np.max(data):.2f}])\")\n",
        "\n",
        "        validated_data[sensor_id] = data\n",
        "\n",
        "    print(f\"✅ Validated {len(validated_data)} sensors for training\")\n",
        "    return validated_data\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Train sensor anomaly detection models\")\n",
        "    parser.add_argument('--data_path', type=str, required=True, help='Path to dataset')\n",
        "    parser.add_argument('--models_dir', type=str, default='./trained_models', help='Directory to save models')\n",
        "    parser.add_argument('--window_length', type=int, default=50, help='Sequence window length')\n",
        "    parser.add_argument('--model_type', type=str, default='lstm_autoencoder',\n",
        "                       choices=['lstm_autoencoder', 'vae'], help='Model type')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    parser.add_argument('--sensors', type=str, default=None,\n",
        "                       help='Comma-separated sensor IDs to train (default: all)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"🚀 SENSOR MODEL PRE-TRAINING SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"📂 Data path: {args.data_path}\")\n",
        "    print(f\"💾 Models directory: {args.models_dir}\")\n",
        "    print(f\"📏 Window length: {args.window_length}\")\n",
        "    print(f\"🧠 Model type: {args.model_type}\")\n",
        "    print(f\"🔄 Epochs: {args.epochs}\")\n",
        "    print()\n",
        "\n",
        "    # Load dataset\n",
        "    sensor_data = load_your_dataset(args.data_path)\n",
        "\n",
        "    # Validate dataset\n",
        "    validated_data = validate_dataset(sensor_data, args.window_length)\n",
        "\n",
        "    if not validated_data:\n",
        "        print(\"❌ No valid sensors found for training\")\n",
        "        return\n",
        "\n",
        "    # Filter sensors if specified\n",
        "    if args.sensors:\n",
        "        requested_sensors = [int(x.strip()) for x in args.sensors.split(',')]\n",
        "        validated_data = {sid: data for sid, data in validated_data.items()\n",
        "                         if sid in requested_sensors}\n",
        "        print(f\"🎯 Training only sensors: {list(validated_data.keys())}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SensorModelTrainer(\n",
        "        window_length=args.window_length,\n",
        "        model_type=args.model_type,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    # Train models\n",
        "    print(f\"\\n🏋️ TRAINING {len(validated_data)} SENSOR MODELS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    training_results = {}\n",
        "    successful_training = 0\n",
        "\n",
        "    for sensor_id, data in validated_data.items():\n",
        "        try:\n",
        "            print(f\"\\n🎯 SENSOR {sensor_id}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Train model\n",
        "            model, training_info = trainer.train_sensor_model(data, sensor_id)\n",
        "\n",
        "            # Save model\n",
        "            model_path, metadata_path = trainer.save_model(model, training_info, args.models_dir)\n",
        "\n",
        "            training_results[sensor_id] = {\n",
        "                'success': True,\n",
        "                'model_path': model_path,\n",
        "                'metadata_path': metadata_path,\n",
        "                'training_info': training_info\n",
        "            }\n",
        "\n",
        "            successful_training += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Training failed: {str(e)}\")\n",
        "            training_results[sensor_id] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n📊 TRAINING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"✅ Successful: {successful_training}/{len(validated_data)} sensors\")\n",
        "    print(f\"💾 Models saved to: {args.models_dir}\")\n",
        "\n",
        "    if successful_training > 0:\n",
        "        print(f\"\\n🏆 TRAINED SENSORS:\")\n",
        "        for sensor_id, result in training_results.items():\n",
        "            if result['success']:\n",
        "                info = result['training_info']\n",
        "                print(f\"  Sensor {sensor_id}: {info['epochs_trained']} epochs, \"\n",
        "                      f\"final loss: {info['final_loss']:.6f}\")\n",
        "\n",
        "    failed_sensors = [sid for sid, result in training_results.items() if not result['success']]\n",
        "    if failed_sensors:\n",
        "        print(f\"\\n❌ FAILED SENSORS: {failed_sensors}\")\n",
        "\n",
        "    # Save training summary\n",
        "    summary_path = os.path.join(args.models_dir, 'training_summary.pkl')\n",
        "    with open(summary_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'training_results': training_results,\n",
        "            'config': vars(args),\n",
        "            'timestamp': datetime.now()\n",
        "        }, f)\n",
        "\n",
        "    print(f\"\\n💾 Training summary saved to: {summary_path}\")\n",
        "    print(f\"✅ PRE-TRAINING COMPLETED!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/SensorAgent.ipynb",
      "authorship_tag": "ABX9TyN3rmBybJEg+T+XsfhBIQGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}