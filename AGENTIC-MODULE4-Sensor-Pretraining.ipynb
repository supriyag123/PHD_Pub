{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e111a8cf-415e-4fee-963e-0bf1c2c39279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Simple Sensor Pre-Training\n",
            "========================================\n",
            "Loading data...\n",
            "Total AE windows: 432142\n",
            "AE data shape: (432142, 100, 12)\n",
            "Will train 12 sensors\n",
            "\n",
            "‚ö° Launching parallel sensor training...\n",
            "\n",
            "\n",
            "üîß Launching Worker for Sensor 0 (1/12) ‚Äî remaining: 11\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Simple Sensor Pre-Training System\n",
        "=================================\n",
        "\n",
        "Trains LSTM Autoencoders for each sensor and saves models with baseline statistics.\n",
        "\n",
        "Usage:\n",
        "    python sensor_pretraining.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "import multiprocessing\n",
        "import traceback\n",
        "\n",
        "def train_sensor_worker(sensor_id, sensor_data, base_path, window_length, return_dict):\n",
        "    \"\"\"Worker process for a single sensor.\"\"\"\n",
        "    try:\n",
        "        # Re-import inside process (TensorFlow isolation)\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras.models import Model\n",
        "        from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "        from tensorflow.keras.optimizers import Adam\n",
        "        from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        import numpy as np\n",
        "        import pickle\n",
        "        import os\n",
        "        from datetime import datetime\n",
        "\n",
        "        # REBUILD MODEL (identical as your existing one)\n",
        "        def build_lstm_autoencoder(window_length: int, latent_dim: int = 4):\n",
        "            inputs = Input(shape=(window_length, 1))\n",
        "            encoded = LSTM(latent_dim, activation='relu', return_sequences=False)(inputs)\n",
        "            decoded = RepeatVector(window_length)(encoded)\n",
        "            decoded = LSTM(latent_dim, activation='relu', return_sequences=True)(decoded)\n",
        "            outputs = TimeDistributed(Dense(1))(decoded)\n",
        "            model = Model(inputs, outputs)\n",
        "            model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "            return model\n",
        "\n",
        "        print(f\"[Worker {sensor_id}] Starting... Data shape: {sensor_data.shape}\")\n",
        "\n",
        "        # TRAIN/VAL split\n",
        "        n = len(sensor_data)\n",
        "        n_train = int(0.8 * n)\n",
        "        X_train, X_val = sensor_data[:n_train], sensor_data[n_train:]\n",
        "\n",
        "        # Paths\n",
        "        sensor_dir = os.path.join(base_path, \"sensor\", \"model\")\n",
        "        ckpt_dir = os.path.join(sensor_dir, \"checkpoints\")\n",
        "        os.makedirs(sensor_dir, exist_ok=True)\n",
        "        os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "        ckpt_path = os.path.join(ckpt_dir, f\"sensor_{sensor_id}_best.h5\")\n",
        "        model_path = os.path.join(sensor_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        meta_path = os.path.join(sensor_dir, f\"sensor_{sensor_id}_metadata.pkl\")\n",
        "\n",
        "        # Build model\n",
        "        model = build_lstm_autoencoder(window_length)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
        "            EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-6, verbose=0),\n",
        "        ]\n",
        "\n",
        "        print(f\"[Worker {sensor_id}] Training AE...\")\n",
        "        history = model.fit(\n",
        "            X_train, X_train,\n",
        "            validation_data=(X_val, X_val),\n",
        "            epochs=50,\n",
        "            batch_size=64,\n",
        "            verbose=1,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # Compute baseline reconstruction errors\n",
        "        val_pred = model.predict(X_val, verbose=0)\n",
        "        errors = [\n",
        "            mean_squared_error(X_val[i].flatten(), val_pred[i].flatten())\n",
        "            for i in range(len(X_val))\n",
        "        ]\n",
        "\n",
        "        baseline = {\n",
        "            \"mean\": float(np.mean(errors)),\n",
        "            \"std\": float(np.std(errors)),\n",
        "            \"q95\": float(np.percentile(errors, 95)),\n",
        "            \"q99\": float(np.percentile(errors, 99)),\n",
        "            \"median\": float(np.median(errors)),\n",
        "            \"mad\": float(np.median(np.abs(errors - np.median(errors)))),\n",
        "        }\n",
        "\n",
        "        # Save model & metadata\n",
        "        model.save(model_path)\n",
        "        with open(meta_path, \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"sensor_id\": sensor_id,\n",
        "                \"window_length\": window_length,\n",
        "                \"baseline_stats\": baseline,\n",
        "                \"trained_at\": datetime.now(),\n",
        "            }, f)\n",
        "\n",
        "        print(f\"[Worker {sensor_id}] Done ‚úì\")\n",
        "        return_dict[sensor_id] = baseline\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Worker {sensor_id}] ERROR:\", e)\n",
        "        traceback.print_exc()\n",
        "        return_dict[sensor_id] = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    # Your exact paths\n",
        "    data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy'\n",
        "    labelpath = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/window_labels_3class.npy'\n",
        "    train_mask = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/train_mask.npy'\n",
        "\n",
        "    base_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/'\n",
        "\n",
        "    print(\"üöÄ Simple Sensor Pre-Training\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    data = np.load(data_path)           # shape ~ (1,068,551, 100, 12)\n",
        "    label = np.load(labelpath)          # shape ~ (1,068,551,)\n",
        "    train_mask = np.load(train_mask)  # shape ~ (1,068,551,)\n",
        "\n",
        "    # ---- AE WINDOW EXTRACTION (ONLY ONCE) ----\n",
        "    ae_mask = np.logical_and(train_mask, label == 0)\n",
        "    print(\"Total AE windows:\", np.sum(ae_mask))\n",
        "\n",
        "    # Extract final AE training windows\n",
        "    training_data = data[ae_mask]\n",
        "    print(f\"AE data shape: {training_data.shape}\")\n",
        "\n",
        "    batch_size, window_length, num_sensors = training_data.shape\n",
        "    print(f\"Will train {num_sensors} sensors\")\n",
        "\n",
        "    # Train each sensor\n",
        "\n",
        "    print(\"\\n‚ö° Launching parallel sensor training...\\n\")\n",
        "\n",
        "    manager = multiprocessing.Manager()\n",
        "    return_dict = manager.dict()\n",
        "    processes = []\n",
        "\n",
        "    MAX_WORKERS = 4   # Colab usually supports 2‚Äì4 CPU workers reliably\n",
        "\n",
        "    # Loop sensors\n",
        "    for sensor_id in range(num_sensors):\n",
        "\n",
        "        model_path = os.path.join(base_path, \"sensor\", \"model\", f\"sensor_{sensor_id}_model.h5\")\n",
        "        ckpt_path = os.path.join(base_path, \"sensor\", \"model\", \"checkpoints\", f\"sensor_{sensor_id}_best.h5\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(ckpt_path):\n",
        "            print(f\"‚è≠Ô∏è Sensor {sensor_id} already trained. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        sensor_data = training_data[:, :, sensor_id:sensor_id+1]\n",
        "        print(f\"\\nüîß Launching Worker for Sensor {sensor_id} \" f\"({sensor_id+1}/{num_sensors}) ‚Äî remaining: {num_sensors - sensor_id - 1}\")\n",
        "        # Start worker process\n",
        "        p = Process(target=train_sensor_worker,\n",
        "                    args=(sensor_id, sensor_data, base_path, window_length, return_dict))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "        # Limit concurrency\n",
        "        if len(processes) >= MAX_WORKERS:\n",
        "            for p in processes:\n",
        "                p.join()\n",
        "            processes = []\n",
        "\n",
        "    # Join remaining\n",
        "    print(\"‚è≥ Waiting for current batch of workers to finish...\")\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    print(\"‚úÖ Batch completed.\\n\")\n",
        "\n",
        "    print(\"\\nüî• ALL PARALLEL TRAINING DONE\\n\")\n",
        "\n",
        "    print(\"\\nüî• ALL PARALLEL TRAINING DONE\\n\")\n",
        "\n",
        "    results = {}\n",
        "    successful = 0\n",
        "\n",
        "    print(\"üìä Worker Results:\")\n",
        "    for sid in sorted(return_dict.keys()):\n",
        "        res = return_dict[sid]\n",
        "        if res is None:\n",
        "            print(f\"‚ùå Sensor {sid} failed\")\n",
        "            results[sid] = {\"success\": False}\n",
        "        else:\n",
        "            print(f\"‚úÖ Sensor {sid} baseline mean={res['mean']:.6f}\")\n",
        "            results[sid] = {\"success\": True, \"baseline_stats\": res}\n",
        "            successful += 1\n",
        "\n",
        "    print(\"\\nüìä TRAINING SUMMARY\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Trained this run: {successful}/{num_sensors}\")\n",
        "    print(f\"Models saved to: {os.path.join(base_path, 'sensor', 'model')}\")\n",
        "\n",
        "    summary_path = os.path.join(base_path, 'sensor', 'model', 'training_summary.pkl')\n",
        "    with open(summary_path, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"results\": results,\n",
        "            \"training_data_shape\": training_data.shape,\n",
        "            \"successful_sensors\": successful,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }, f)\n",
        "\n",
        "    print(f\"üíæ Summary saved: {summary_path}\")\n",
        "    print(\"‚úÖ Pre-training completed!\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "    main()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb",
      "authorship_tag": "ABX9TyOtlx+POkEIT7ktwhiUqA84",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}