{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/CLAUDE_METROPM_Step1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-BJGPTX7XS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f0c3ec-fb56-4dab-8053-343c96cb3d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting automated feature selection...\n",
            "Input data shape: (3676, 15)\n",
            "Using adaptive max_lag: 24 (data length: 3676)\n",
            "Running Granger causality tests...\n"
          ]
        }
      ],
      "source": [
        "!pip install statsmodels --upgrade\n",
        "!pip install -U lingam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tools.eval_measures import rmse, aic\n",
        "import ast\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from numpy import arange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from importlib.metadata import version\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "#MetroPT dataset: https://archive.ics.uci.edu/dataset/791/metropt+3+dataset\n",
        "\n",
        "\n",
        "df=pd.read_csv(r'/content/drive/MyDrive/PHD/metropt+3+dataset (1).zip (Unzipped Files)/MetroPT3(AirCompressor).csv', parse_dates={'datetime':[1]}, index_col=['datetime'])\n",
        "df.head()\n",
        "#df.drop(['Pressure_switch','Caudal_impulses','Oil_level'], axis=1, inplace=True) #remove columns\n",
        "\n",
        "\n",
        "\n",
        "##########Based on analyses we see following features can be removed for various reasons###################################################\n",
        "#########-------------DV Pressure, Oil level, Caudal impulses, pressure switch, MPG, H1, Oil temp.\n",
        "#############------------------- we also exclude data before April 2020 as some features have no values before this.\n",
        "\n",
        "\n",
        "#df = df[['Global_active_power','Global_reactive_power','Global_intensity']]\n",
        "#Imputing NULL\n",
        "df = df.replace('?', np.nan)\n",
        "df.isnull().sum()\n",
        "\n",
        "\n",
        "\n",
        "def fill_missing(values):\n",
        "    one_day = 24*6\n",
        "    for row in range(df.shape[0]):\n",
        "        for col in range(df.shape[1]):\n",
        "            if np.isnan(values[row][col]):\n",
        "                values[row,col] = values[row-one_day,col]\n",
        "df = df.astype('float32')\n",
        "fill_missing(df.values)\n",
        "df.isnull().sum()\n",
        "\n",
        "\n",
        "# --------------------------- to perform granger causality on the entire time series history------------------------------to reduce computation--------------####\n",
        "daily_df = df.resample('1H').mean().backfill()\n",
        "daily_df.head()\n",
        "\n",
        "\n",
        "\n",
        "#daily_df = daily_df.dropna(how='any')\n",
        "\n",
        "ts_len = daily_df.shape[0]\n",
        "\n",
        "#Now convert index to column\n",
        "daily_df['datetime']=daily_df.index\n",
        "\n",
        "#take data from April 2020, as March is mostly constant for many variables\n",
        "daily_df  = daily_df.loc[(daily_df['datetime'] >= '2020-04-01')]\n",
        "\n",
        "#VISUALISE THE TIMESERIES\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=daily_df['datetime'], y=daily_df['Reservoirs'], name='CO(GT)'))\n",
        "\n",
        "fig.update_layout(showlegend=True, title='Air Quality')\n",
        "fig.show()\n",
        "\n",
        "#remove index column unless required.\n",
        "\n",
        "daily_df.drop(daily_df.columns[0], axis=1, inplace=True) #remove ID column\n",
        "daily_df.drop(daily_df.columns[15], axis=1, inplace=True) #remove datetime\n",
        "#daily_df.drop(daily_df.columns[2], axis=1, inplace=True) #dropped Voltage\n",
        "#Scaling the values\n",
        "\n",
        "whole_series  = daily_df\n",
        "\n",
        "scalers={}\n",
        "for i in daily_df.columns:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    s_s = scaler.fit_transform(whole_series[i].values.reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ i] = scaler\n",
        "    whole_series[i]=s_s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def augmented_dickey_fuller_statistics(time_series):\n",
        "  result = adfuller(time_series.values)\n",
        "  ADF_Statistic = result[0]\n",
        "  p_value = result[1]\n",
        "  Critical_values_1 = result[4][\"1%\"]\n",
        "  Critical_values_5 = result[4][\"5%\"]\n",
        "  Critical_values_10 = result[4][\"10%\"]\n",
        "\n",
        "  # We take that p-value should be less than 0.05 and ADF_statistic should be less than critical value at 5% confidence Critical_value_5\n",
        "  if p_value <0.05 and ADF_Statistic < Critical_values_5:\n",
        "    return \"stationary\"\n",
        "  else:\n",
        "    return \"non-stationary\"\n",
        "\n",
        "for i in range(0,whole_series.shape[1]):\n",
        "  print('Augmented Dickey-Fuller Test Result:', whole_series.iloc[:,i].name)\n",
        "  x= augmented_dickey_fuller_statistics(whole_series.iloc[:,i])\n",
        "  print(x)\n",
        "  #if any of the x is \"non-stationary\": df_difference = whole_series.diff() -- and then deal with df_difference\n",
        "\n",
        "#Granger Causality\n",
        "\n",
        "max_lag_GC = 25\n",
        "\n",
        "def granger_causation_matrix(data, variables, max_lag=25, test='ssr_chi2test', verbose=False):\n",
        "    \"\"\"\n",
        "    Check Granger Causality of all possible combinations of the Time series.\n",
        "    The rows are the response variable, columns are predictors. The values in the table\n",
        "    are the P-Values. P-Values lesser than the significance level (0.05), implies\n",
        "    the Null Hypothesis that the coefficients of the corresponding past values is\n",
        "    zero, that is, the X does not cause Y can be rejected.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            try:\n",
        "                test_result = grangercausalitytests(data[[r,c]], maxlag=max_lag, verbose=False)\n",
        "                p_values = [round(test_result[i+1][0][test][1],4) for i in range(max_lag)]\n",
        "                if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "                min_p_value = np.min(p_values)\n",
        "                df.loc[r, c] = min_p_value\n",
        "            except:\n",
        "                df.loc[r, c] = 1.0  # No causality if test fails\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "# AUTOMATED FEATURE SELECTION based on Granger Causality\n",
        "def auto_feature_selection(data, max_lag='adaptive', significance_level=0.05, min_features=3, removal_strategy='conservative'):\n",
        "    \"\"\"Automatically remove features with weak causal relationships\"\"\"\n",
        "    print(\"Starting automated feature selection...\")\n",
        "    print(f\"Input data shape: {data.shape}\")\n",
        "\n",
        "    # Adaptive max_lag based on data characteristics\n",
        "    if max_lag == 'adaptive':\n",
        "        data_length = len(data)\n",
        "        if data_length > 2000:  # Large dataset\n",
        "            max_lag = min(24, data_length // 100)  # Up to daily cycle\n",
        "        elif data_length > 1000:  # Medium dataset\n",
        "            max_lag = min(12, data_length // 80)   # Half-day cycle\n",
        "        else:  # Small dataset\n",
        "            max_lag = min(6, data_length // 50)    # Quarter-day cycle\n",
        "\n",
        "        print(f\"Using adaptive max_lag: {max_lag} (data length: {data_length})\")\n",
        "\n",
        "    # Quick Granger causality with adaptive max_lag\n",
        "    print(\"Running Granger causality tests...\")\n",
        "    gc_matrix = granger_causation_matrix(data, data.columns, max_lag)\n",
        "\n",
        "    # Debug: Show the causality matrix\n",
        "    print(\"\\nGranger Causality Matrix (p-values < 0.05 are significant):\")\n",
        "    significant_mask = gc_matrix < significance_level\n",
        "    print(f\"Total significant relationships: {significant_mask.sum().sum()}\")\n",
        "    print(f\"Percentage significant: {(significant_mask.sum().sum() / (len(data.columns)**2)) * 100:.1f}%\")\n",
        "\n",
        "    # Count significant relationships for each feature\n",
        "    feature_scores = {}\n",
        "    for col in data.columns:\n",
        "        # Count how many features this one significantly causes\n",
        "        causes_count = (gc_matrix[col + '_x'] < significance_level).sum()\n",
        "        # Count how many features significantly cause this one\n",
        "        caused_by_count = (gc_matrix.loc[col + '_y'] < significance_level).sum()\n",
        "        total_score = causes_count + caused_by_count\n",
        "        feature_scores[col] = total_score\n",
        "        print(f\"{col}: causes {causes_count}, caused by {caused_by_count}, total score: {total_score}\")\n",
        "\n",
        "    # Different removal strategies\n",
        "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if removal_strategy == 'conservative':\n",
        "        # Only remove features with NO significant relationships\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] > 0]\n",
        "    elif removal_strategy == 'moderate':\n",
        "        # Remove features with very few relationships (less than 25% of possible)\n",
        "        threshold = len(data.columns) * 0.5  # At least 50% relationships\n",
        "        keep_features = [f[0] for f in sorted_features if f[1] >= threshold]\n",
        "    elif removal_strategy == 'aggressive':\n",
        "        # Keep only top 75% of features\n",
        "        keep_count = max(min_features, int(len(data.columns) * 0.75))\n",
        "        keep_features = [f[0] for f in sorted_features[:keep_count]]\n",
        "\n",
        "    # Ensure minimum number of features\n",
        "    if len(keep_features) < min_features:\n",
        "        print(f\"Warning: Only {len(keep_features)} features selected, keeping top {min_features}\")\n",
        "        keep_features = [f[0] for f in sorted_features[:min_features]]\n",
        "\n",
        "    removed_features = set(data.columns) - set(keep_features)\n",
        "    print(f\"\\nFeature Selection Results:\")\n",
        "    print(f\"Selected {len(keep_features)} features out of {len(data.columns)}\")\n",
        "    if removed_features:\n",
        "        print(f\"Removed features: {removed_features}\")\n",
        "    else:\n",
        "        print(\"No features removed - all have significant causal relationships!\")\n",
        "\n",
        "    return data[keep_features]\n",
        "\n",
        "# SMART VAR MODEL SELECTION with optional early stopping\n",
        "def smart_var_selection(series, max_lag=None, early_stopping=True, patience=5):\n",
        "    \"\"\"\n",
        "    Smart VAR model selection with configurable early stopping\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    series : np.array\n",
        "        Time series data\n",
        "    max_lag : int\n",
        "        Maximum lag to try (default: sequence length - 1)\n",
        "    early_stopping : bool\n",
        "        Whether to use early stopping (default: True)\n",
        "    patience : int\n",
        "        How many iterations without improvement before stopping (default: 5)\n",
        "    \"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = len(series)//(series.shape[1]*10)\n",
        "\n",
        "    AIC_values = []\n",
        "    best_aic = float('inf')\n",
        "    best_lag = 1\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    print(f\"    Starting VAR selection: will try up to {max_lag} lags\")\n",
        "    if early_stopping:\n",
        "        print(f\"    Early stopping enabled: patience = {patience}\")\n",
        "    else:\n",
        "        print(f\"    Comprehensive search: will try ALL {max_lag} lags\")\n",
        "\n",
        "    try:\n",
        "        # Try variance threshold to avoid numerical issues\n",
        "        selector = VarianceThreshold(0.00002)  # Same as your original\n",
        "        series_filtered = selector.fit_transform(series)\n",
        "\n",
        "        if series_filtered.shape[1] < 2:  # Need at least 2 features\n",
        "            # Try with original data if filtering removes too much\n",
        "            series_filtered = series\n",
        "            print(f\"    Using {series_filtered.shape[1]} features after variance filtering\")\n",
        "\n",
        "        model = VAR(series_filtered)\n",
        "\n",
        "        # TRY LAGS with optional early stopping\n",
        "        for lag in range(max_lag):\n",
        "            try:\n",
        "                results = model.fit(lag)\n",
        "                current_aic = results.aic\n",
        "\n",
        "                # Progress indicator with details\n",
        "                progress_percent = ((lag + 1) / max_lag) * 100\n",
        "                print(f'    Lag {lag:2d}/{max_lag-1} ({progress_percent:5.1f}%) | AIC: {current_aic:8.4f} | BIC: {results.bic:8.4f}', end='')\n",
        "\n",
        "                AIC_values.append(current_aic)\n",
        "\n",
        "                # Track best AIC and early stopping logic\n",
        "                if current_aic < best_aic:\n",
        "                    best_aic = current_aic\n",
        "                    best_lag = lag + 1  # +1 because we return 1-indexed\n",
        "                    no_improvement_count = 0\n",
        "                    print(f' ← NEW BEST!')\n",
        "                else:\n",
        "                    no_improvement_count += 1\n",
        "                    if early_stopping:\n",
        "                        print(f' (no improvement: {no_improvement_count}/{patience})')\n",
        "                    else:\n",
        "                        print()\n",
        "\n",
        "                # Early stopping check\n",
        "                if early_stopping and no_improvement_count >= patience:\n",
        "                    print(f'    *** EARLY STOP: No improvement for {patience} iterations ***')\n",
        "                    print(f'    *** BEST RESULT: Lag {best_lag} with AIC {best_aic:.4f} ***')\n",
        "                    print(f'    *** SAVED TIME: Skipped {max_lag - lag - 1} remaining lags ***')\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                AIC_values.append(99999)  # Same as your original\n",
        "                no_improvement_count += 1\n",
        "                print(f'    Lag {lag:2d}/{max_lag-1} | FAILED: {str(e)[:50]}...')\n",
        "\n",
        "        # Find minimum AIC index from what we tried\n",
        "        if AIC_values:\n",
        "            minAIC_index = AIC_values.index(min(AIC_values)) + 1\n",
        "            trials_completed = len(AIC_values)\n",
        "            print(f'    ========================================')\n",
        "            print(f'    FINAL RESULT: Optimal lag = {minAIC_index}')\n",
        "            print(f'    FINAL AIC: {min(AIC_values):.4f}')\n",
        "            print(f'    TRIALS: {trials_completed}/{max_lag} ({(trials_completed/max_lag)*100:.1f}%)')\n",
        "            print(f'    ========================================')\n",
        "            return minAIC_index\n",
        "        else:\n",
        "            print(f'    *** ERROR: No valid AIC values found ***')\n",
        "            return 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'    *** CRITICAL ERROR: {e} ***')\n",
        "        return 99999  # Same as your original\n",
        "\n",
        "\n",
        "def create_failure_labels_metropt3(df):\n",
        "    \"\"\"\n",
        "    Create failure labels based on MetroPT-3 documented failure periods\n",
        "\n",
        "    Documented failures in 2020:\n",
        "    1. 4/18/2020 0:00 - 4/18/2020 23:59: Air leak (High stress)\n",
        "    2. 5/29/2020 23:30 - 5/30/2020 6:00: Air leak (High stress)\n",
        "    3. 6/5/2020 10:00 - 6/7/2020 14:30: Air leak (High stress)\n",
        "    4. 7/15/2020 14:30 - 7/15/2020 19:00: Air leak (High stress)\n",
        "    \"\"\"\n",
        "    print(\"Creating failure labels for MetroPT-3 dataset...\")\n",
        "\n",
        "    # Initialize all labels as normal (0)\n",
        "    labels = np.zeros(len(df))\n",
        "\n",
        "    # Define the 4 documented failure periods\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    total_failure_points = 0\n",
        "\n",
        "    for start_time, end_time, failure_type in failure_periods:\n",
        "        try:\n",
        "            # Find indices within the failure period\n",
        "            failure_mask = (df.index >= start_time) & (df.index <= end_time)\n",
        "            failure_indices = np.where(failure_mask)[0]\n",
        "\n",
        "            if len(failure_indices) > 0:\n",
        "                labels[failure_indices] = 1\n",
        "                total_failure_points += len(failure_indices)\n",
        "                print(f\"Labeled {failure_type}: {start_time} to {end_time} ({len(failure_indices)} points)\")\n",
        "            else:\n",
        "                print(f\"Warning: No data found for {failure_type} period\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error labeling {failure_type}: {e}\")\n",
        "\n",
        "    failure_count = np.sum(labels)\n",
        "    normal_count = len(labels) - failure_count\n",
        "\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(f\"  Normal periods: {normal_count:,} ({normal_count/len(labels)*100:.2f}%)\")\n",
        "    print(f\"  Failure periods: {failure_count:,} ({failure_count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    if failure_count == 0:\n",
        "        print(\"WARNING: No failure periods found! Check date ranges in your data.\")\n",
        "\n",
        "    return labels\n",
        "\n",
        "# =======================================================================================\n",
        "# MODIFICATION TO YOUR EXISTING CODE - REPLACE THIS SECTION:\n",
        "# =======================================================================================\n",
        "\n",
        "# BEFORE (around line 125 in your code):\n",
        "# labels = np.arange(len(whole_series_auto))  # Simple sequential labels - replace with your actual labels\n",
        "\n",
        "# AFTER - Replace with:\n",
        "print(\"Creating MetroPT-3 failure labels...\")\n",
        "labels = create_failure_labels_metropt3(whole_series_auto)\n",
        "# AUTO-FILTER window sizes based on distribution\n",
        "def auto_filter_windows(window_data, sequences, labels=None, percentile_range=(10, 90)):\n",
        "    \"\"\"Automatically filter window sizes based on percentile range\"\"\"\n",
        "    lower_bound = np.percentile(window_data, percentile_range[0])\n",
        "    upper_bound = np.percentile(window_data, percentile_range[1])\n",
        "\n",
        "    valid_indices = np.where((window_data >= lower_bound) & (window_data <= upper_bound))[0]\n",
        "\n",
        "    print(f\"Auto-filtering: keeping windows between {lower_bound:.1f} and {upper_bound:.1f}\")\n",
        "    print(f\"Filtered from {len(window_data)} to {len(valid_indices)} sequences\")\n",
        "\n",
        "    filtered_windows = window_data[valid_indices]\n",
        "    filtered_sequences = sequences[valid_indices]\n",
        "\n",
        "    if labels is not None:\n",
        "        filtered_labels = labels[valid_indices]\n",
        "        return filtered_windows, filtered_sequences, filtered_labels\n",
        "\n",
        "    return filtered_windows, filtered_sequences\n",
        "\n",
        "# YOUR EXISTING FUNCTIONS (modified for optional labels)\n",
        "def extract_windows_vectorized(array, large_seq_size, labels=None):\n",
        "    start = 0\n",
        "    last_index = len(array)-1\n",
        "    max_time =  last_index - large_seq_size +1\n",
        "\n",
        "    sub_windows = (\n",
        "        start +\n",
        "        np.expand_dims(np.arange(large_seq_size), 0) +\n",
        "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
        "    ).astype(int)\n",
        "\n",
        "    windows = array[sub_windows]\n",
        "\n",
        "    if labels is not None:\n",
        "        label_indices = sub_windows[:, -1]\n",
        "        window_labels = labels[label_indices]\n",
        "        return windows, window_labels\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def create_failure_labels_metropt3(df):\n",
        "    \"\"\"\n",
        "    Create failure labels based on MetroPT-3 documented failure periods\n",
        "\n",
        "    Documented failures in 2020:\n",
        "    1. 4/18/2020 0:00 - 4/18/2020 23:59: Air leak (High stress)\n",
        "    2. 5/29/2020 23:30 - 5/30/2020 6:00: Air leak (High stress)\n",
        "    3. 6/5/2020 10:00 - 6/7/2020 14:30: Air leak (High stress)\n",
        "    4. 7/15/2020 14:30 - 7/15/2020 19:00: Air leak (High stress)\n",
        "    \"\"\"\n",
        "    print(\"Creating failure labels for MetroPT-3 dataset...\")\n",
        "\n",
        "    # Initialize all labels as normal (0)\n",
        "    labels = np.zeros(len(df))\n",
        "\n",
        "    # Define the 4 documented failure periods\n",
        "    failure_periods = [\n",
        "        ('2020-04-18 00:00:00', '2020-04-18 23:59:59', 'Air_leak_1'),\n",
        "        ('2020-05-29 23:30:00', '2020-05-30 06:00:00', 'Air_leak_2'),\n",
        "        ('2020-06-05 10:00:00', '2020-06-07 14:30:00', 'Air_leak_3'),\n",
        "        ('2020-07-15 14:30:00', '2020-07-15 19:00:00', 'Air_leak_4')\n",
        "    ]\n",
        "\n",
        "    total_failure_points = 0\n",
        "\n",
        "    for start_time, end_time, failure_type in failure_periods:\n",
        "        try:\n",
        "            # Find indices within the failure period\n",
        "            failure_mask = (df.index >= start_time) & (df.index <= end_time)\n",
        "            failure_indices = np.where(failure_mask)[0]\n",
        "\n",
        "            if len(failure_indices) > 0:\n",
        "                labels[failure_indices] = 1\n",
        "                total_failure_points += len(failure_indices)\n",
        "                print(f\"Labeled {failure_type}: {start_time} to {end_time} ({len(failure_indices)} points)\")\n",
        "            else:\n",
        "                print(f\"Warning: No data found for {failure_type} period\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error labeling {failure_type}: {e}\")\n",
        "\n",
        "    failure_count = np.sum(labels)\n",
        "    normal_count = len(labels) - failure_count\n",
        "\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(f\"  Normal periods: {normal_count:,} ({normal_count/len(labels)*100:.2f}%)\")\n",
        "    print(f\"  Failure periods: {failure_count:,} ({failure_count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    if failure_count == 0:\n",
        "        print(\"WARNING: No failure periods found! Check date ranges in your data.\")\n",
        "\n",
        "    return labels\n",
        "\n",
        "# =======================================================================================\n",
        "# MODIFICATION TO YOUR EXISTING CODE - REPLACE THIS SECTION:\n",
        "# =======================================================================================\n",
        "\n",
        "# BEFORE (around line 125 in your code):\n",
        "# labels = np.arange(len(whole_series_auto))  # Simple sequential labels - replace with your actual labels\n",
        "\n",
        "# AFTER - Replace with:\n",
        "print(\"Creating MetroPT-3 failure labels...\")\n",
        "labels = create_failure_labels_metropt3(whole_series_auto)\n",
        "# AUTOMATED PIPELINE\n",
        "print(\"=\"*50)\n",
        "print(\"STARTING AUTOMATED PIPELINE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 1: Automated feature selection\n",
        "whole_series_auto = auto_feature_selection(data = whole_series, removal_strategy='moderate')\n",
        "\n",
        "# Step 2: Create labels (modify this for your use case)\n",
        "#labels = np.arange(len(whole_series_auto))  # Simple sequential labels - replace with your actual labels\n",
        "print(\"Creating MetroPT-3 failure labels...\")\n",
        "labels = create_failure_labels_metropt3(whole_series_auto)\n",
        "\n",
        "# The rest of your code stays exactly the same!\n",
        "# Your extract_windows_vectorized function will now use\n",
        "# Step 3: Extract windows\n",
        "K = 50\n",
        "large_seq_size = K\n",
        "n_features = whole_series_auto.shape[1]\n",
        "\n",
        "print(f\"Creating {K}-length sequences from {len(whole_series_auto)} timesteps with {n_features} features...\")\n",
        "\n",
        "Long, Long_labels = extract_windows_vectorized(whole_series_auto.values, large_seq_size, labels)\n",
        "Long_train = Long\n",
        "Long_train_labels = Long_labels\n",
        "\n",
        "print(f\"Created {Long_train.shape[0]} long sequences\")\n",
        "\n",
        "# Step 4: AUTOMATED VAR model selection (much faster)\n",
        "print(\"Starting automated VAR model selection...\")\n",
        "best_window_for_long_seq = []\n",
        "\n",
        "# Parallel processing option (uncomment if you have joblib)\n",
        "# from joblib import Parallel, delayed\n",
        "# best_windows = Parallel(n_jobs=-1)(delayed(fast_var_selection)(Long_train[i,:,:]) for i in range(Long_train.shape[0]))\n",
        "\n",
        "# Sequential processing (faster than your original)\n",
        "for i in range(Long_train.shape[0]):\n",
        "    if i % 100 == 0:  # Progress indicator\n",
        "        print(f\"Processed {i}/{Long_train.shape[0]} sequences...\")\n",
        "\n",
        "    cur_seq = Long_train[i,:,:]\n",
        "    optimal_lag = smart_var_selection(cur_seq, max_lag=min(25, K//2))\n",
        "    best_window_for_long_seq.append(optimal_lag)\n",
        "\n",
        "print(f\"VAR selection completed. Window distribution:\")\n",
        "Window = np.array(best_window_for_long_seq)\n",
        "print(f\"Mean: {Window.mean():.2f}, Std: {Window.std():.2f}, Min: {Window.min()}, Max: {Window.max()}\")\n",
        "\n",
        "# Step 5: AUTO-FILTER sequences\n",
        "print(\"Auto-filtering sequences...\")\n",
        "wdata_filtered, tdata_filtered, ldata_filtered = auto_filter_windows(\n",
        "    Window, Long_train, Long_train_labels, percentile_range=(10, 90)\n",
        ")\n",
        "\n",
        "# Step 6: Save results\n",
        "output_dir = r'/content/drive/MyDrive/PHD/2024/TEMP_OUTPUT_METROPM/'\n",
        "\n",
        "np.save(f'{output_dir}multivariate_long_sequences-TRAIN-AUTO.npy', tdata_filtered)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_WINDOW-AUTO.npy', wdata_filtered)\n",
        "np.save(f'{output_dir}multivariate_long_sequences_LABELS-AUTO.npy', ldata_filtered)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"AUTOMATED PIPELINE COMPLETED!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Final dataset: {tdata_filtered.shape[0]} sequences, {tdata_filtered.shape[1]} timesteps, {tdata_filtered.shape[2]} features\")\n",
        "print(f\"Window range: {wdata_filtered.min()} to {wdata_filtered.max()}\")\n",
        "print(f\"Files saved with -AUTO suffix\")\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original Window Distribution\", fontsize=15)\n",
        "sns.histplot(Window, kde=True, color=\"blue\", alpha=0.7)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Filtered Window Distribution\", fontsize=15)\n",
        "sns.histplot(wdata_filtered, kde=True, color=\"red\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Pipeline ready for new datasets - no manual intervention needed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-RMQkgLak9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xp-Mv3Ksamxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zEUESMBM_Eo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6ZgIvrdRNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/CLAUDE_METROPM_Step1.ipynb",
      "authorship_tag": "ABX9TyNGSzAAmGFhr4zET6b7a4cv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}