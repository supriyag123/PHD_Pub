{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/DGRNet-Step2-hourly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bx-5b_puABG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "8dfa6505-9cf6-48b1-a5af-b4247cd6a11e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dded00cf5a02>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Example Reconstruction of Testing Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature 6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "import plotly.express as px # for data visualization\n",
        "#from keras.utils import plot_model\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#window1 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-500.npy')\n",
        "#window2 = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences_WINDOW-1000.npy')\n",
        "#window = np.concatenate((window1, window2), axis=0)\n",
        "\n",
        "#test_data = np.load(r'/content/drive/MyDrive/PHD/2021/multivariate_long_sequences-TEST.npy')\n",
        "\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN_hourly.npy') #------for Hourly data\n",
        "index = 500\n",
        "#We missed i=500 from processing the iosw. So here we are dropping row with index =500\n",
        "train_data= np.delete(train_data, index, axis=0)\n",
        "#get data\n",
        "#train_data = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences-TRAIN-Daily-May2024.npy')  ------for Daily data\n",
        "\n",
        "#window_label = np.load(r'/content/drive/MyDrive/PHD/2024/multivariate_long_sequences_WINDOW-Daily-May2024.npy')\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "\n",
        "maxval = train_data.shape[0]\n",
        "count_train = int(math.ceil(0.8*maxval))\n",
        "x_train = train_data[:count_train]\n",
        "x_test = train_data[count_train:]\n",
        "#x_test = train_data[count_train:,:,:]\n",
        "\n",
        "\n",
        "#create sampling layer\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "#Build the encoder\n",
        "latent_dim = 10\n",
        "intermediate_dim = 256\n",
        "\n",
        "\n",
        "#Encoder\n",
        "encoder_inputs =  layers.Input(shape=(window_size, n_features),name=\"encoder_input\")\n",
        "x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\" )(xx)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "\n",
        "#Dcoder\n",
        "\n",
        "inp_z = Input(shape=(latent_dim,),name=\"decoder\")\n",
        "x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "x2= layers.Dense(int(intermediate_dim/2),  name=\"Dense2\")(x1)\n",
        "x22= layers.LSTM(int(intermediate_dim/2),activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "x3 = layers.LSTM(intermediate_dim,activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "#decode_out = layers.LSTM(n_features,name='decodeout', return_sequences=True)(x2) #Alternative\n",
        "decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "\n",
        "#Parameters\n",
        "n_epochs = 100 # total number of epochs\n",
        "klstart = 3 # The number of epochs at which KL loss should be included\n",
        "kl_annealtime = n_epochs-klstart\n",
        "# the starting value of weight is 0\n",
        "# define it as a keras backend variable\n",
        "weight = K.variable(0.0)\n",
        "\n",
        "#Define the VAE as a Model with a custom train_step\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            K.print_tensor(weight)\n",
        "            total_loss = reconstruction_loss + (weight*kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mean_squared_error(data, reconstruction), axis=-1),keepdims=True\n",
        "                )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                  }\n",
        "\n",
        "\n",
        "\n",
        "# CALLBACKS\n",
        "es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "class AnnealingCallback(Callback):\n",
        "    def __init__(self, weight):\n",
        "        self.weight = weight\n",
        "    def on_epoch_end (self, epoch, logs={}):\n",
        "        if epoch > klstart :\n",
        "            new_weight = min(K.get_value(self.weight) + (1./ kl_annealtime), 1.)\n",
        "            K.set_value(self.weight, new_weight)\n",
        "        print (\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "#Train the VAE\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "\n",
        "\n",
        "vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "history=vae.fit( x_train,\n",
        "                 epochs=n_epochs,\n",
        "                 batch_size=50,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[es,AnnealingCallback(weight),reduce_lr])\n",
        "\n",
        "\n",
        "encoder.save(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-hourly-latent10-dim256-latest.model')\n",
        "decoder.save(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-hourly-latent10-dim256-latest.model')\n",
        "\n",
        "encoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-encoder-saved-hourly-latent10-dim256-latest.model')\n",
        "decoder = keras.models.load_model(r'/content/drive/MyDrive/PHD/2024/VAE_SIMULATION/vae-decoder-saved-hourly-latent10-dim256-latest.model')\n",
        "\n",
        "e =\n",
        "#vae2.evaluate(test_data)\n",
        "\n",
        "#All loss together\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['reconstruction_loss'], label='reconstruction_loss')\n",
        "plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.legend(['kl_loss'], loc='upper left')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n",
        "\n",
        "#Just Loss\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.legend(['kl_loss'], loc='upper left')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.ylim(0, 100)\n",
        "plt.show()\n",
        "\n",
        "#PLOT TRAIN RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_train)\n",
        "X_test_predict = decoder(X_test_encoded[2][0])\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize ='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "\n",
        "plt.plot(x_train[0,:,0],\"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[0,:,0],\"b\", label=\"reconstructed\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#PLOT TEST RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_test[:,:,:])\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize ='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_test[:,:,5],\"r\")\n",
        "plt.plot(X_test_predict[:,:,5],\"b\")\n",
        "plt.show()\n",
        "\n",
        "fig = px.scatter(None, x=X_test_encoded[2][:,0], y=X_test_encoded[2][:,1],opacity=1, color=window_label.astype(str))\n",
        "fig.update_layout(dict(plot_bgcolor = 'white'))\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()\n",
        "\n",
        "#--------------------------------------Trysimple encoder--------------------------------------------------------------------------------------\n",
        "\n",
        "#CONSTRUCT AEC\n",
        "encoder_inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
        "LSTM1= LSTM(512,activation='tanh', return_sequences = True)(encoder_inputs)\n",
        "LSTM2 =LSTM(256, activation='tanh', return_sequences=False)(LSTM1)\n",
        "decode_bridge = RepeatVector(x_train.shape[1])(LSTM2)\n",
        "LSTM2_decoded = LSTM(256,activation='tanh', return_sequences=True)(decode_bridge)\n",
        "LSTM1_decoded = LSTM(512, activation='tanh', return_sequences=True)(LSTM2_decoded)\n",
        "decoded = TimeDistributed(Dense(x_train.shape[2]))(LSTM1_decoded)\n",
        "model = Model(encoder_inputs, decoded)\n",
        "\n",
        "model.summary()\n",
        "#plot_model(model, to_file=u'/content/drive/MyDrive/PHD/PHD/PMHWORK/TRANSFORMED/model2021.png')\n",
        "\n",
        "#Compile\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
        "#TRAIN\n",
        "\n",
        "history=model.fit(x_train,x_train,epochs=100,\n",
        "                  validation_split=0.2,\n",
        "                  batch_size=50,verbose=1,callbacks=[reduce_lr])\n",
        "\n",
        "#history = model.fit(X_train, X_train, epochs=500, batch_size=32, validation_split=0.1,\n",
        "#                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')], shuffle=False)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, x_test, verbose=0)\n",
        "print(scores)\n",
        "\n",
        "\n",
        "#Determine Anomaly - train MAE and decide Threshold\n",
        "x_train_pred = model.predict(x_train, verbose=0)\n",
        "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
        "\n",
        "\n",
        "#PLOT TRAIN RECONSTRUCTION\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize ='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_train[:,:,0],\"r\", label=\"Actual\")\n",
        "plt.plot(x_train_pred[:,:,0],\"b\", label=\"reconstructed\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(train_mae_loss, bins=500)\n",
        "plt.xlabel('Train MAE loss')\n",
        "plt.ylabel('Number of Samples');\n",
        "#PLOT TRAIN RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(train_data[0:100,:,:])\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize ='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "\n",
        "plt.plot(train_data[90,:,0],\"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[90,:,0],\"b\", label=\"reconstructed\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#PLOT TEST RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(test_data[0:100,:,:])\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize ='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(test_data[50,:,5],\"r\")\n",
        "plt.plot(X_test_predict[50,:,5],\"b\")\n",
        "plt.show()\n",
        "\n",
        "fig = px.scatter(None, x=X_test_encoded[2][:,0], y=X_test_encoded[2][:,1],opacity=1, color=window_label.astype(str))\n",
        "fig.update_layout(dict(plot_bgcolor = 'white'))\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()\n",
        "\n",
        "# serialize autoencoder model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "   #vae2.evaluate(test_data)\n",
        "\n",
        "# serialize encoder model to JSON\n",
        "#model_json_encoder = encoder.to_json()\n",
        "#with open(\"model_encoder.json\", \"w\") as json_file:\n",
        "#    json_file.write(model_json_encoder)\n",
        "# serialize encoder weights to HDF5\n",
        "#encoder.save_weights(\"model_encoder.h5\")\n",
        "\n",
        "# serialize whole model to JSON\n",
        "#vae.save(r'/content/drive/MyDrive/PHD/2021/vae-saved.model')\n",
        "#modelVAEjson = vae.to_json()\n",
        "#with open(\"modelVAE.json\", \"w\") as json_file:\n",
        "# json_file.write(modelVAE_json)\n",
        "# serialize weights to HDF5\n",
        "#vae.save_weights(\"modelVAE.h5\")\n",
        "#print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qqghsw2ZXv7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/DGRNet-Step2-hourly.ipynb",
      "authorship_tag": "ABX9TyPQNrQeFzGLnEkE21dFjlC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}