{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0484604-3170-44d3-c422-55b575f7d64d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded subsequences: (3627, 50, 12)\n",
            "üöÄ Creating robust system with 12 sensors\n",
            "‚úÖ AE model loaded for sensor 0\n",
            "‚úÖ AE model loaded for sensor 1\n",
            "‚úÖ AE model loaded for sensor 2\n",
            "‚úÖ AE model loaded for sensor 3\n",
            "‚úÖ AE model loaded for sensor 4\n",
            "‚úÖ AE model loaded for sensor 5\n",
            "‚úÖ AE model loaded for sensor 6\n",
            "‚úÖ AE model loaded for sensor 7\n",
            "‚úÖ AE model loaded for sensor 8\n",
            "‚úÖ AE model loaded for sensor 9\n",
            "‚úÖ AE model loaded for sensor 10\n",
            "‚úÖ AE model loaded for sensor 11\n",
            "‚úÖ Created system: 12/12 models loaded\n",
            "‚ñ∂Ô∏è Running streaming test on 100 hold-out subsequences\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 50,\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1):\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {'mean': 0.0, 'std': 1.0, 'q95': 0.0, 'q99': 0.0}\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.drift_cooldown = 0\n",
        "\n",
        "        self.anomaly_cooldown_steps = 5    # you can tune\n",
        "        self.drift_cooldown_steps = 10     # you can tune\n",
        "\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.consecutive_anomaly_votes = 0\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Correct metadata file\n",
        "                metadata_path = model_path.replace('_model.h5', '_metadata.pkl')\n",
        "\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "\n",
        "                    baseline = metadata.get('baseline_stats', None)\n",
        "\n",
        "                    if baseline is not None:\n",
        "                        # Initialize rolling stats from training\n",
        "                        self.rolling_stats['mean'] = baseline['mean']\n",
        "                        self.rolling_stats['std']  = baseline['std']\n",
        "                        self.rolling_stats['q95']  = baseline['q95']\n",
        "                        self.rolling_stats['q99']  = baseline['q99']\n",
        "\n",
        "                        # Save baseline distribution for drift detection\n",
        "                        self.baseline_errors = np.array(baseline['baseline_errors'])\n",
        "\n",
        "                # AE was trained on raw, NOT scaled\n",
        "                self.scaler = None\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format ‚Äì expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"‚úÖ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3. Update rolling stats periodically\n",
        "        if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "            self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model on RAW values.\"\"\"\n",
        "        try:\n",
        "            # Ensure shape: [1, window_length, 1]\n",
        "            X = subsequence.reshape(1, self.window_length, 1)\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "\n",
        "            error = mean_squared_error(\n",
        "                subsequence.flatten(),\n",
        "                reconstruction.flatten()\n",
        "            )\n",
        "            return max(0.0, error)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            # Fallback: variance of raw subsequence\n",
        "            return float(np.var(subsequence))\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "        self.rolling_stats['mean'] = np.mean(errors_array)\n",
        "        self.rolling_stats['std'] = np.std(errors_array) + 1e-8\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        is_anomaly_now = score > threshold\n",
        "\n",
        "        # Cooldown active ‚Üí suppress anomaly\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            return False\n",
        "\n",
        "        # No cooldown and anomaly happened ‚Üí activate cooldown\n",
        "        if is_anomaly_now:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            is_drift_now = js_divergence > self.drift_threshold\n",
        "\n",
        "            # Cooldown active ‚Üí suppress\n",
        "            if self.drift_cooldown > 0:\n",
        "                self.drift_cooldown -= 1\n",
        "                return False\n",
        "\n",
        "            # Multi-step confirmation: require 3 drift votes in last few steps\n",
        "            if is_drift_now:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "            if self.consecutive_drift_votes >= 3:\n",
        "                self.drift_cooldown = self.drift_cooldown_steps\n",
        "                self.consecutive_drift_votes = 0\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        if self.rolling_stats['std'] == 0: return 0.5\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        distance_from_threshold = abs(score - threshold) / self.rolling_stats['std']\n",
        "        return min(1.0, distance_from_threshold / 3.0)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"üöÄ Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=50,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1)\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"‚úÖ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "\n",
        "    return sensor_agents, master\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_sensor_drift_trends(sensor_logs, metadata_list, window_size=20):\n",
        "    \"\"\"\n",
        "    Plot reconstruction error trend lines for each sensor.\n",
        "\n",
        "    Args:\n",
        "        sensor_logs: list of length T, where each element is a dict:\n",
        "            {\n",
        "                'sensor_id': int,\n",
        "                'scores': [score1, score2, ... for each timestep],\n",
        "                'anomalies': [0/1 flags],\n",
        "                'drift': [0/1 flags]\n",
        "            }\n",
        "        metadata_list: list of sensor metadata:\n",
        "            baseline_stats['mean'], ['q95'], ['q99']\n",
        "    \"\"\"\n",
        "\n",
        "    num_sensors = len(sensor_logs)\n",
        "    cols = 3\n",
        "    rows = int(np.ceil(num_sensors / cols))\n",
        "\n",
        "    plt.figure(figsize=(18, 5 * rows))\n",
        "\n",
        "    for sid, log in enumerate(sensor_logs):\n",
        "        scores = np.array(log['scores'])\n",
        "        anomalies = np.array(log['anomalies'])\n",
        "        drift = np.array(log['drift'])\n",
        "\n",
        "        # Baseline thresholds\n",
        "        baseline = metadata_list[sid]['mean']\n",
        "        q95 = metadata_list[sid]['q95']\n",
        "        q99 = metadata_list[sid]['q99']\n",
        "\n",
        "        # Smooth trend line: EMA\n",
        "        ema = pd.Series(scores).ewm(span=window_size, adjust=False).mean()\n",
        "\n",
        "        ax = plt.subplot(rows, cols, sid + 1)\n",
        "\n",
        "        # Raw reconstruction error\n",
        "        ax.plot(scores, color='blue', alpha=0.4, label=\"Reconstruction Error\")\n",
        "\n",
        "        # EMA trend line\n",
        "        ax.plot(ema, color='black', linewidth=2, label=\"EMA Trend\")\n",
        "\n",
        "        # Baseline bands\n",
        "        ax.axhline(baseline, color='green', linestyle='--', label=\"Baseline Mean\")\n",
        "        ax.axhline(q95, color='orange', linestyle='--', label=\"95th Percentile\")\n",
        "        ax.axhline(q99, color='red', linestyle='--', label=\"99th Percentile\")\n",
        "\n",
        "        # Mark anomalies\n",
        "        anomaly_points = np.where(anomalies == 1)[0]\n",
        "        ax.scatter(anomaly_points, scores[anomaly_points], color='red',\n",
        "                   s=40, marker='x', label=\"Anomaly\" if sid == 0 else \"\")\n",
        "\n",
        "        # Mark drifts\n",
        "        drift_points = np.where(drift == 1)[0]\n",
        "        ax.scatter(drift_points, scores[drift_points], color='purple',\n",
        "                   s=60, marker='D', label=\"Drift\" if sid == 0 else \"\")\n",
        "\n",
        "        ax.set_title(f\"Sensor {sid} Drift Trend\")\n",
        "        ax.set_xlabel(\"Streaming Step\")\n",
        "        ax.set_ylabel(\"Reconstruction Error\")\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "        if sid == 0:\n",
        "            ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# DEMO ENTRYPOINT\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "if __name__ == \"__main__\":\n",
        "    models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "\n",
        "    # Load precomputed subsequences\n",
        "    data_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy\"\n",
        "    subsequences = np.load(data_path)   # shape: [num_samples, window_length, num_sensors]\n",
        "\n",
        "    print(f\"‚úÖ Loaded subsequences: {subsequences.shape}\")\n",
        "    num_samples, window_length, num_sensors = subsequences.shape\n",
        "\n",
        "    # Take last 1000 samples as hold-out\n",
        "    holdout = subsequences[-1000:]\n",
        "\n",
        "    # Create agents + master\n",
        "    sensor_agents, master = create_robust_system(num_sensors=num_sensors, models_dir=models_dir)\n",
        "\n",
        "    # --- Initialize logging for each sensor ---\n",
        "    sensor_logs = []\n",
        "    for sid in range(num_sensors):\n",
        "        sensor_logs.append({\n",
        "            'sensor_id': sid,\n",
        "            'scores': [],\n",
        "            'anomalies': [],\n",
        "            'drift': []\n",
        "        })\n",
        "\n",
        "    # Load metadata for drift/baseline bands\n",
        "    metadata_list = []\n",
        "    for sid in range(num_sensors):\n",
        "        meta_path = os.path.join(models_dir, f\"sensor_{sid}_metadata.pkl\")\n",
        "        if os.path.exists(meta_path):\n",
        "            with open(meta_path, 'rb') as f:\n",
        "                meta = pickle.load(f)\n",
        "                metadata_list.append(meta['baseline_stats'])\n",
        "        else:\n",
        "            metadata_list.append({'mean':0,'q95':0,'q99':0})\n",
        "\n",
        "    # Streaming test: pick 10‚Äì100 samples\n",
        "    test_samples = 100  # you can set 10, 20, 100, etc.\n",
        "\n",
        "    print(f\"‚ñ∂Ô∏è Running streaming test on {test_samples} hold-out subsequences\")\n",
        "\n",
        "    for i in range(test_samples):\n",
        "        live_seq = holdout[i]   # shape [window_length, num_sensors]\n",
        "\n",
        "        result = master.process_system_input(live_seq)\n",
        "\n",
        "        # Print system-level decision\n",
        "        print(f\"\\nStep {i+1}/{test_samples}\")\n",
        "        print(\"System decision:\", result[\"system_decisions\"])\n",
        "\n",
        "        # Print sensor-level results\n",
        "        for r in result[\"sensor_results\"]:\n",
        "            print(f\"  Sensor {r['sensor_id']}: \"\n",
        "                  f\"anomaly={r['is_anomaly']} \"\n",
        "                  f\"drift={r['drift_flag']} \"\n",
        "                  f\"score={r['anomaly_score']:.4f} \"\n",
        "                  f\"conf={r['confidence']:.2f}\")\n",
        "            sid = r['sensor_id']\n",
        "            sensor_logs[sid]['scores'].append(r['anomaly_score'])\n",
        "            sensor_logs[sid]['anomalies'].append(1 if r['is_anomaly'] else 0)\n",
        "            sensor_logs[sid]['drift'].append(1 if r['drift_flag'] else 0)\n",
        "\n",
        "\n",
        "    print(\"\\nüìà Generating Sensor Drift Trend Plots...\")\n",
        "    plot_sensor_drift_trends(sensor_logs, metadata_list, window_size=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Agent.ipynb",
      "authorship_tag": "ABX9TyPq79awyYl21jjSJ/7B7X2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}