{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HoP7OuWNxlsJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Sensor Model Pre-Training System - LSTM Autoencoder Only\n",
        "========================================================\n",
        "\n",
        "This script handles the initial training of sensor models using your pre-processed dataset.\n",
        "Uses LSTM Autoencoder for anomaly detection. Models are trained once and saved to disk\n",
        "for use in the production agent system.\n",
        "\n",
        "Dataset format: [num_samples, window_length, num_sensors] - already scaled and sequenced.\n",
        "\n",
        "Usage:\n",
        "    python sensor_pretraining.py --data_path /path/to/dataset.npy --models_dir ./trained_models\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è TensorFlow not available. Install with: pip install tensorflow\")\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "class SensorModelTrainer:\n",
        "    \"\"\"\n",
        "    Handles training and saving of LSTM Autoencoder models for individual sensors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 window_length: int = 50,\n",
        "                 latent_dim: int = 32,\n",
        "                 epochs: int = 100,\n",
        "                 batch_size: int = 32):\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise ImportError(\"TensorFlow required for model training\")\n",
        "\n",
        "    def build_lstm_autoencoder(self) -> Model:\n",
        "        \"\"\"Build LSTM Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.window_length, 1), name='encoder_input')\n",
        "        encoded = LSTM(self.latent_dim, activation='relu', return_sequences=False, name='encoder_lstm')(inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = RepeatVector(self.window_length, name='repeat_vector')(encoded)\n",
        "        decoded = LSTM(self.latent_dim, activation='relu', return_sequences=True, name='decoder_lstm')(decoded)\n",
        "        outputs = TimeDistributed(Dense(1, activation='linear'), name='decoder_output')(decoded)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs, outputs, name='sensor_lstm_autoencoder')\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_sensor_model(self, sensor_data: np.ndarray, sensor_id: int) -> Tuple[Model, Dict]:\n",
        "        \"\"\"\n",
        "        Train LSTM Autoencoder for a single sensor using pre-prepared sequences.\n",
        "\n",
        "        Args:\n",
        "            sensor_data: Pre-scaled sequences [num_samples, window_length] for this sensor\n",
        "            sensor_id: Sensor identifier\n",
        "\n",
        "        Returns:\n",
        "            Trained model and training history\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Training LSTM Autoencoder for sensor {sensor_id}...\")\n",
        "        print(f\"  Data shape: {sensor_data.shape}\")\n",
        "        print(f\"  Total sequences: {len(sensor_data)}\")\n",
        "\n",
        "        if len(sensor_data) < 100:\n",
        "            raise ValueError(f\"Insufficient sequences ({len(sensor_data)}) for training. Need at least 100.\")\n",
        "\n",
        "        # Data is already in correct format for LSTM: [num_samples, window_length, features]\n",
        "        # If features=1 (univariate), we might need to add the feature dimension\n",
        "        if sensor_data.ndim == 2:\n",
        "            # Add feature dimension: [num_samples, window_length] -> [num_samples, window_length, 1]\n",
        "            X = sensor_data.reshape(len(sensor_data), self.window_length, 1)\n",
        "            print(f\"  Added feature dimension: {sensor_data.shape} -> {X.shape}\")\n",
        "        else:\n",
        "            # Data already has feature dimension\n",
        "            X = sensor_data\n",
        "            print(f\"  Data already in LSTM format: {X.shape}\")\n",
        "\n",
        "        # Verify final shape\n",
        "        expected_shape = (len(sensor_data), self.window_length, 1)\n",
        "        if X.shape != expected_shape:\n",
        "            raise ValueError(f\"Expected shape {expected_shape}, got {X.shape}\")\n",
        "\n",
        "        # Split data into train/validation/test\n",
        "        # 70% train, 15% validation, 15% test\n",
        "        n_samples = len(X)\n",
        "        n_train = int(0.7 * n_samples)\n",
        "        n_val = int(0.15 * n_samples)\n",
        "\n",
        "        # Shuffle indices to randomize splits\n",
        "        indices = np.random.RandomState(42).permutation(n_samples)\n",
        "\n",
        "        train_idx = indices[:n_train]\n",
        "        val_idx = indices[n_train:n_train + n_val]\n",
        "        test_idx = indices[n_train + n_val:]\n",
        "\n",
        "        X_train = X[train_idx]\n",
        "        X_val = X[val_idx]\n",
        "        X_test = X[test_idx]\n",
        "\n",
        "        print(f\"  Train sequences: {len(X_train)} | shape: {X_train.shape}\")\n",
        "        print(f\"  Validation sequences: {len(X_val)} | shape: {X_val.shape}\")\n",
        "        print(f\"  Test sequences: {len(X_test)} | shape: {X_test.shape}\")\n",
        "\n",
        "        # Build LSTM Autoencoder\n",
        "        model = self.build_lstm_autoencoder()\n",
        "        print(f\"  Model: LSTM Autoencoder with {model.count_params():,} parameters\")\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        print(\"  Starting training...\")\n",
        "        history = model.fit(\n",
        "            X_train, X_train,  # Autoencoder: input = output\n",
        "            epochs=self.epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_data=(X_val, X_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        training_time = datetime.now() - start_time\n",
        "        print(f\"  Training completed in {training_time}\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"  Evaluating on test set...\")\n",
        "        test_loss = model.evaluate(X_test, X_test, verbose=0)\n",
        "        test_mse = test_loss[0] if isinstance(test_loss, list) else test_loss\n",
        "        test_mae = test_loss[1] if isinstance(test_loss, list) and len(test_loss) > 1 else 0.0\n",
        "\n",
        "        print(f\"  Test MSE: {test_mse:.6f}\")\n",
        "        if test_mae > 0:\n",
        "            print(f\"  Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "        # Compute baseline errors for anomaly detection using validation set\n",
        "        print(\"  Computing baseline error statistics...\")\n",
        "        baseline_errors = []\n",
        "\n",
        "        # Process validation set in batches to compute reconstruction errors\n",
        "        for i in range(0, len(X_val), self.batch_size):\n",
        "            batch = X_val[i:i + self.batch_size]\n",
        "            predictions = model.predict(batch, verbose=0)\n",
        "\n",
        "            # Compute MSE for each sequence in the batch\n",
        "            for j, pred in enumerate(predictions):\n",
        "                error = mean_squared_error(batch[j].flatten(), pred.flatten())\n",
        "                baseline_errors.append(error)\n",
        "\n",
        "        # Statistical analysis of baseline errors\n",
        "        baseline_stats = {\n",
        "            'mean': float(np.mean(baseline_errors)),\n",
        "            'std': float(np.std(baseline_errors)) + 1e-8,  # Add small epsilon\n",
        "            'median': float(np.median(baseline_errors)),\n",
        "            'q75': float(np.percentile(baseline_errors, 75)),\n",
        "            'q90': float(np.percentile(baseline_errors, 90)),\n",
        "            'q95': float(np.percentile(baseline_errors, 95)),\n",
        "            'q99': float(np.percentile(baseline_errors, 99)),\n",
        "            'min': float(np.min(baseline_errors)),\n",
        "            'max': float(np.max(baseline_errors)),\n",
        "            'count': len(baseline_errors)\n",
        "        }\n",
        "\n",
        "        # Prepare training information\n",
        "        training_info = {\n",
        "            'sensor_id': sensor_id,\n",
        "            'model_type': 'lstm_autoencoder',\n",
        "            'architecture': {\n",
        "                'window_length': self.window_length,\n",
        "                'latent_dim': self.latent_dim,\n",
        "                'total_params': int(model.count_params())\n",
        "            },\n",
        "            'training_data': {\n",
        "                'total_sequences': len(sensor_data),\n",
        "                'training_sequences': len(X_train),\n",
        "                'validation_sequences': len(X_val),\n",
        "                'test_sequences': len(X_test),\n",
        "                'input_shape': list(X.shape)\n",
        "            },\n",
        "            'training_process': {\n",
        "                'epochs_requested': self.epochs,\n",
        "                'epochs_trained': len(history.history['loss']),\n",
        "                'batch_size': self.batch_size,\n",
        "                'training_time': str(training_time),\n",
        "                'early_stopping': len(history.history['loss']) < self.epochs\n",
        "            },\n",
        "            'performance': {\n",
        "                'final_train_loss': float(history.history['loss'][-1]),\n",
        "                'final_val_loss': float(history.history['val_loss'][-1]),\n",
        "                'best_val_loss': float(min(history.history['val_loss'])),\n",
        "                'test_mse': float(test_mse),\n",
        "                'test_mae': float(test_mae) if test_mae > 0 else None\n",
        "            },\n",
        "            'baseline_errors': baseline_errors[-100:],  # Store last 100 for drift detection\n",
        "            'baseline_stats': baseline_stats,\n",
        "            'trained_at': datetime.now(),\n",
        "            'tensorflow_version': tf.__version__ if KERAS_AVAILABLE else None\n",
        "        }\n",
        "\n",
        "        # Training summary\n",
        "        print(f\"  ‚úÖ Training successful!\")\n",
        "        print(f\"     Epochs trained: {training_info['training_process']['epochs_trained']}\")\n",
        "        print(f\"     Final train loss: {training_info['performance']['final_train_loss']:.6f}\")\n",
        "        print(f\"     Final val loss: {training_info['performance']['final_val_loss']:.6f}\")\n",
        "        print(f\"     Test MSE: {training_info['performance']['test_mse']:.6f}\")\n",
        "        print(f\"     Baseline error: {baseline_stats['mean']:.6f} ¬± {baseline_stats['std']:.6f}\")\n",
        "        print(f\"     Error range: [{baseline_stats['min']:.6f}, {baseline_stats['max']:.6f}]\")\n",
        "\n",
        "        return model, training_info\n",
        "\n",
        "    def save_model(self, model: Model, training_info: Dict, models_dir: str) -> Tuple[str, str]:\n",
        "        \"\"\"Save trained model and metadata.\"\"\"\n",
        "\n",
        "        sensor_id = training_info['sensor_id']\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Save model in Keras format\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        model.save(model_path)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_path = os.path.join(models_dir, f\"sensor_{sensor_id}_metadata.pkl\")\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump(training_info, f)\n",
        "\n",
        "        print(f\"  üíæ Saved model: {model_path}\")\n",
        "        print(f\"  üíæ Saved metadata: {metadata_path}\")\n",
        "\n",
        "        return model_path, metadata_path\n",
        "\n",
        "\n",
        "def load_your_dataset(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load your pre-processed dataset.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to your dataset\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (training_data, holdout_data)\n",
        "        - training_data: [num_samples, window_length, num_sensors] for model training\n",
        "        - holdout_data: [1000, window_length, num_sensors] for streaming simulation\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üìÇ Loading pre-processed dataset from: {data_path}\")\n",
        "\n",
        "    if data_path.endswith('.npy'):\n",
        "        # Numpy array format\n",
        "        print(\"  Format: Numpy array (.npy)\")\n",
        "        data = np.load(data_path)\n",
        "\n",
        "    elif data_path.endswith('.npz'):\n",
        "        # Compressed numpy format\n",
        "        print(\"  Format: Compressed numpy (.npz)\")\n",
        "        data_file = np.load(data_path)\n",
        "        # Assume main data is stored with key 'data' or use first key\n",
        "        if 'data' in data_file:\n",
        "            data = data_file['data']\n",
        "        else:\n",
        "            data = data_file[list(data_file.keys())[0]]\n",
        "\n",
        "    elif data_path.endswith('.h5') or data_path.endswith('.hdf5'):\n",
        "        # HDF5 format\n",
        "        print(\"  Format: HDF5 file\")\n",
        "        import h5py\n",
        "\n",
        "        with h5py.File(data_path, 'r') as f:\n",
        "            # Assume main data is stored with key 'data' or use first key\n",
        "            if 'data' in f:\n",
        "                data = f['data'][:]\n",
        "            else:\n",
        "                data = f[list(f.keys())[0]][:]\n",
        "\n",
        "    elif data_path.endswith('.pkl') or data_path.endswith('.pickle'):\n",
        "        # Pickle format\n",
        "        print(\"  Format: Pickle file\")\n",
        "        import pickle\n",
        "        with open(data_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            # If pickle contains dict, extract the array\n",
        "            if isinstance(data, dict):\n",
        "                if 'data' in data:\n",
        "                    data = data['data']\n",
        "                else:\n",
        "                    data = list(data.values())[0]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported data format: {data_path}. Supported: .npy, .npz, .h5, .hdf5, .pkl\")\n",
        "\n",
        "    # Validate data format\n",
        "    if not isinstance(data, np.ndarray):\n",
        "        raise ValueError(f\"Data must be numpy array, got {type(data)}\")\n",
        "\n",
        "    if data.ndim != 3:\n",
        "        raise ValueError(f\"Data must be 3D [num_samples, window_length, num_sensors], got {data.ndim}D: {data.shape}\")\n",
        "\n",
        "    num_samples, window_length, num_sensors = data.shape\n",
        "    print(f\"  üìä Dataset shape: {data.shape}\")\n",
        "    print(f\"     Total samples: {num_samples:,}\")\n",
        "    print(f\"     Window length: {window_length}\")\n",
        "    print(f\"     Number of sensors: {num_sensors}\")\n",
        "\n",
        "    # Check if data appears scaled\n",
        "    data_min, data_max = np.min(data), np.max(data)\n",
        "    print(f\"  üìà Data range: [{data_min:.3f}, {data_max:.3f}]\")\n",
        "\n",
        "    if -10 <= data_min and data_max <= 10:\n",
        "        print(\"  ‚úÖ Data appears to be pre-scaled\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è Data range seems large - verify it's properly scaled\")\n",
        "\n",
        "    # Check for invalid values\n",
        "    invalid_count = np.sum(~np.isfinite(data))\n",
        "    if invalid_count > 0:\n",
        "        print(f\"  ‚ö†Ô∏è Found {invalid_count} invalid values (NaN/Inf) - will be handled during training\")\n",
        "\n",
        "    # Split into training and holdout\n",
        "    if num_samples <= 1000:\n",
        "        raise ValueError(f\"Dataset too small ({num_samples} samples). Need more than 1000 samples.\")\n",
        "\n",
        "    holdout_data = data[-1000:].copy()  # Last 1000 samples\n",
        "    training_data = data[:-1000].copy()  # Everything except last 1000\n",
        "\n",
        "    print(f\"  üìä Data split:\")\n",
        "    print(f\"     Training: {training_data.shape[0]:,} samples\")\n",
        "    print(f\"     Holdout: {holdout_data.shape[0]:,} samples\")\n",
        "\n",
        "    return training_data, holdout_data\n",
        "\n",
        "\n",
        "def validate_dataset(dataset: np.ndarray, window_length: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Validate the pre-processed dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: Pre-processed dataset [num_samples, window_length, num_sensors]\n",
        "        window_length: Expected window length\n",
        "\n",
        "    Returns:\n",
        "        Validated dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîç Validating dataset...\")\n",
        "\n",
        "    num_samples, actual_window_length, num_sensors = dataset.shape\n",
        "\n",
        "    # Check window length\n",
        "    if actual_window_length != window_length:\n",
        "        raise ValueError(f\"Window length mismatch: expected {window_length}, got {actual_window_length}\")\n",
        "\n",
        "    # Check for invalid values\n",
        "    invalid_mask = ~np.isfinite(dataset)\n",
        "    invalid_count = np.sum(invalid_mask)\n",
        "\n",
        "    if invalid_count > 0:\n",
        "        invalid_ratio = invalid_count / dataset.size\n",
        "        print(f\"  ‚ö†Ô∏è Found {invalid_count:,} invalid values ({invalid_ratio:.2%} of total)\")\n",
        "\n",
        "        if invalid_ratio > 0.1:\n",
        "            raise ValueError(f\"Too many invalid values ({invalid_ratio:.1%}). Check data quality.\")\n",
        "\n",
        "        # Replace invalid values with sensor means\n",
        "        print(\"  üîß Replacing invalid values with sensor means...\")\n",
        "        for sensor_id in range(num_sensors):\n",
        "            sensor_data = dataset[:, :, sensor_id]\n",
        "            valid_data = sensor_data[np.isfinite(sensor_data)]\n",
        "            if len(valid_data) > 0:\n",
        "                mean_val = np.mean(valid_data)\n",
        "                sensor_mask = invalid_mask[:, :, sensor_id]\n",
        "                dataset[:, :, sensor_id][sensor_mask] = mean_val\n",
        "\n",
        "    # Statistical validation per sensor\n",
        "    print(f\"  üìä Per-sensor statistics:\")\n",
        "    for sensor_id in range(num_sensors):\n",
        "        sensor_data = dataset[:, :, sensor_id]\n",
        "\n",
        "        mean_val = np.mean(sensor_data)\n",
        "        std_val = np.std(sensor_data)\n",
        "        min_val = np.min(sensor_data)\n",
        "        max_val = np.max(sensor_data)\n",
        "\n",
        "        print(f\"    Sensor {sensor_id}: Œº={mean_val:.3f}, œÉ={std_val:.3f}, \"\n",
        "              f\"range=[{min_val:.3f}, {max_val:.3f}]\")\n",
        "\n",
        "        # Check for constant values\n",
        "        if std_val < 1e-6:\n",
        "            print(f\"    ‚ö†Ô∏è Sensor {sensor_id}: Nearly constant values (œÉ={std_val:.2e})\")\n",
        "\n",
        "    print(f\"‚úÖ Dataset validation completed\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline for pre-processed dataset.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Train LSTM Autoencoder models on pre-processed dataset\")\n",
        "    parser.add_argument('--data_path', type=str, required=True,\n",
        "                       help='Path to pre-processed dataset [num_samples, window_length, num_sensors]')\n",
        "    parser.add_argument('--models_dir', type=str, default='./trained_models',\n",
        "                       help='Directory to save models')\n",
        "    parser.add_argument('--holdout_dir', type=str, default='./holdout_data',\n",
        "                       help='Directory to save holdout data for streaming simulation')\n",
        "    parser.add_argument('--window_length', type=int, default=50,\n",
        "                       help='Expected sequence window length')\n",
        "    parser.add_argument('--latent_dim', type=int, default=32,\n",
        "                       help='LSTM latent dimension')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    parser.add_argument('--sensors', type=str, default=None,\n",
        "                       help='Comma-separated sensor IDs to train (default: all)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"üöÄ SENSOR MODEL PRE-TRAINING SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìÇ Data path: {args.data_path}\")\n",
        "    print(f\"üíæ Models directory: {args.models_dir}\")\n",
        "    print(f\"üì¶ Holdout directory: {args.holdout_dir}\")\n",
        "    print(f\"üìè Window length: {args.window_length}\")\n",
        "    print(f\"üß† Model: LSTM Autoencoder (latent_dim={args.latent_dim})\")\n",
        "    print(f\"üîÑ Epochs: {args.epochs}\")\n",
        "    print(f\"üì¶ Batch size: {args.batch_size}\")\n",
        "    print()\n",
        "\n",
        "    # Load and split dataset\n",
        "    training_data, holdout_data = load_your_dataset(args.data_path)\n",
        "\n",
        "    # Validate dataset\n",
        "    training_data = validate_dataset(training_data, args.window_length)\n",
        "\n",
        "    # Save holdout data for streaming simulation\n",
        "    os.makedirs(args.holdout_dir, exist_ok=True)\n",
        "    holdout_path = os.path.join(args.holdout_dir, 'holdout_data.npy')\n",
        "    np.save(holdout_path, holdout_data)\n",
        "    print(f\"üíæ Saved holdout data to: {holdout_path}\")\n",
        "    print(f\"   Holdout shape: {holdout_data.shape}\")\n",
        "\n",
        "    # Get dataset dimensions\n",
        "    num_samples, window_length, num_sensors = training_data.shape\n",
        "\n",
        "    # Filter sensors if specified\n",
        "    sensor_list = list(range(num_sensors))\n",
        "    if args.sensors:\n",
        "        requested_sensors = [int(x.strip()) for x in args.sensors.split(',')]\n",
        "        sensor_list = [sid for sid in requested_sensors if sid < num_sensors]\n",
        "        print(f\"üéØ Training only sensors: {sensor_list}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = SensorModelTrainer(\n",
        "        window_length=window_length,\n",
        "        latent_dim=args.latent_dim,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    # Train models for each sensor\n",
        "    print(f\"\\nüèãÔ∏è TRAINING {len(sensor_list)} LSTM AUTOENCODER MODELS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    training_results = {}\n",
        "    successful_training = 0\n",
        "\n",
        "    for sensor_id in sensor_list:\n",
        "        try:\n",
        "            print(f\"\\nüéØ SENSOR {sensor_id}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Extract data for this sensor: [num_samples, window_length]\n",
        "            sensor_sequences = training_data[:, :, sensor_id]\n",
        "\n",
        "            # Train model\n",
        "            model, training_info = trainer.train_sensor_model(sensor_sequences, sensor_id)\n",
        "\n",
        "            # Save model\n",
        "            model_path, metadata_path = trainer.save_model(model, training_info, args.models_dir)\n",
        "\n",
        "            training_results[sensor_id] = {\n",
        "                'success': True,\n",
        "                'model_path': model_path,\n",
        "                'metadata_path': metadata_path,\n",
        "                'training_info': training_info\n",
        "            }\n",
        "\n",
        "            successful_training += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Training failed: {str(e)}\")\n",
        "            import traceback\n",
        "            print(f\"  üìã Error details: {traceback.format_exc()}\")\n",
        "\n",
        "            training_results[sensor_id] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nüìä TRAINING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"‚úÖ Successful: {successful_training}/{len(sensor_list)} sensors\")\n",
        "    print(f\"üíæ Models saved to: {args.models_dir}\")\n",
        "    print(f\"üì¶ Holdout data saved to: {holdout_path}\")\n",
        "\n",
        "    if successful_training > 0:\n",
        "        print(f\"\\nüèÜ TRAINED SENSORS:\")\n",
        "        for sensor_id, result in training_results.items():\n",
        "            if result['success']:\n",
        "                info = result['training_info']\n",
        "                perf = info['performance']\n",
        "                print(f\"  Sensor {sensor_id}: {info['training_process']['epochs_trained']} epochs, \"\n",
        "                      f\"test MSE: {perf['test_mse']:.6f}, \"\n",
        "                      f\"baseline error: {info['baseline_stats']['mean']:.6f}\")\n",
        "\n",
        "    failed_sensors = [sid for sid, result in training_results.items() if not result['success']]\n",
        "    if failed_sensors:\n",
        "        print(f\"\\n‚ùå FAILED SENSORS: {failed_sensors}\")\n",
        "        print(\"   Check error messages above for details\")\n",
        "\n",
        "    # Save comprehensive training summary\n",
        "    summary_path = os.path.join(args.models_dir, 'training_summary.pkl')\n",
        "    training_summary = {\n",
        "        'training_results': training_results,\n",
        "        'config': vars(args),\n",
        "        'dataset_info': {\n",
        "            'original_shape': training_data.shape,\n",
        "            'holdout_shape': holdout_data.shape,\n",
        "            'num_sensors': num_sensors,\n",
        "            'window_length': window_length,\n",
        "            'total_training_samples': num_samples\n",
        "        },\n",
        "        'model_info': {\n",
        "            'model_type': 'lstm_autoencoder',\n",
        "            'latent_dim': args.latent_dim,\n",
        "            'architecture': 'Encoder-Decoder with RepeatVector'\n",
        "        },\n",
        "        'timestamp': datetime.now(),\n",
        "        'successful_sensors': successful_training,\n",
        "        'failed_sensors': failed_sensors,\n",
        "        'holdout_data_path': holdout_path,\n",
        "        'tensorflow_version': tf.__version__ if KERAS_AVAILABLE else None\n",
        "    }\n",
        "\n",
        "    with open(summary_path, 'wb') as f:\n",
        "        pickle.dump(training_summary, f)\n",
        "\n",
        "    print(f\"\\nüíæ Training summary saved to: {summary_path}\")\n",
        "    print(f\"‚úÖ PRE-TRAINING COMPLETED!\")\n",
        "\n",
        "    # Instructions for next steps\n",
        "    print(f\"\\nüìã NEXT STEPS:\")\n",
        "    print(f\"  1. Use trained models: {args.models_dir}\")\n",
        "    print(f\"  2. Use holdout data for streaming: {holdout_path}\")\n",
        "    print(f\"  3. Run production system with:\")\n",
        "    print(f\"     python production_agent_system.py --models_dir {args.models_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE4-Sensor-Pretraining.ipynb",
      "authorship_tag": "ABX9TyOZY0sOrL67QspOCYHr3SdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}