{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx-5b_puABG1",
        "outputId": "839c68bc-6f7c-45ab-db07-913636a10fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existing VAR analysis results found. Need to run VAR analysis...\n",
            "Starting VAR analysis with dynamic batching...\n",
            "Starting fresh VAR analysis\n",
            "Total samples to process: 362700\n",
            "VAR could not solve row number 0 48\n",
            "VAR could not solve row number 0 49\n",
            "Processed 1000/362700 samples. Current SW = 11\n",
            "Processed 2000/362700 samples. Current SW = 32\n",
            "Processed 3000/362700 samples. Current SW = 41\n",
            "Processed 4000/362700 samples. Current SW = 25\n",
            "Processed 5000/362700 samples. Current SW = 9\n",
            "Checkpoint saved at sample 5000\n",
            "VAR could not solve row number 5000 48\n",
            "VAR could not solve row number 5000 49\n",
            "Processed 6000/362700 samples. Current SW = 24\n",
            "Processed 7000/362700 samples. Current SW = 6\n",
            "Processed 8000/362700 samples. Current SW = 32\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed, Input\n",
        "from keras.models import Model\n",
        "from keras import saving\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "import plotly\n",
        "from keras import losses\n",
        "import plotly.express as px\n",
        "\n",
        "# Load data\n",
        "train_data = np.load(r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-Daily-DIRECT-VAR.npy')\n",
        "window_label = np.load(r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences_WINDOW-Daily-DIRECT-VAR.npy')\n",
        "\n",
        "n_seq = train_data.shape[0]\n",
        "window_size = train_data.shape[1]\n",
        "n_features = train_data.shape[2]\n",
        "\n",
        "maxval = train_data.shape[0]\n",
        "count_train = int(math.ceil(0.8*maxval))\n",
        "x_train = train_data[:count_train]\n",
        "x_test = train_data[count_train:]\n",
        "\n",
        "# Always define the custom Sampling layer (needed for loading existing models)\n",
        "saving.get_custom_objects().clear()\n",
        "\n",
        "@saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def __init__(self, factor):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "\n",
        "# Check if trained VAE models already exist\n",
        "encoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-encoder-latent5-dim256.keras'\n",
        "decoder_path = r'/content/drive/MyDrive/PHD/2025/VAE_SIMULATION/METROPM_vae-decoder-latent5-dim256.keras'\n",
        "\n",
        "if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "    print(\"Found existing trained VAE models. Loading...\")\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "    print(\"VAE models loaded successfully!\")\n",
        "    history = None  # No training history since we didn't train\n",
        "\n",
        "else:\n",
        "    print(\"No existing VAE models found. Training new VAE...\")\n",
        "\n",
        "    # Build the encoder\n",
        "    latent_dim = 5\n",
        "    intermediate_dim = 256\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(window_size, n_features), name=\"encoder_input\")\n",
        "    x = layers.LSTM(intermediate_dim, activation='tanh', name=\"lstm1\", return_sequences=True)(encoder_inputs)\n",
        "    xx = layers.LSTM(int(intermediate_dim/2), activation='tanh', name=\"lstm2\", return_sequences=False)(x)\n",
        "    x1 = layers.Dense(int(intermediate_dim/2), name=\"dense\")(xx)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x1)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x1)\n",
        "    z = Sampling(1)([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "\n",
        "    # Decoder\n",
        "    inp_z = Input(shape=(latent_dim,), name=\"decoder\")\n",
        "    x1 = layers.RepeatVector(window_size, name=\"repeatvect\")(inp_z)\n",
        "    x2 = layers.Dense(int(intermediate_dim/2), name=\"Dense2\")(x1)\n",
        "    x22 = layers.LSTM(int(intermediate_dim/2), activation='tanh', return_sequences=True, name=\"lstm1\")(x2)\n",
        "    x3 = layers.LSTM(intermediate_dim, activation='tanh', return_sequences=True, name=\"lstm2\")(x22)\n",
        "    decode_out = layers.TimeDistributed(Dense(n_features), name=\"decodeout\")(x3)\n",
        "    decoder = keras.Model(inp_z, decode_out, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "\n",
        "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "    # Parameters\n",
        "    n_epochs = 150\n",
        "    klstart = 20\n",
        "    kl_annealtime = n_epochs - klstart\n",
        "    weight = K.variable(0.0)\n",
        "\n",
        "    # Define the VAE as a Model with a custom train_step\n",
        "    class VAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(VAE, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "        def train_step(self, data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(data)\n",
        "                reconstruction = self.decoder(z)\n",
        "                reconstruction_loss = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "                )\n",
        "\n",
        "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "                total_loss = reconstruction_loss + (weight * kl_loss)\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        def test_step(self, data):\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    losses.mean_squared_error(data, reconstruction), axis=-1), keepdims=True\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "    # CALLBACKS\n",
        "    es = keras.callbacks.EarlyStopping(patience=50, verbose=1, min_delta=0.0001, monitor='loss', mode='auto', restore_best_weights=True)\n",
        "\n",
        "    class AnnealingCallback(Callback):\n",
        "        def __init__(self, weight):\n",
        "            self.weight = weight\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if epoch > klstart and epoch < klstart * 1.2:\n",
        "                new_weight = min(K.get_value(self.weight) + (1. / kl_annealtime), 1.)\n",
        "                K.set_value(self.weight, new_weight)\n",
        "            print(\"Current KL Weight is \" + str(K.get_value(self.weight)))\n",
        "\n",
        "    # Train the VAE\n",
        "    vae = VAE(encoder, decoder)\n",
        "    vae.compile(optimizer=keras.optimizers.Adam(clipnorm=1))\n",
        "    history = vae.fit(x_train,\n",
        "                      epochs=n_epochs,\n",
        "                      batch_size=32,\n",
        "                      validation_split=0.1,\n",
        "                      callbacks=[AnnealingCallback(weight)])\n",
        "\n",
        "    # Save models\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "    print(\"VAE training complete and models saved!\")\n",
        "\n",
        "    # Reload models to ensure consistency\n",
        "    encoder = keras.models.load_model(encoder_path)\n",
        "    decoder = keras.models.load_model(decoder_path)\n",
        "\n",
        "# Plot training history (only if we actually trained)\n",
        "if history is not None:\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['reconstruction_loss'], label='reconstruction_loss')\n",
        "    plt.plot(history.history['kl_loss'], label='kl_Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "    # Just Loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training plots since VAE was loaded from existing models\")\n",
        "\n",
        "# PLOT TRAIN RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_train)\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Training Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_train[:, :, 5], \"r\", label=\"Actual\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\", label=\"reconstructed\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT TEST RECONSTRUCTION\n",
        "X_test_encoded = encoder.predict(x_test[:, :, :])\n",
        "X_test_predict = decoder(X_test_encoded[2])\n",
        "plt.suptitle('Example Reconstruction of Testing Data')\n",
        "plt.xlabel('Time', fontsize='10')\n",
        "plt.ylabel('Feature 6', fontsize='10')\n",
        "plt.plot(x_test[:, :, 5], \"r\")\n",
        "plt.plot(X_test_predict[:, :, 5], \"b\")\n",
        "plt.show()\n",
        "\n",
        "# Get the corresponding window labels for test data\n",
        "window_label_test = window_label[count_train:]  # Get test portion of window_label\n",
        "fig = px.scatter(None, x=X_test_encoded[2][:, 0], y=X_test_encoded[2][:, 1], opacity=1, color=window_label_test.astype(str))\n",
        "fig.update_layout(dict(plot_bgcolor='white'))\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()\n",
        "\n",
        "# Check if generated data already exists\n",
        "generated_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated_large_subsquence2_data.npy'\n",
        "\n",
        "if os.path.exists(generated_data_path):\n",
        "    print(\"Found existing generated data. Loading...\")\n",
        "    results1 = np.load(generated_data_path)\n",
        "    print(f\"Loaded generated data with shape: {results1.shape}\")\n",
        "else:\n",
        "    print(\"No existing generated data found. Generating new data...\")\n",
        "    # Generate data for MLP\n",
        "    generator_multiply = 100\n",
        "\n",
        "    X_train_encoded = encoder.predict(train_data)\n",
        "    mu, logvar, z = X_train_encoded\n",
        "    sigma = tf.exp(0.5 * logvar)\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "    store = list()\n",
        "\n",
        "    for i in range(0, batch):\n",
        "        all_Z_i = tf.random.normal(shape=(generator_multiply, dim), mean=mu[i, :], stddev=sigma[i, :])\n",
        "        X_train_decoded = decoder.predict(all_Z_i)\n",
        "        X_train_decoded = X_train_decoded.reshape((X_train_decoded.shape[0], window_size * n_features))\n",
        "        store.append(X_train_decoded)\n",
        "\n",
        "    results1 = np.concatenate(store, axis=0)\n",
        "    np.save(generated_data_path, results1)\n",
        "    print(f\"Generated and saved new data with shape: {results1.shape}\")\n",
        "\n",
        "# Generate window labels using VAR analysis - Dynamic batching with checkpoints\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "# Check if final VAR analysis results already exist\n",
        "final_window_labels_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy'\n",
        "final_data_path = r'/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data2.npy'\n",
        "\n",
        "if os.path.exists(final_window_labels_path) and os.path.exists(final_data_path):\n",
        "    print(\"Found existing VAR analysis results. Loading...\")\n",
        "    y = np.load(final_window_labels_path)\n",
        "    x_final = np.load(final_data_path)\n",
        "    print(f\"Loaded final results - X: {x_final.shape}, Y: {y.shape}\")\n",
        "else:\n",
        "    print(\"No existing VAR analysis results found. Need to run VAR analysis...\")\n",
        "\n",
        "    # Make sure we have the generated data (results1)\n",
        "    x = results1\n",
        "    x_3d = x.reshape((x.shape[0], window_size, n_features))\n",
        "    n_future = 1\n",
        "    K = window_size\n",
        "\n",
        "    from statsmodels.tsa.api import VAR\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    def process_with_dynamic_batching(x_3d, batch_size=50000, checkpoint_dir=\"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM\"):\n",
        "        checkpoint_file = f\"{checkpoint_dir}/var_analysis_progress.pkl\"\n",
        "\n",
        "        # Load previous progress if exists\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                start_idx, all_windows = pickle.load(f)\n",
        "            print(f\"Resuming VAR analysis from sample {start_idx}. Already processed: {len(all_windows)} samples\")\n",
        "        else:\n",
        "            start_idx, all_windows = 0, []\n",
        "            print(\"Starting fresh VAR analysis\")\n",
        "\n",
        "        total_samples = x_3d.shape[0]\n",
        "        print(f\"Total samples to process: {total_samples}\")\n",
        "\n",
        "        batch_num = start_idx // batch_size + 1\n",
        "\n",
        "        for i in range(start_idx, total_samples):\n",
        "            # Your exact same VAR analysis logic\n",
        "            rmse_list = []\n",
        "            for k in range(2, round(K)):\n",
        "                cur_seq = x_3d[i, :, :]\n",
        "                df = pd.DataFrame(cur_seq, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12'])\n",
        "                df_train, df_test = df[0:-n_future], df[-n_future:]\n",
        "                model = VAR(df_train)\n",
        "                try:\n",
        "                    model_fitted1 = model.fit(k)\n",
        "                    forecast_input1 = df_train.values[-k:]\n",
        "                    fc1 = model_fitted1.forecast(y=forecast_input1, steps=n_future)\n",
        "                    df_forecast1 = pd.DataFrame(fc1, index=df.index[-n_future:], columns=df.columns)\n",
        "                    mse = mean_squared_error(df_test['V1'], df_forecast1['V1'].values)\n",
        "                    rmse_list.append(mse)\n",
        "                except:\n",
        "                    rmse_list.append(99999)\n",
        "                    if i % 5000 == 0:  # Reduce print frequency\n",
        "                        print('VAR could not solve row number', i, k)\n",
        "\n",
        "            min_index = rmse_list.index(min(rmse_list))\n",
        "            min_sw = min_index + 2\n",
        "            all_windows.append(min_sw)\n",
        "\n",
        "            # Progress reporting and checkpointing\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f'Processed {i + 1}/{total_samples} samples. Current SW = {min_sw}')\n",
        "\n",
        "            # Save checkpoint every 5000 samples\n",
        "            if (i + 1) % 5000 == 0:\n",
        "                with open(checkpoint_file, 'wb') as f:\n",
        "                    pickle.dump((i + 1, all_windows), f)\n",
        "                print(f\"Checkpoint saved at sample {i + 1}\")\n",
        "\n",
        "                # Optional: Clean up memory\n",
        "                gc.collect()\n",
        "\n",
        "            # Save batch results (compatible with original format) every batch_size samples\n",
        "            if (i + 1) % batch_size == 0 or (i + 1) == total_samples:\n",
        "                batch_start = ((i + 1) - 1) // batch_size * batch_size\n",
        "                batch_windows = all_windows[batch_start:i + 1]\n",
        "                batch_data = x[batch_start:i + 1, :]\n",
        "\n",
        "                np.save(f'{checkpoint_dir}/generated-data-true-window2-BATCH{batch_num}.npy', np.array(batch_windows))\n",
        "                np.save(f'{checkpoint_dir}/generated-data2-BATCH{batch_num}.npy', batch_data)\n",
        "\n",
        "                print(f\"Batch {batch_num} saved: samples {batch_start} to {i}\")\n",
        "                batch_num += 1\n",
        "\n",
        "        # Final save\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump((total_samples, all_windows), f)\n",
        "\n",
        "        # Clean up checkpoint file\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "\n",
        "        return np.array(all_windows)\n",
        "\n",
        "    # Run the dynamic batching process\n",
        "    print(\"Starting VAR analysis with dynamic batching...\")\n",
        "    y = process_with_dynamic_batching(x_3d)\n",
        "\n",
        "    # Save final combined results\n",
        "    np.save(final_window_labels_path, y)\n",
        "    np.save(final_data_path, x[:len(y)])\n",
        "    print(f\"VAR analysis complete. Saved final results - X: {x[:len(y)].shape}, Y: {y.shape}\")\n",
        "\n",
        "print(\"VAE training complete. Generated data and window labels saved.\")\n",
        "print(f\"Generated data shape: {x[:len(y)].shape}\")\n",
        "print(f\"Window labels shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nxij89jyeebm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE2-VAE.ipynb",
      "authorship_tag": "ABX9TyM5hYnlbcaj9/eD8SZvSxCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}