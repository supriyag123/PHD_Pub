{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-REVISED-DecisionExpertLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoP7OuWNxlsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e034bd-2e91-4e70-97e7-07d24322f478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 10839 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79a40336b1a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 10840 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79a1942b0180> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Fault Classification Pipeline\n",
        "# ============================================================\n",
        "\n",
        "############PASTE ADAPTIVE WINDOW HERE - so everything is in one file - later, we can import as a package#####################\n",
        "\n",
        "\n",
        "# ====== AdaptiveWindowAgent ======\n",
        "# =====================================================\n",
        "# AdaptiveWindowAgent (improved version)\n",
        "# =====================================================\n",
        "# agents/adaptive_window_agent.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, Any\n",
        "import datetime as dt\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor  # ✅ MOVED TO TOP\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "class EventStore:\n",
        "    def __init__(self, db_path=\"event_store.db\"):\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self._init_tables()\n",
        "\n",
        "    def _init_tables(self):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS events (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT,\n",
        "                event_type TEXT,\n",
        "                packet_json TEXT,\n",
        "                expert_json TEXT,\n",
        "                human_json TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.conn.commit()\n",
        "\n",
        "    def save_event(self, event_type, packet, expert=None, human=None):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\n",
        "            \"INSERT INTO events(timestamp,event_type,packet_json,expert_json,human_json) VALUES (?,?,?,?,?)\",\n",
        "            (\n",
        "                datetime.now().isoformat(),\n",
        "                event_type,\n",
        "                json.dumps(packet, default=str),\n",
        "                json.dumps(expert, default=str) if expert else None,\n",
        "                json.dumps(human, default=str) if human else None,\n",
        "            )\n",
        "        )\n",
        "        self.conn.commit()\n",
        "\n",
        "    def fetch_recent(self, limit=100):\n",
        "        c = self.conn.cursor()\n",
        "        c.execute(\"SELECT packet_json, expert_json, human_json FROM events ORDER BY id DESC LIMIT ?\", (limit,))\n",
        "        return [json.loads(row[0]) for row in c.fetchall()]\n",
        "\n",
        "######################################################################################\n",
        "# DECISION POLICY CONFIGS-----------------------------------------------------------\n",
        "######################################################################################'\n",
        "\n",
        "# =====================================================\n",
        "# DECISION POLICY (SEPARATED FROM LOGIC)\n",
        "# =====================================================\n",
        "\n",
        "BASE_POLICY = {\n",
        "    # --- detection fusion ---\n",
        "    \"w_sensor\": 0.60,\n",
        "    \"w_window\": 0.40,\n",
        "\n",
        "    # --- drift fusion ---\n",
        "    \"w_fdi\": 0.50,\n",
        "    \"w_wdi\": 0.25,\n",
        "    \"w_sensor_drift\": 0.25,\n",
        "\n",
        "    # --- failure prediction ---\n",
        "    \"w_fault\": 0.70,\n",
        "    \"w_warn\": 0.15,\n",
        "    \"w_det_context\": 0.15,\n",
        "\n",
        "    # --- thresholds ---\n",
        "    \"detection_threshold\": 0.50,\n",
        "    \"drift_threshold\": 0.35,\n",
        "    \"failure_threshold\": 0.50,\n",
        "    \"failure_critical_threshold\": 0.80,\n",
        "\n",
        "    # --- alert mapping ---\n",
        "    \"alert_low\": 0.35,\n",
        "    \"alert_med\": 0.55,\n",
        "    \"alert_high\": 0.75,\n",
        "}\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Window Agent - Global Context or Global Predictive Context\n",
        "#########################################################################\n",
        "\n",
        "class AdaptiveWindowAgent:\n",
        "    \"\"\"\n",
        "    Adaptive Window Agent:\n",
        "    - Predicts window size using MLP\n",
        "    - Evaluates forecast with RF/persistence\n",
        "    - Computes:\n",
        "        * FDS: Forecast Deviation Score (normalized error)\n",
        "        * FDI: Forecast Drift Index (JSD over FDS distribution)\n",
        "        * WSS: Window Shift Score (normalized window size)\n",
        "        * WDI: Window Drift Index (JSD over window size distribution)\n",
        "    - Detects anomaly (local) + drift (regime) events.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id=\"adaptive_window_agent\",\n",
        "                 model_path=None, checkpoint_path=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.model_path = model_path or \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        "\n",
        "        self.baseline_path = \"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_baseline.pkl\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Core model\n",
        "        # --------------------------------------------------\n",
        "        self.model = None\n",
        "        self.transformer = StandardScaler()\n",
        "        self.transformer_fitted = False\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Baseline error stats (median / MAD) – filled from baseline file\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Metrics memory\n",
        "        # --------------------------------------------------\n",
        "        # Raw error\n",
        "        self.error_memory = deque(maxlen=300)     # long-term errors\n",
        "        self.recent_errors = deque(maxlen=50)     # kept for backward compat (not central now)\n",
        "\n",
        "        # FDS (Forecast Deviation Score) history\n",
        "        self.fds_memory = deque(maxlen=300)\n",
        "        self.recent_fds = deque(maxlen=50)\n",
        "\n",
        "        # Window history\n",
        "        self.window_memory = deque(maxlen=300)\n",
        "        self.recent_windows = deque(maxlen=50)\n",
        "\n",
        "        # Last-step metrics (for returning)\n",
        "        self.last_fds = 0.0\n",
        "        self.last_fdi = 0.0\n",
        "        self.last_wss = 0.0\n",
        "        self.last_wdi = 0.0\n",
        "\n",
        "        # Baseline distributions\n",
        "        self.baseline_errors = None\n",
        "        self.baseline_fds = None\n",
        "        self.baseline_windows = None\n",
        "        self.window_mean = 50.0    # a reasonable mid value\n",
        "        self.window_std = 10.0     # non-zero, avoids div-by-zero\n",
        "\n",
        "        # Optional histogram bins stored in baseline\n",
        "        self.window_hist_bins = None\n",
        "        self.window_hist_counts = None\n",
        "\n",
        "        # Debug flag (OFF by default)\n",
        "        self.debug = False\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Anomaly / drift settings\n",
        "        # --------------------------------------------------\n",
        "        self.threshold_k = 3.0\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.anomaly_cooldown_steps = 5\n",
        "\n",
        "        # Drift detection\n",
        "        self.drift_threshold = 0.25          # for FDI (JSD over FDS)\n",
        "        self.window_drift_threshold = 0.20   # for WDI (JSD over window sizes)\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.drift_cooldown = 0\n",
        "        self.drift_votes_required = 10\n",
        "        self.drift_cooldown_steps = 100\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Retraining buffer (unchanged)\n",
        "        # --------------------------------------------------\n",
        "        self.performance_stats = {\n",
        "            'total_predictions': 0,\n",
        "            'avg_mse': 0.0,\n",
        "            'avg_mae': 0.0,\n",
        "            'last_retrain_time': None,\n",
        "            'drift_events': 0,\n",
        "            'anomaly_events': 0,\n",
        "            'retraining_events': 0\n",
        "        }\n",
        "\n",
        "        self.retraining_data = {\n",
        "            'x_buffer': deque(maxlen=10000),\n",
        "            'y_buffer': deque(maxlen=10000)\n",
        "        }\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Prediction history (for reporting)\n",
        "        # --------------------------------------------------\n",
        "        self.prediction_history = deque(maxlen=1000)\n",
        "        self.mse_history = deque(maxlen=200)\n",
        "        self.mae_history = deque(maxlen=200)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load baseline (errors + windows)\n",
        "        # --------------------------------------------------\n",
        "        self._load_baseline()\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load model last\n",
        "        # --------------------------------------------------\n",
        "        self.load_model()\n",
        "        print(f\"AdaptiveWindowAgent {self.agent_id} initialized\")\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Load NSP (Next-Step Predictor)\n",
        "        # --------------------------------------------------\n",
        "        self.nsp_model_path = \"/content/drive/MyDrive/PHD/2025/NSP_LSTM_next_step.keras\"\n",
        "        self.nsp_model = keras.models.load_model(self.nsp_model_path)\n",
        "        print(\"✅ Loaded NSP LSTM next-step predictor\")\n",
        "\n",
        "    # =================== BASELINE LOADING ===================\n",
        "\n",
        "    def _load_baseline(self):\n",
        "        \"\"\"\n",
        "        Load baseline stats:\n",
        "          - baseline_errors, median, mad\n",
        "          - baseline_windows, window_mean, window_std\n",
        "          - optional histogram bins/counts for windows\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.baseline_path):\n",
        "            with open(self.baseline_path, \"rb\") as f:\n",
        "                base = pickle.load(f)\n",
        "\n",
        "            # Error baseline\n",
        "            self.baseline_errors = np.array(base[\"baseline_errors\"])\n",
        "            self.rolling_stats[\"median\"] = base[\"median\"]\n",
        "            self.rolling_stats[\"mad\"] = base[\"mad\"]\n",
        "\n",
        "            # Precompute baseline FDS distribution\n",
        "            med = self.rolling_stats[\"median\"]\n",
        "            mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "            self.baseline_fds = (self.baseline_errors - med) / (mad + 1e-8)\n",
        "\n",
        "            # Window baseline (may or may not exist)\n",
        "            if \"baseline_windows\" in base:\n",
        "                self.baseline_windows = np.array(base[\"baseline_windows\"])\n",
        "                self.window_mean = float(base.get(\"window_mean\", np.mean(self.baseline_windows)))\n",
        "                self.window_std = float(base.get(\"window_std\", np.std(self.baseline_windows) + 1e-8))\n",
        "                self.window_hist_bins = np.array(base.get(\"window_hist_bins\", [])) if \"window_hist_bins\" in base else None\n",
        "                self.window_hist_counts = np.array(base.get(\"window_hist_counts\", [])) if \"window_hist_counts\" in base else None\n",
        "            else:\n",
        "                self.baseline_windows = None\n",
        "                self.window_mean = 0.0\n",
        "                self.window_std = 1.0\n",
        "                self.window_hist_bins = None\n",
        "                self.window_hist_counts = None\n",
        "\n",
        "            print(\"✅ Loaded baseline error + window distribution.\")\n",
        "            print(f\"   Error median={self.rolling_stats['median']:.6f}, MAD={self.rolling_stats['mad']:.6f}\")\n",
        "            if self.baseline_windows is not None:\n",
        "                print(f\"   Window mean={self.window_mean:.3f}, std={self.window_std:.3f}\")\n",
        "        else:\n",
        "            print(\"⚠️ No baseline found. Using live history only.\")\n",
        "            self.baseline_errors = None\n",
        "            self.baseline_fds = None\n",
        "            self.baseline_windows = None\n",
        "\n",
        "    # =================== MODEL LOADING ===================\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                self.model = keras.models.load_model(self.model_path)\n",
        "                self.is_model_loaded = True\n",
        "                print(f\"✅ Loaded MLP model from {self.model_path}\")\n",
        "\n",
        "                # Try to load transformer\n",
        "                transformer_path = self.model_path.replace('.keras', '_transformer.pkl')\n",
        "                if os.path.exists(transformer_path):\n",
        "                    with open(transformer_path, 'rb') as f:\n",
        "                        self.transformer = pickle.load(f)\n",
        "                    self.transformer_fitted = True\n",
        "                else:\n",
        "                    # Fit transformer from true window labels\n",
        "                    y_original = np.load(\n",
        "                        \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/generated-data-true-window2.npy\"\n",
        "                    )\n",
        "                    self.transformer.fit(y_original.reshape(-1, 1))\n",
        "                    self.transformer_fitted = True\n",
        "                    with open(transformer_path, 'wb') as f:\n",
        "                        pickle.dump(self.transformer, f)\n",
        "                    print(\"⚠️ No transformer found, fitted a new one.\")\n",
        "            else:\n",
        "                print(f\"❌ Model not found at {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {e}\")\n",
        "\n",
        "    # =================== FORECAST EVALUATION ===================\n",
        "\n",
        "    def evaluate_forecast_performance(self, sequence_3d, predicted_window, n_future=1):\n",
        "        try:\n",
        "            seq = np.asarray(sequence_3d)\n",
        "            T, F = seq.shape\n",
        "\n",
        "            W = int(predicted_window)\n",
        "            if W < 2:\n",
        "                W = 2\n",
        "            if W > T - n_future - 1:\n",
        "                W = max(2, T - n_future - 1)\n",
        "\n",
        "            # --- Prepare NSP input ---\n",
        "            window = seq[-W:-n_future, :]   # shape (W-1, F)\n",
        "            x = window[np.newaxis, ...]      # shape (1, W-1, F)\n",
        "\n",
        "            # --- NSP prediction ---\n",
        "            y_pred = self.nsp_model.predict(x, verbose=0)[0]  # (F,)\n",
        "            y_true = seq[-n_future, :]                        # (F,)\n",
        "\n",
        "            mse = float(np.mean((y_true - y_pred) ** 2))\n",
        "            mae = float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "            return {\n",
        "                \"mse\": mse,\n",
        "                \"mae\": mae,\n",
        "                \"forecast_success\": True,\n",
        "                \"actual_values\": y_true.tolist(),\n",
        "                \"predicted_values\": y_pred.tolist(),\n",
        "                \"window_size_used\": W,\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"mse\": 9999.0,\n",
        "                \"mae\": 9999.0,\n",
        "                \"forecast_success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"method\": \"NSP_LSTM\"\n",
        "            }\n",
        "\n",
        "    # =================== PERSISTENCE FALLBACK ===================\n",
        "\n",
        "    def _persistence_forecast(self, seq, target_sensor_index, n_future):\n",
        "        \"\"\"\n",
        "        Persistence fallback for RF evaluation.\n",
        "        Last-value-carried-forward for target sensor.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            seq = np.asarray(seq)\n",
        "            if len(seq) < 2:\n",
        "                return {\n",
        "                    'mse': 9999,\n",
        "                    'mae': 9999,\n",
        "                    'forecast_success': False,\n",
        "                    'error': 'Sequence too short',\n",
        "                    'method': 'Persistence'\n",
        "                }\n",
        "\n",
        "            last_value = seq[-1, target_sensor_index]\n",
        "            predicted_vals = [last_value]\n",
        "            actual = [seq[-1, target_sensor_index]]\n",
        "\n",
        "            mse = 0.0\n",
        "            mae = 0.0\n",
        "\n",
        "            return {\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'forecast_success': True,\n",
        "                'actual_values': actual,\n",
        "                'predicted_values': predicted_vals,\n",
        "                'target_sensor_index': target_sensor_index,\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'mse': 9999,\n",
        "                'mae': 9999,\n",
        "                'forecast_success': False,\n",
        "                'error': str(e),\n",
        "                'method': 'Persistence',\n",
        "                'note': 'persistence_fallback_failed'\n",
        "            }\n",
        "\n",
        "    # =================== PREDICTION PIPELINE ===================\n",
        "\n",
        "    def predict_window_size(self, feature_vector, sequence_3d):\n",
        "        \"\"\"\n",
        "        Main entrypoint:\n",
        "          - Predict window W_t\n",
        "          - Evaluate forecast error e_t\n",
        "          - Compute FDS (S_t) and WSS (Z_t)\n",
        "          - Update drift/anomaly logic (FDI, WDI, events)\n",
        "          - Return full metrics packet\n",
        "        \"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {'predicted_window': 20, 'error': \"Model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            if feature_vector.ndim == 1:\n",
        "                feature_vector = feature_vector.reshape(1, -1)\n",
        "\n",
        "            pred_raw = self.model.predict(feature_vector, verbose=0)\n",
        "\n",
        "            if self.transformer_fitted:\n",
        "                predicted_window = int(round(self.transformer.inverse_transform(pred_raw)[0, 0]))\n",
        "            else:\n",
        "                predicted_window = int(round(pred_raw[0, 0]))\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # WINDOW CLAMP — HARD SAFETY FIX\n",
        "            # ----------------------------------------\n",
        "            # Prevent negative, zero, or extreme window sizes\n",
        "            predicted_window = max(2, predicted_window)        # lower bound\n",
        "\n",
        "            # Forecast evaluation\n",
        "            forecast_metrics = self.evaluate_forecast_performance(sequence_3d, predicted_window, n_future=1)\n",
        "\n",
        "            fds = None\n",
        "            wss = None\n",
        "\n",
        "            if forecast_metrics.get(\"forecast_success\", False):\n",
        "                mse = forecast_metrics[\"mse\"]\n",
        "                mae = forecast_metrics[\"mae\"]\n",
        "\n",
        "                # Basic stats\n",
        "                self.mse_history.append(mse)\n",
        "                self.mae_history.append(mae)\n",
        "                self.error_memory.append(mse)\n",
        "\n",
        "                self.performance_stats['total_predictions'] += 1\n",
        "                self.performance_stats['avg_mse'] = float(np.mean(self.mse_history))\n",
        "                self.performance_stats['avg_mae'] = float(np.mean(self.mae_history))\n",
        "\n",
        "                # ---------- Forecast Deviation Score (FDS) ----------\n",
        "                baseline_median = self.rolling_stats[\"median\"]\n",
        "                baseline_mad = self.rolling_stats[\"mad\"] if self.rolling_stats[\"mad\"] > 0 else 1e-6\n",
        "                fds = (mse - baseline_median) / (baseline_mad + 1e-8)\n",
        "                fds = float(fds) if fds is not None and not np.isnan(fds) else 0.0\n",
        "\n",
        "\n",
        "                self.last_fds = fds\n",
        "                self.fds_memory.append(fds)\n",
        "                self.recent_fds.append(fds)\n",
        "\n",
        "                # ---------- Window Shift Score (WSS) ----------\n",
        "                if self.baseline_windows is not None and self.window_std > 0:\n",
        "                    wss = (predicted_window - self.window_mean) / (self.window_std + 1e-8)\n",
        "                else:\n",
        "                    wss = 0.0\n",
        "\n",
        "                wss = float(wss)\n",
        "                self.last_wss = wss\n",
        "                self.window_memory.append(predicted_window)\n",
        "                self.recent_windows.append(predicted_window)\n",
        "\n",
        "            # Event (ANOMALY / DRIFT) + Drift indices\n",
        "            event, sev, fdi, wdi = self._check_for_event()\n",
        "            self.last_fdi = fdi\n",
        "            self.last_wdi = wdi\n",
        "\n",
        "            # Save history for reporting\n",
        "            record = {\n",
        "                'timestamp': dt.datetime.now(),\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'wss': self.last_wss,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev\n",
        "            }\n",
        "            self.prediction_history.append(record)\n",
        "\n",
        "            return {\n",
        "                'predicted_window': predicted_window,\n",
        "                'forecast_metrics': forecast_metrics,\n",
        "                'fds': self.last_fds,\n",
        "                'fdi': self.last_fdi,\n",
        "                'wss': self.last_wss,\n",
        "                'wdi': self.last_wdi,\n",
        "                'event_type': event,\n",
        "                'severity': sev,\n",
        "                'performance_stats': self.get_recent_performance()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'predicted_window': 20, 'error': str(e)}\n",
        "\n",
        "    # =================== EVENT LOGIC (ANOMALY + DRIFT) ===================\n",
        "\n",
        "    def _check_for_event(self):\n",
        "        \"\"\"\n",
        "        Event detection for the Adaptive Window Agent.\n",
        "\n",
        "        - ANOMALY: deviation of last MSE from baseline (median + k * MAD)\n",
        "        - DRIFT:\n",
        "            * FDI: JSD between recent FDS distribution and baseline FDS\n",
        "            * WDI: JSD between recent window distribution and baseline window distribution\n",
        "        \"\"\"\n",
        "        # Require enough history\n",
        "        if len(self.error_memory) < 30:\n",
        "            return None, 0.0, None, None\n",
        "\n",
        "        last_mse = float(self.error_memory[-1])\n",
        "        live_errors = np.array(self.error_memory)\n",
        "\n",
        "        # ---------- BASELINE STATS ----------\n",
        "        if self.baseline_errors is not None and len(self.baseline_errors) > 10:\n",
        "            base_errors = np.array(self.baseline_errors)\n",
        "            baseline_median = np.median(base_errors)\n",
        "            baseline_mad = np.median(np.abs(base_errors - baseline_median)) + 1e-8\n",
        "        else:\n",
        "            baseline_median = np.median(live_errors)\n",
        "            baseline_mad = np.median(np.abs(live_errors - baseline_median)) + 1e-8\n",
        "\n",
        "        # Update rolling_stats so other components can see latest baseline-ish values\n",
        "        self.rolling_stats[\"median\"] = baseline_median\n",
        "        self.rolling_stats[\"mad\"] = baseline_mad\n",
        "\n",
        "        # ---------- LIVE STATS ----------\n",
        "        live_median = np.median(live_errors)\n",
        "        live_mad = np.median(np.abs(live_errors - live_median)) + 1e-8\n",
        "\n",
        "        # ---------- ANOMALY THRESHOLD ----------\n",
        "        baseline_threshold = baseline_median + self.threshold_k * baseline_mad\n",
        "        live_threshold = live_median + self.threshold_k * live_mad\n",
        "\n",
        "        anomaly_threshold = 0.8 * baseline_threshold + 0.2 * live_threshold\n",
        "\n",
        "        is_anomaly = last_mse > anomaly_threshold\n",
        "\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            is_anomaly = False\n",
        "        elif is_anomaly:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "\n",
        "        if is_anomaly:\n",
        "            severity = (last_mse - anomaly_threshold) / (baseline_mad + 1e-6)\n",
        "            severity = float(severity)\n",
        "            self.performance_stats[\"anomaly_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[ANOMALY] mse={last_mse:.6f}, thr={anomaly_threshold:.6f}, sev={severity:.3f}\")\n",
        "            return \"ANOMALY\", severity, 0.0, 0.0\n",
        "\n",
        "        # ---------- DRIFT (FDI + WDI) ----------\n",
        "        fdi = None\n",
        "        wdi = None\n",
        "\n",
        "        # FDI: JSD over FDS distribution\n",
        "        if self.baseline_fds is not None and len(self.recent_fds) >= 30:\n",
        "            base_fds = np.asarray(self.baseline_fds)\n",
        "            recent_fds = np.asarray(self.recent_fds)\n",
        "\n",
        "            hist_base, bins = np.histogram(base_fds, bins=25, density=True)\n",
        "            hist_recent, _ = np.histogram(recent_fds, bins=bins, density=True)\n",
        "\n",
        "            hist_base = hist_base / (hist_base.sum() + 1e-12)\n",
        "            hist_recent = hist_recent / (hist_recent.sum() + 1e-12)\n",
        "\n",
        "            fdi = float(jensenshannon(hist_base + 1e-12, hist_recent + 1e-12))\n",
        "\n",
        "        # WDI: JSD over window-size distribution\n",
        "        if self.baseline_windows is not None and len(self.recent_windows) >= 30:\n",
        "            base_win = np.asarray(self.baseline_windows)\n",
        "            recent_win = np.asarray(self.recent_windows)\n",
        "\n",
        "            hist_w_base, bins_w = np.histogram(base_win, bins=20, density=True)\n",
        "            hist_w_recent, _ = np.histogram(recent_win, bins=bins_w, density=True)\n",
        "\n",
        "            hist_w_base = hist_w_base / (hist_w_base.sum() + 1e-12)\n",
        "            hist_w_recent = hist_w_recent / (hist_w_recent.sum() + 1e-12)\n",
        "\n",
        "            wdi = float(jensenshannon(hist_w_base + 1e-12, hist_w_recent + 1e-12))\n",
        "\n",
        "        # Decide drift if either index is high\n",
        "        is_drift_fdi = fdi is not None and fdi > self.drift_threshold\n",
        "        is_drift_wdi = wdi is not None and wdi > self.window_drift_threshold\n",
        "\n",
        "        is_drift = is_drift_fdi or is_drift_wdi\n",
        "\n",
        "        if self.drift_cooldown > 0:\n",
        "            self.drift_cooldown -= 1\n",
        "            is_drift = False\n",
        "        else:\n",
        "            if is_drift:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "        if self.consecutive_drift_votes >= self.drift_votes_required:\n",
        "            self.consecutive_drift_votes = 0\n",
        "            self.drift_cooldown = self.drift_cooldown_steps\n",
        "            self.performance_stats[\"drift_events\"] += 1\n",
        "            if self.debug:\n",
        "                print(f\"[DRIFT] FDI={fdi:.4f} WDI={wdi:.4f}\")\n",
        "            fdi = float(fdi) if fdi is not None else 0.0\n",
        "            wdi = float(wdi) if wdi is not None else 0.0\n",
        "            return \"DRIFT\", fdi, fdi, wdi\n",
        "\n",
        "        # Make safe for printing\n",
        "        fdi = float(fdi) if fdi is not None else 0.0\n",
        "        wdi = float(wdi) if wdi is not None else 0.0\n",
        "\n",
        "        return None, 0.0, fdi, wdi\n",
        "\n",
        "\n",
        "    # =================== HELPERS ===================\n",
        "\n",
        "    def get_recent_performance(self):\n",
        "        all_preds = list(self.prediction_history)\n",
        "\n",
        "        successful_predictions = [\n",
        "            p for p in all_preds\n",
        "            if p.get('forecast_metrics', {}).get('forecast_success', False)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'total_predictions': len(all_preds),\n",
        "            'successful_predictions': len(successful_predictions),\n",
        "            'success_rate': len(successful_predictions) / max(len(all_preds), 1),\n",
        "            'drift_events': self.performance_stats['drift_events'],\n",
        "            'anomaly_events': self.performance_stats['anomaly_events'],\n",
        "            'retraining_events': self.performance_stats['retraining_events'],\n",
        "            'recent_mse': float(np.mean(list(self.mse_history)[-10:])) if self.mse_history else 0,\n",
        "            'avg_mse': float(np.mean(self.mse_history)) if self.mse_history else 0,\n",
        "            'recent_mae': float(np.mean(list(self.mae_history)[-10:])) if self.mae_history else 0,\n",
        "            'avg_mae': float(np.mean(self.mae_history)) if self.mae_history else 0,\n",
        "            'transformer_fitted': self.transformer_fitted,\n",
        "            'last_fdi': self.last_fdi,\n",
        "            'last_wdi': self.last_wdi,\n",
        "        }\n",
        "\n",
        "    def save_performance_state(self, filepath: str):\n",
        "        \"\"\"Save performance statistics + prediction history to JSON\"\"\"\n",
        "        try:\n",
        "            state = {\n",
        "                'performance_stats': self.performance_stats.copy(),\n",
        "                'prediction_history': list(self.prediction_history)[-100:],\n",
        "                'mse_history': list(self.mse_history),\n",
        "                'mae_history': list(self.mae_history),\n",
        "                'transformer_fitted': self.transformer_fitted\n",
        "            }\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(state, f, indent=2, default=str)\n",
        "            print(f\"✅ Performance state saved to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save performance state: {e}\")\n",
        "\n",
        "\n",
        "#===============================================================================================================================================\n",
        "###  SENSOR AGENTS - INDIVIDUAL AND MASTER\n",
        "#--------------------------------------==========================================================================================================\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# Deep learning\n",
        "try:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST SENSOR AGENT - Observes ONE sensor with AE model\n",
        "# =====================================================\n",
        "\n",
        "class RobustSensorAgent:\n",
        "    \"\"\"\n",
        "    Robust Sensor Agent for ONE sensor with advanced anomaly & drift detection.\n",
        "\n",
        "    Loads pretrained AE model + metadata (scaler, baseline errors, rolling stats).\n",
        "    Computes anomaly score via reconstruction error, applies adaptive thresholding,\n",
        "    drift detection, and outputs robust anomaly/drift/retrain flags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sensor_id: int,\n",
        "                 model_path: str = None,\n",
        "                 window_length: int = 10, #K\n",
        "                 memory_size: int = 1000,\n",
        "                 threshold_k: float = 2.0,\n",
        "                 drift_threshold: float = 0.1,\n",
        "                warmup_steps: int = 100):    # <── NEW PARAM\n",
        "\n",
        "        self.sensor_id = sensor_id\n",
        "        self.window_length = window_length\n",
        "        self.threshold_k = threshold_k\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "        # Model & metadata\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Buffers\n",
        "        self.error_memory = deque(maxlen=memory_size)\n",
        "        self.data_memory = deque(maxlen=memory_size)\n",
        "        self.recent_errors = deque(maxlen=100)\n",
        "\n",
        "        # Rolling stats\n",
        "        self.rolling_stats = {\n",
        "            'median': 0.0,\n",
        "            'mad': 1.0,\n",
        "            'mean': 0.0,   # backward compatibility\n",
        "            'std': 1.0,    # backward compatibility\n",
        "            'q95': 0.0,\n",
        "            'q99': 0.0\n",
        "        }\n",
        "        self.baseline_errors = None\n",
        "\n",
        "        # Counters\n",
        "        self.total_processed = 0\n",
        "        self.anomalies_detected = 0\n",
        "        self.drift_detected_count = 0\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "        self.anomaly_cooldown = 0\n",
        "        self.drift_cooldown = 0\n",
        "\n",
        "        self.anomaly_cooldown_steps = 5    # you can tune\n",
        "        self.drift_cooldown_steps = 10     # you can tune\n",
        "\n",
        "        self.consecutive_drift_votes = 0\n",
        "        self.consecutive_anomaly_votes = 0\n",
        "\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path: str) -> bool:\n",
        "        \"\"\"Load pretrained AE model + metadata.\"\"\"\n",
        "        try:\n",
        "            if KERAS_AVAILABLE and model_path.endswith('.h5'):\n",
        "                self.model = load_model(model_path, compile=False)\n",
        "\n",
        "                # Correct metadata file\n",
        "                metadata_path = model_path.replace('_model.h5', '_metadata.pkl')\n",
        "\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'rb') as f:\n",
        "                        metadata = pickle.load(f)\n",
        "\n",
        "                    baseline = metadata.get('baseline_stats', None)\n",
        "\n",
        "                    if baseline is not None:\n",
        "                        # Initialize rolling stats from training\n",
        "                        # Load robust baseline stats\n",
        "                        self.rolling_stats['median'] = baseline.get('median')\n",
        "                        self.rolling_stats['mad']    = baseline.get('mad')\n",
        "\n",
        "                        # Backward compatibility for other parts of system\n",
        "                        self.rolling_stats['mean'] = self.rolling_stats['median']\n",
        "                        self.rolling_stats['std']  = self.rolling_stats['mad']\n",
        "\n",
        "                        self.rolling_stats['q95']  = baseline['q95']\n",
        "                        self.rolling_stats['q99']  = baseline['q99']\n",
        "\n",
        "                        # Save baseline distribution for drift detection\n",
        "                        self.baseline_errors = np.array(baseline['baseline_errors'])\n",
        "\n",
        "                # AE was trained on raw, NOT scaled\n",
        "                self.scaler = None\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model format – expecting .h5 AE model\")\n",
        "\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"✅ AE model loaded for sensor {self.sensor_id}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load AE model for sensor {self.sensor_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def observe(self, sensor_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Observe subsequence [window_length] and return anomaly/drift flags.\"\"\"\n",
        "        if not self.is_model_loaded:\n",
        "            return {\"sensor_id\": self.sensor_id, \"error\": \"no_model_loaded\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        if len(sensor_subsequence) != self.window_length:\n",
        "            return {\"sensor_id\": self.sensor_id,\n",
        "                    \"error\": f\"invalid_length_expected_{self.window_length}_got_{len(sensor_subsequence)}\",\n",
        "                    \"timestamp\": datetime.now()}\n",
        "\n",
        "        # 1. Anomaly score\n",
        "        anomaly_score = self._compute_robust_anomaly_score(sensor_subsequence)\n",
        "\n",
        "        # 2. Update memory\n",
        "        self.data_memory.append(sensor_subsequence.copy())\n",
        "        self.error_memory.append(anomaly_score)\n",
        "        self.recent_errors.append(anomaly_score)\n",
        "\n",
        "        # 3\n",
        "\n",
        "        # --------------- WARM-UP PHASE -----------------\n",
        "        # During warm-up, rolling stats ignore live data and stay fixed\n",
        "        if self.total_processed < self.warmup_steps:\n",
        "            med = np.median(self.baseline_errors)\n",
        "            mad = np.median(np.abs(self.baseline_errors - med)) + 1e-8\n",
        "\n",
        "            self.rolling_stats['median'] = med\n",
        "            self.rolling_stats['mad'] = mad\n",
        "            self.rolling_stats['mean'] = med     # backward compatibility\n",
        "            self.rolling_stats['std'] = mad\n",
        "        else:\n",
        "            # After warm-up, rolling stats evolve normally\n",
        "            if len(self.error_memory) >= 50 and len(self.error_memory) % 10 == 0:\n",
        "                self._update_rolling_stats(list(self.error_memory)[-50:])\n",
        "\n",
        "        # 4. Flags\n",
        "        is_anomaly = self._check_adaptive_anomaly(anomaly_score)\n",
        "        drift_flag = self._check_advanced_drift()\n",
        "        needs_retrain = self._check_retrain_need()\n",
        "        confidence = self._compute_robust_confidence(anomaly_score)\n",
        "\n",
        "        # 5. Update counters\n",
        "        self.total_processed += 1\n",
        "        if is_anomaly: self.anomalies_detected += 1\n",
        "        if drift_flag: self.drift_detected_count += 1\n",
        "\n",
        "        return {\n",
        "            \"sensor_id\": self.sensor_id,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"is_anomaly\": bool(is_anomaly),\n",
        "            \"drift_flag\": bool(drift_flag),\n",
        "            \"needs_retrain_flag\": bool(needs_retrain),\n",
        "            \"anomaly_score\": float(anomaly_score),\n",
        "            \"confidence\": float(confidence),\n",
        "            \"threshold_used\": float(self.rolling_stats['median'] + self.threshold_k * self.rolling_stats['mad']),\n",
        "            \"anomaly_rate\": self.anomalies_detected / max(1, self.total_processed),\n",
        "            \"drift_rate\": self.drift_detected_count / max(1, self.total_processed)\n",
        "        }\n",
        "\n",
        "    def _compute_robust_anomaly_score(self, subsequence: np.ndarray) -> float:\n",
        "        \"\"\"Compute reconstruction error using AE model on RAW values.\"\"\"\n",
        "        try:\n",
        "            # Ensure shape: [1, window_length, 1]\n",
        "            X = subsequence.reshape(1, self.window_length, 1)\n",
        "            reconstruction = self.model.predict(X, verbose=0)\n",
        "\n",
        "            error = mean_squared_error(\n",
        "                subsequence.flatten(),\n",
        "                reconstruction.flatten()\n",
        "            )\n",
        "            return max(0.0, error)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ AE inference failed for sensor {self.sensor_id}: {e}\")\n",
        "            # Fallback: variance of raw subsequence\n",
        "            return float(np.var(subsequence))\n",
        "\n",
        "    def _update_rolling_stats(self, errors: List[float]):\n",
        "        errors_array = np.array(errors)\n",
        "\n",
        "        median = np.median(errors_array)\n",
        "        mad = np.median(np.abs(errors_array - median)) + 1e-8  # avoid zero\n",
        "\n",
        "        # Store\n",
        "        self.rolling_stats['median'] = median\n",
        "        self.rolling_stats['mad'] = mad\n",
        "\n",
        "        # Backward compatibility fields (for plotting)\n",
        "        self.rolling_stats['mean'] = median\n",
        "        self.rolling_stats['std'] = mad\n",
        "\n",
        "        # Percentile bands (unchanged; good for drift & visualization)\n",
        "        self.rolling_stats['q95'] = np.percentile(errors_array, 95)\n",
        "        self.rolling_stats['q99'] = np.percentile(errors_array, 99)\n",
        "\n",
        "        self.last_stats_update = datetime.now()\n",
        "\n",
        "    def _check_adaptive_anomaly(self, score: float) -> bool:\n",
        "        median = self.rolling_stats.get('median', self.rolling_stats['mean'])\n",
        "        mad = self.rolling_stats.get('mad', self.rolling_stats['std'])\n",
        "        threshold = median + self.threshold_k * mad\n",
        "        is_anomaly_now = score > threshold\n",
        "\n",
        "        # Cooldown active → suppress anomaly\n",
        "        if self.anomaly_cooldown > 0:\n",
        "            self.anomaly_cooldown -= 1\n",
        "            return False\n",
        "\n",
        "        # No cooldown and anomaly happened → activate cooldown\n",
        "        if is_anomaly_now:\n",
        "            self.anomaly_cooldown = self.anomaly_cooldown_steps\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _check_advanced_drift(self) -> bool:\n",
        "        if self.baseline_errors is None or len(self.recent_errors) < 30:\n",
        "            return False\n",
        "        try:\n",
        "            hist_baseline, bins = np.histogram(self.baseline_errors, bins=20, density=True)\n",
        "            hist_recent, _ = np.histogram(list(self.recent_errors), bins=bins, density=True)\n",
        "            hist_baseline += 1e-10; hist_recent += 1e-10\n",
        "            hist_baseline /= hist_baseline.sum(); hist_recent /= hist_recent.sum()\n",
        "            js_divergence = jensenshannon(hist_baseline, hist_recent)\n",
        "            is_drift_now = js_divergence > self.drift_threshold\n",
        "\n",
        "            # Cooldown active → suppress\n",
        "            if self.drift_cooldown > 0:\n",
        "                self.drift_cooldown -= 1\n",
        "                return False\n",
        "\n",
        "            # Multi-step confirmation: require 3 drift votes in last few steps\n",
        "            if is_drift_now:\n",
        "                self.consecutive_drift_votes += 1\n",
        "            else:\n",
        "                self.consecutive_drift_votes = 0\n",
        "\n",
        "            if self.consecutive_drift_votes >= 3:\n",
        "                self.drift_cooldown = self.drift_cooldown_steps\n",
        "                self.consecutive_drift_votes = 0\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                _, p_value = stats.ks_2samp(self.baseline_errors, list(self.recent_errors))\n",
        "                return p_value < 0.05\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "    def _check_retrain_need(self) -> bool:\n",
        "        if len(self.error_memory) < 100: return False\n",
        "        recent_errors = list(self.error_memory)[-50:]\n",
        "        threshold = self.rolling_stats['mean'] + self.threshold_k * self.rolling_stats['std']\n",
        "        anomaly_rate = sum(1 for e in recent_errors if e > threshold) / len(recent_errors)\n",
        "        criteria = [\n",
        "            anomaly_rate > 0.3,\n",
        "            self.drift_detected_count > 0.1 * self.total_processed,\n",
        "            np.mean(recent_errors) > 2.0 * self.rolling_stats['mean'] if len(recent_errors) > 0 else False,\n",
        "            (datetime.now() - self.last_stats_update).days > 7\n",
        "        ]\n",
        "        return sum(criteria) >= 2\n",
        "\n",
        "    def _compute_robust_confidence(self, score: float) -> float:\n",
        "        median = self.rolling_stats.get('median')\n",
        "        mad = self.rolling_stats.get('mad')\n",
        "\n",
        "        if mad == 0:\n",
        "            return 0.5\n",
        "\n",
        "        threshold = median + self.threshold_k * mad\n",
        "\n",
        "        z = (score - threshold) / mad  # how far beyond threshold?\n",
        "\n",
        "        # Smooth probability-like mapping\n",
        "        confidence = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        return float(np.clip(confidence, 0.0, 1.0))\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ROBUST MASTER AGENT\n",
        "# =====================================================\n",
        "\n",
        "class RobustMasterAgent:\n",
        "    \"\"\"Aggregates sensor results, makes system-level anomaly/drift/retrain decisions.\"\"\"\n",
        "    def __init__(self, sensor_agents: List[RobustSensorAgent],\n",
        "                 system_anomaly_threshold: float = 0.3,\n",
        "                 drift_threshold: float = 0.2,\n",
        "                 retrain_threshold: float = 0.15):\n",
        "        self.sensor_agents = sensor_agents\n",
        "        self.num_sensors = len(sensor_agents)\n",
        "        self.system_anomaly_threshold = system_anomaly_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "        self.retrain_threshold = retrain_threshold\n",
        "\n",
        "    def process_system_input(self, system_subsequence: np.ndarray) -> Dict:\n",
        "        \"\"\"Process [window_length, num_sensors] multivariate subsequence.\"\"\"\n",
        "        timestamp = datetime.now()\n",
        "        if system_subsequence.shape[1] != self.num_sensors:\n",
        "            return {\"error\": f\"Expected {self.num_sensors} sensors, got {system_subsequence.shape[1]}\",\n",
        "                    \"timestamp\": timestamp}\n",
        "\n",
        "        # 1. Collect sensor observations\n",
        "        sensor_results = []\n",
        "        for i, agent in enumerate(self.sensor_agents):\n",
        "            sensor_data = system_subsequence[:, i]\n",
        "            result = agent.observe(sensor_data)\n",
        "            sensor_results.append(result)\n",
        "\n",
        "        # 2. Simple aggregation\n",
        "        anomalies = sum(1 for r in sensor_results if r.get(\"is_anomaly\"))\n",
        "        drifts = sum(1 for r in sensor_results if r.get(\"drift_flag\"))\n",
        "        retrains = sum(1 for r in sensor_results if r.get(\"needs_retrain_flag\"))\n",
        "\n",
        "        anomaly_rate = anomalies / max(1, self.num_sensors)\n",
        "        drift_rate = drifts / max(1, self.num_sensors)\n",
        "        retrain_rate = retrains / max(1, self.num_sensors)\n",
        "\n",
        "        system_decisions = {\n",
        "            \"system_anomaly\": anomaly_rate >= self.system_anomaly_threshold,\n",
        "            \"system_drift\": drift_rate >= self.drift_threshold,\n",
        "            \"system_needs_retrain\": retrain_rate >= self.retrain_threshold,\n",
        "            \"anomaly_rate\": anomaly_rate,\n",
        "            \"drift_rate\": drift_rate,\n",
        "            \"retrain_rate\": retrain_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sensor_results\": sensor_results,\n",
        "            \"system_decisions\": system_decisions\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SYSTEM CREATION\n",
        "# =====================================================\n",
        "\n",
        "def create_robust_system(num_sensors: int, models_dir: str, win_length: int, warmup_steps: int = 100) -> Tuple[List[RobustSensorAgent], RobustMasterAgent]:\n",
        "    \"\"\"Create robust sensor system loading AE models + metadata.\"\"\"\n",
        "    print(f\"🚀 Creating robust system with {num_sensors} sensors\")\n",
        "    sensor_agents = []\n",
        "    for sensor_id in range(num_sensors):\n",
        "        model_path = os.path.join(models_dir, f\"sensor_{sensor_id}_model.h5\")\n",
        "        agent = RobustSensorAgent(sensor_id=sensor_id,\n",
        "                                  model_path=model_path if os.path.exists(model_path) else None,\n",
        "                                  window_length=win_length,\n",
        "                                  memory_size=1000,\n",
        "                                  threshold_k=2.0,\n",
        "                                  drift_threshold=0.1,\n",
        "                                  warmup_steps=warmup_steps)       # <── NEW\n",
        "        sensor_agents.append(agent)\n",
        "\n",
        "    master = RobustMasterAgent(sensor_agents=sensor_agents,\n",
        "                               system_anomaly_threshold=0.3,\n",
        "                               drift_threshold=0.2,\n",
        "                               retrain_threshold=0.15)\n",
        "    print(f\"✅ Created system: {len([a for a in sensor_agents if a.is_model_loaded])}/{num_sensors} models loaded\")\n",
        "\n",
        "    return sensor_agents, master\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# DECISION AGENT V4 (Comprehensive fusion) — FIXED\n",
        "# =====================================================\n",
        "from typing import Optional, Dict, Any, List\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "class DecisionAgent:\n",
        "    \"\"\"\n",
        "    DecisionAgent V4 (surgical refactor):\n",
        "      - Keeps your sensor + window perception aggregation\n",
        "      - Makes WARNING (class-1 / 2h ahead) a first-class output: final_warning\n",
        "      - WARNING baseline = transformer class-1 probability (p_warn)\n",
        "      - WARNING refinement = boost/penalty based on explicit 'contradiction' score\n",
        "      - Prioritises window drift/anomaly (WDI/WSS) over forecast drift/anomaly (FDI/FDS)\n",
        "      - Keeps operational alert mapping and retrain logic\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "\n",
        "        # ---------- weights for detection ----------\n",
        "        w_sensor: float = 0.60,\n",
        "        w_window: float = 0.40,\n",
        "\n",
        "        # ---------- drift weights (WINDOW drift dominant) ----------\n",
        "        w_wdi: float = 0.60,          # ↑ window drift index (dominant)\n",
        "        w_sensor_drift: float = 0.30, # sensor drift\n",
        "        w_fdi: float = 0.10,          # ↓ forecast drift index (minor)\n",
        "\n",
        "        # ---------- failure thresholds ----------\n",
        "        failure_threshold: float = 0.50,\n",
        "        failure_critical_threshold: float = 0.80,\n",
        "\n",
        "        detection_threshold: float = 0.50,\n",
        "        drift_threshold: float = 0.35,\n",
        "\n",
        "        # ---------- warning refinement ----------\n",
        "        warn_threshold: float = 0.50,           # final_warning cutoff\n",
        "        #warn_gate_low_context: float = 0.20,    # \"quiet context\"\n",
        "        warn_gate_high_contra: float = 0.65,    # \"strong contradiction\"\n",
        "        warn_boost: float = 0.15,               # max boost\n",
        "        warn_penalty: float = 0.25,             # max penalty\n",
        "\n",
        "        # ---------- alert mapping ----------\n",
        "        alert_low: float = 0.35,\n",
        "        alert_med: float = 0.55,\n",
        "        alert_high: float = 0.75,\n",
        "\n",
        "        # ---------- sensor aggregation ----------\n",
        "        topk_sensors: int = 5,\n",
        "        use_confidence: bool = True,\n",
        "    ):\n",
        "        self.w_sensor = w_sensor\n",
        "        self.w_window = w_window\n",
        "\n",
        "        self.w_wdi = w_wdi\n",
        "        self.w_sensor_drift = w_sensor_drift\n",
        "        self.w_fdi = w_fdi\n",
        "\n",
        "        self.failure_threshold = failure_threshold\n",
        "        self.failure_critical_threshold = failure_critical_threshold\n",
        "\n",
        "        self.detection_threshold = detection_threshold\n",
        "        self.drift_threshold = drift_threshold\n",
        "\n",
        "        self.warn_threshold = warn_threshold\n",
        "        #self.warn_gate_low_context = warn_gate_low_context\n",
        "        self.warn_gate_high_contra = warn_gate_high_contra\n",
        "        self.warn_boost = warn_boost\n",
        "        self.warn_penalty = warn_penalty\n",
        "\n",
        "        self.alert_low = alert_low\n",
        "        self.alert_med = alert_med\n",
        "        self.alert_high = alert_high\n",
        "\n",
        "        self.topk_sensors = topk_sensors\n",
        "        self.use_confidence = use_confidence\n",
        "\n",
        "        self.history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _clip01(self, x):\n",
        "        try:\n",
        "            return float(np.clip(float(x), 0.0, 1.0))\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    # ------------\n",
        "    # ---------------------------\n",
        "    # Sensor aggregation\n",
        "    # ---------------------------\n",
        "    def _sensor_intensity(self, master_output: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        if not master_output:\n",
        "            return {\n",
        "                \"sensor_anom_intensity\": 0.0,\n",
        "                \"sensor_drift_intensity\": 0.0,\n",
        "                \"sensor_retrain_intensity\": 0.0,\n",
        "                \"top_sensors\": [],\n",
        "                \"rates\": {\"anomaly_rate\": 0.0, \"drift_rate\": 0.0, \"retrain_rate\": 0.0}\n",
        "            }\n",
        "\n",
        "        sys_dec = master_output.get(\"system_decisions\", {}) or {}\n",
        "        rates = {\n",
        "            \"anomaly_rate\": float(sys_dec.get(\"anomaly_rate\", 0.0)),\n",
        "            \"drift_rate\": float(sys_dec.get(\"drift_rate\", 0.0)),\n",
        "            \"retrain_rate\": float(sys_dec.get(\"retrain_rate\", 0.0)),\n",
        "        }\n",
        "\n",
        "        sensor_results = master_output.get(\"sensor_results\", []) or []\n",
        "\n",
        "        strengths = []\n",
        "        for r in sensor_results:\n",
        "            score = float(r.get(\"anomaly_score\", 0.0))\n",
        "            conf  = float(r.get(\"confidence\", 0.5))\n",
        "            is_an = 1.0 if r.get(\"is_anomaly\", False) else 0.0\n",
        "            dr    = 1.0 if r.get(\"drift_flag\", False) else 0.0\n",
        "            rt    = 1.0 if r.get(\"needs_retrain_flag\", False) else 0.0\n",
        "\n",
        "            base = score * (conf if self.use_confidence else 1.0)\n",
        "            strength = base + 0.25 * is_an\n",
        "\n",
        "            strengths.append({\n",
        "                \"sensor_id\": r.get(\"sensor_id\"),\n",
        "                \"strength\": float(strength),\n",
        "                \"anomaly_score\": score,\n",
        "                \"confidence\": conf,\n",
        "                \"is_anomaly\": bool(is_an),\n",
        "                \"drift_flag\": bool(dr),\n",
        "                \"needs_retrain_flag\": bool(rt),\n",
        "            })\n",
        "\n",
        "        strengths_sorted = sorted(strengths, key=lambda x: x[\"strength\"], reverse=True)\n",
        "        top = strengths_sorted[: max(1, self.topk_sensors)]\n",
        "\n",
        "        vals = np.array([t[\"strength\"] for t in top], dtype=float)\n",
        "        top_strength_mean = float(1.0 - np.exp(-np.mean(np.maximum(vals, 0.0)))) if len(vals) else 0.0\n",
        "\n",
        "        drift_flags = np.array([1.0 if t[\"drift_flag\"] else 0.0 for t in strengths_sorted], dtype=float)\n",
        "        drift_vote_rate = float(drift_flags.mean()) if len(drift_flags) else 0.0\n",
        "\n",
        "        retrain_flags = np.array([1.0 if t[\"needs_retrain_flag\"] else 0.0 for t in strengths_sorted], dtype=float)\n",
        "        retrain_vote_rate = float(retrain_flags.mean()) if len(retrain_flags) else 0.0\n",
        "\n",
        "        return {\n",
        "            \"sensor_anom_intensity\": self._clip01(0.5 * rates[\"anomaly_rate\"] + 0.5 * top_strength_mean),\n",
        "            \"sensor_drift_intensity\": self._clip01(0.6 * rates[\"drift_rate\"] + 0.4 * drift_vote_rate),\n",
        "            \"sensor_retrain_intensity\": self._clip01(0.6 * rates[\"retrain_rate\"] + 0.4 * retrain_vote_rate),\n",
        "            \"top_sensors\": top,\n",
        "            \"rates\": rates\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Window aggregation (FIXED: now inside class)\n",
        "    # ---------------------------\n",
        "    def _window_intensity(self, window_output: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        if not window_output:\n",
        "            return {\n",
        "                \"win_anom_intensity\": 0.0,\n",
        "                \"win_drift_intensity\": 0.0,\n",
        "                \"fds\": 0.0, \"fdi\": 0.0, \"wss\": 0.0, \"wdi\": 0.0,\n",
        "                \"event_type\": None,\n",
        "                \"severity\": 0.0,\n",
        "                \"window_mse\": None,\n",
        "                \"predicted_window\": None\n",
        "            }\n",
        "\n",
        "        event_type = window_output.get(\"event_type\")\n",
        "        severity = float(window_output.get(\"severity\", 0.0) or 0.0)\n",
        "\n",
        "        fds = float(window_output.get(\"fds\", 0.0) or 0.0)\n",
        "        fdi = float(window_output.get(\"fdi\", 0.0) or 0.0)\n",
        "        wss = float(window_output.get(\"wss\", 0.0) or 0.0)\n",
        "        wdi = float(window_output.get(\"wdi\", 0.0) or 0.0)\n",
        "\n",
        "        predicted_window = window_output.get(\"predicted_window\")\n",
        "\n",
        "        window_mse = None\n",
        "        if window_output.get(\"forecast_metrics\"):\n",
        "            window_mse = window_output[\"forecast_metrics\"].get(\"mse\")\n",
        "\n",
        "        wss_int = 1.0 / (1.0 + np.exp(-abs(wss)))\n",
        "        fds_int = 1.0 / (1.0 + np.exp(-fds))\n",
        "        sev_int = 1.0 - np.exp(-max(severity, 0.0))\n",
        "\n",
        "        win_anom = self._clip01(\n",
        "            0.55 * wss_int +\n",
        "            0.30 * sev_int +\n",
        "            0.10 * fds_int +\n",
        "            (0.05 if event_type == \"ANOMALY\" else 0.0)\n",
        "        )\n",
        "\n",
        "        win_drift = self._clip01(\n",
        "            0.80 * wdi +\n",
        "            0.15 * fdi +\n",
        "            (0.10 if event_type == \"DRIFT\" else 0.0)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"win_anom_intensity\": float(win_anom),\n",
        "            \"win_drift_intensity\": float(win_drift),\n",
        "            \"fds\": fds,\n",
        "            \"fdi\": fdi,\n",
        "            \"wss\": wss,\n",
        "            \"wdi\": wdi,\n",
        "            \"event_type\": event_type,\n",
        "            \"severity\": severity,\n",
        "            \"window_mse\": float(window_mse) if window_mse is not None else None,\n",
        "            \"predicted_window\": predicted_window,\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # ML prediction aggregation\n",
        "    # ---------------------------\n",
        "    def _prediction_probs(self, model_outputs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        if not model_outputs:\n",
        "            return {\"p_fault\": 0.0, \"p_warn\": 0.0, \"p_normal\": 1.0}\n",
        "\n",
        "        # Your holdout loop uses:\n",
        "        # - failure_prob := probs[2] (class 2)\n",
        "        # - transformer_prob := probs[1] (class 1)\n",
        "        p_fault = float(model_outputs.get(\"failure_prob\", 0.0) or 0.0)\n",
        "        p_warn  = float(model_outputs.get(\"transformer_prob\", 0.0) or 0.0)\n",
        "        p_norm  = float(model_outputs.get(\"p_normal\", max(0.0, 1.0 - p_fault - p_warn)))\n",
        "\n",
        "        return {\n",
        "            \"p_fault\": self._clip01(p_fault),\n",
        "            \"p_warn\": self._clip01(p_warn),\n",
        "            \"p_normal\": self._clip01(p_norm),\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Warning Refinement - with transformer prediction as baseline - deterministic and traceable\n",
        "    # ---------------------------\n",
        "\n",
        "    def _warning_refine(\n",
        "        self,\n",
        "        p_warn: float,\n",
        "        detection_risk: float,\n",
        "        drift_risk: float,\n",
        "        sens: Dict[str, Any],\n",
        "        win: Dict[str, Any],\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Traceable rule-based refinement:\n",
        "        - baseline = p_warn\n",
        "        - boost when context supports early fault\n",
        "        - penalize only when context strongly contradicts\n",
        "        \"\"\"\n",
        "        p_warn = self._clip01(p_warn)\n",
        "\n",
        "        # Support signals: window drift is the strongest supporting evidence (your requirement)\n",
        "        support = self._clip01(\n",
        "            0.55 * self._clip01(win.get(\"win_drift_intensity\", 0.0)) +\n",
        "            0.25 * self._clip01(sens.get(\"sensor_anom_intensity\", 0.0)) +\n",
        "            0.20 * self._clip01(detection_risk)\n",
        "        )\n",
        "\n",
        "        # Contradiction: system looks very normal (low detection + low drift + low sensor anomalies)\n",
        "        contradiction = self._clip01(\n",
        "            0.45 * (1.0 - self._clip01(detection_risk)) +\n",
        "            0.35 * (1.0 - self._clip01(drift_risk)) +\n",
        "            0.20 * (1.0 - self._clip01(sens.get(\"sensor_anom_intensity\", 0.0)))\n",
        "        )\n",
        "\n",
        "        # Traceable gates\n",
        "        gates = {\n",
        "            \"boost_gate\": bool(support >= 0.55),\n",
        "            \"penalty_gate\": bool(contradiction >= self.warn_gate_high_contra),\n",
        "        }\n",
        "\n",
        "        warn_risk = p_warn\n",
        "\n",
        "        # Boost only if transformer already suspects warning OR context is strongly supportive\n",
        "        if gates[\"boost_gate\"] and p_warn >= 0.40:\n",
        "            warn_risk = self._clip01(warn_risk + self.warn_boost * support)\n",
        "\n",
        "        # Penalize only if transformer is weak AND contradiction is strong\n",
        "        if gates[\"penalty_gate\"] and p_warn <= 0.60:\n",
        "            warn_risk = self._clip01(warn_risk - self.warn_penalty * contradiction)\n",
        "\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"p_warn_base\": p_warn,\n",
        "            \"support\": float(support),\n",
        "            \"contradiction\": float(contradiction),\n",
        "            \"p_warn_refined\": float(warn_risk),\n",
        "            \"gates\": gates\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Main entrypoint\n",
        "    # ---------------------------\n",
        "    def decide(\n",
        "        self,\n",
        "        master_output: Dict[str, Any],\n",
        "        window_output: Dict[str, Any],\n",
        "        model_outputs: Optional[Dict[str, Any]] = None,\n",
        "        metadata: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "\n",
        "        ts = datetime.now().isoformat()\n",
        "\n",
        "        sens = self._sensor_intensity(master_output)\n",
        "        win  = self._window_intensity(window_output)\n",
        "        pred = self._prediction_probs(model_outputs)\n",
        "\n",
        "\n",
        "        detection_risk = self._clip01(\n",
        "            self.w_sensor * sens[\"sensor_anom_intensity\"] +\n",
        "            self.w_window * win[\"win_anom_intensity\"]\n",
        "        )\n",
        "\n",
        "        drift_risk = self._clip01(\n",
        "            self.w_fdi * self._clip01(win[\"fdi\"]) +\n",
        "            self.w_wdi * self._clip01(win[\"wdi\"]) +\n",
        "            self.w_sensor_drift * sens[\"sensor_drift_intensity\"]\n",
        "        )\n",
        "\n",
        "        # Failure: class-2 is a direct signal; allow context upgrade\n",
        "        base_failure = pred[\"p_fault\"] >= self.failure_threshold\n",
        "        support_upgrade = (\n",
        "            pred[\"p_fault\"] >= 0.35 and\n",
        "            (detection_risk >= 0.55 or drift_risk >= 0.50)\n",
        "        )\n",
        "        final_failure = bool(base_failure or support_upgrade)\n",
        "\n",
        "        # WARNING refinement: class-1 only (your 2-hour ahead target)\n",
        "        warn_ref = self._warning_refine(\n",
        "            p_warn=pred[\"p_warn\"],\n",
        "            detection_risk=detection_risk,\n",
        "            drift_risk=drift_risk,\n",
        "            sens=sens,\n",
        "            win=win,\n",
        "        )\n",
        "\n",
        "      # If it is already failure, don't label it as warning\n",
        "        final_warning = bool((warn_ref[\"p_warn_refined\"] >= self.warn_threshold) and (not final_failure))\n",
        "\n",
        "        final_anomaly = detection_risk >= self.detection_threshold\n",
        "        final_drift   = drift_risk >= self.drift_threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        final_retrain = bool(\n",
        "            sens[\"sensor_retrain_intensity\"] >= 0.35 or\n",
        "            (final_drift and sens[\"rates\"][\"anomaly_rate\"] >= 0.15)\n",
        "        )\n",
        "\n",
        "        # Operational score: keep your prior pattern\n",
        "        if final_failure:\n",
        "            operational_score = self._clip01(\n",
        "                0.70 * pred[\"p_fault\"] + 0.20 * detection_risk + 0.10 * drift_risk\n",
        "            )\n",
        "        else:\n",
        "            # warning should affect ops score a bit (but not dominate)\n",
        "            operational_score = self._clip01(\n",
        "                0.35 * pred[\"p_fault\"] +\n",
        "                0.25 * warn_ref[\"p_warn_refined\"] +\n",
        "                0.25 * detection_risk +\n",
        "                0.15 * drift_risk\n",
        "            )\n",
        "\n",
        "        if pred[\"p_fault\"] >= self.failure_critical_threshold and (detection_risk >= 0.4 or drift_risk >= 0.4):\n",
        "            alert_level = \"CRITICAL\"\n",
        "        else:\n",
        "            if operational_score < self.alert_low:\n",
        "                alert_level = \"NORMAL\"\n",
        "            elif operational_score < self.alert_med:\n",
        "                alert_level = \"LOW\"\n",
        "            elif operational_score < self.alert_high:\n",
        "                alert_level = \"MEDIUM\"\n",
        "            elif operational_score < 0.92:\n",
        "                alert_level = \"HIGH\"\n",
        "            else:\n",
        "                alert_level = \"CRITICAL\"\n",
        "\n",
        "        packet = {\n",
        "            \"timestamp\": ts,\n",
        "\n",
        "            # --- primary outputs ---\n",
        "            \"final_warning\": bool(final_warning),   # ✅ class-1 (your objective)\n",
        "            \"final_failure\": bool(final_failure),   # class-2\n",
        "            \"final_anomaly\": bool(final_anomaly),   # current abnormality\n",
        "            \"final_drift\": bool(final_drift),\n",
        "            \"final_retrain\": bool(final_retrain),\n",
        "            \"alert_level\": alert_level,\n",
        "\n",
        "            # --- scores (traceable) ---\n",
        "\n",
        "            \"scores\": {\n",
        "                \"p_fault\": pred[\"p_fault\"],\n",
        "                \"p_warn_baseline\": warn_ref[\"p_warn_base\"],\n",
        "                \"p_warn_refined\": warn_ref[\"p_warn_refined\"],\n",
        "                \"p_normal\": pred[\"p_normal\"],\n",
        "                \"detection_risk\": float(detection_risk),\n",
        "                \"drift_risk\": float(drift_risk),\n",
        "                \"operational_score\": float(operational_score),\n",
        "\n",
        "                # explanation hooks\n",
        "                \"warning_contradiction\": warn_ref[\"contradiction\"],\n",
        "                \"warn_boost_applied\": warn_ref[\"gates\"][\"boost_gate\"],\n",
        "                \"warn_penalty_applied\": warn_ref[\"gates\"][\"penalty_gate\"],\n",
        "            },\n",
        "            \"raw\": {\n",
        "                \"master_output\": master_output,\n",
        "                \"window_output\": window_output,\n",
        "                \"model_outputs\": model_outputs,\n",
        "                \"metadata\": metadata,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.history.append(packet)\n",
        "        return packet\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# UPGRADED EXPERT AGENT (SOTA-STYLE FOR METROPT)\n",
        "# =====================================================\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, Any, List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExpertAgent:\n",
        "    \"\"\"\n",
        "    ExpertAgent (Upgraded):\n",
        "      - Invoked when DecisionAgent flags an anomaly (or when you choose).\n",
        "      - Uses rich context:\n",
        "          * decision_packet (DecisionAgent output)\n",
        "          * raw master_output (sensor-level results)\n",
        "          * raw window_output (AdaptiveWindowAgent)\n",
        "          * prototype scores, etc. via model_outputs in decision_packet\n",
        "          * recent multivariate sensor window (numeric snapshot)\n",
        "          * recent history from EventStore\n",
        "          * static domain knowledge (sensor metadata, fault patterns)\n",
        "      - Calls an LLM to produce:\n",
        "          * summary\n",
        "          * explanation\n",
        "          * likely_fault\n",
        "          * recommended_action\n",
        "          * severity\n",
        "      - Validates JSON and falls back gracefully on errors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        event_store,\n",
        "        system_description: str = \"MetroPT Air Production Unit (APU)\",\n",
        "        llm_client: Optional[object] = None,\n",
        "        history_limit: int = 100,\n",
        "        max_history_for_prompt: int = 10,\n",
        "        window_preview_len: int = 10,\n",
        "        sensor_metadata: Optional[Dict[str, str]] = None,\n",
        "        fault_knowledge: Optional[List[str]] = None,\n",
        "        max_retries: int = 2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        event_store         : EventStore instance (for past decisions)\n",
        "        system_description  : short description of the system\n",
        "        llm_client          : OpenAI client (OpenAI())\n",
        "        history_limit       : how many past events to fetch from DB\n",
        "        max_history_for_prompt : how many to actually show to LLM\n",
        "        window_preview_len  : last N timesteps per sensor to show\n",
        "        sensor_metadata     : mapping sensor_index -> human-readable name\n",
        "        fault_knowledge     : list of domain fault patterns (strings)\n",
        "        max_retries         : number of LLM retry attempts on failure\n",
        "        \"\"\"\n",
        "        self.store = event_store\n",
        "        self.system_description = system_description\n",
        "        self.llm_client = llm_client\n",
        "        self.history_limit = history_limit\n",
        "        self.max_history_for_prompt = max_history_for_prompt\n",
        "        self.window_preview_len = window_preview_len\n",
        "        self.max_retries = max_retries\n",
        "\n",
        "        # Default MetroPT-style sensor metadata (can be overridden)\n",
        "        if sensor_metadata is None:\n",
        "            self.sensor_metadata = {\n",
        "                0: \"TP2 (bar): the measure of the pressure on the compressor\",\n",
        "                1: \"TP3 (bar): the measure of the pressure generated at the pneumatic panel\",\n",
        "                2: \"H1 (bar) – the measure of the pressure generated due to pressure drop when the discharge of the cyclonic separator filter occurs\",\n",
        "                3: \"DV pressure (bar): the measure of the pressure drop generated when the towers discharge air dryers; a zero reading indicates that the compressor is operating under load\",\n",
        "                4: \"Reservoirs (bar): the measure of the downstream pressure of the reservoirs, which should be close to the pneumatic panel pressure (TP3)\",\n",
        "                5: \"Oil Temperature (ºC) :  the measure of the oil temperature on the compressor\",\n",
        "                6: \"Motor Current (A) –  the measure of the current of one phase of the three-phase motor; it presents values close to 0A - when it turns off, 4A - when working offloaded, 7A - when working under load, and 9A - when it starts working\",\n",
        "                7: \"COMP - the electrical signal of the air intake valve on the compressor; it is active when there is no air intake, indicating that the compressor is either turned off or operating in an offloaded state.\",\n",
        "                8: \"DV electric – the electrical signal that controls the compressor outlet valve; it is active when the compressor is functioning under load and inactive when the compressor is either off or operating in an offloaded state.\",\n",
        "                9: \"TOWERS – the electrical signal that defines the tower responsible for drying the air and the tower responsible for draining the humidity removed from the air; when not active, it indicates that tower one is functioning; when active, it indicates that tower two is in operation.\",\n",
        "                10: \"MPG – the electrical signal responsible for starting the compressor under load by activating the intake valve when the pressure in the air production unit (APU) falls below 8.2 bar; it activates the COMP sensor, which assumes the same behaviour as the MPG sensor\",\n",
        "                11: \"LPS – the electrical signal that detects and activates when the pressure drops below 7 bars\",\n",
        "\n",
        "            }\n",
        "        else:\n",
        "            self.sensor_metadata = sensor_metadata\n",
        "\n",
        "        # Default MetroPT-style fault patterns (can be overridden)\n",
        "        if fault_knowledge is None:\n",
        "            self.fault_knowledge = [\n",
        "                \"Air Leak: gradual pressure decay + increased compressor runtime.\",\n",
        "                \"Blockage: oscillatory or unstable pressure + abnormal valve cycling.\",\n",
        "                \"Overheating: rising oil temperature + increased motor current.\",\n",
        "                \"Valve Stuck: valve digital state frozen while pressure behaviour is abnormal.\",\n",
        "                \"Short Cycling: frequent compressor start/stop in short intervals.\",\n",
        "                \"Sensor Failure: flat-line, impossible values, or inconsistent readings.\",\n",
        "            ]\n",
        "        else:\n",
        "            self.fault_knowledge = fault_knowledge\n",
        "\n",
        "    # =====================================================\n",
        "    # PUBLIC ENTRYPOINT\n",
        "    # =====================================================\n",
        "    def analyse_anomaly(\n",
        "        self,\n",
        "        decision_packet: dict,\n",
        "        system_subsequence: np.ndarray,\n",
        "        extra_context: Optional[dict] = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Main entry point:\n",
        "          - decision_packet: from DecisionAgent.decide(...)\n",
        "          - system_subsequence: [window_length, num_sensors] np.ndarray\n",
        "          - extra_context: anything else (index, timestamps, etc.)\n",
        "\n",
        "        Returns:\n",
        "          expert_packet dict with:\n",
        "            - timestamp\n",
        "            - decision_packet\n",
        "            - prompt_used\n",
        "            - llm_result (JSON from model or fallback)\n",
        "        \"\"\"\n",
        "        # 1) Fetch recent past decisions from EventStore\n",
        "        recent_raw_decisions = self.store.fetch_recent(limit=self.history_limit)\n",
        "\n",
        "        # 2) Build rich prompt\n",
        "        prompt = self._build_prompt(\n",
        "            decision_packet=decision_packet,\n",
        "            system_subsequence=system_subsequence,\n",
        "            extra_context=extra_context,\n",
        "            recent_events=recent_raw_decisions,\n",
        "        )\n",
        "\n",
        "        # 3) Call LLM with JSON-only contract\n",
        "        llm_result = self._call_llm_with_json(prompt)\n",
        "\n",
        "        expert_packet = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"decision_packet\": decision_packet,\n",
        "            \"prompt_used\": prompt,\n",
        "            \"llm_result\": llm_result,\n",
        "        }\n",
        "\n",
        "        return expert_packet\n",
        "\n",
        "    # =====================================================\n",
        "    # PROMPT CONSTRUCTION\n",
        "    # =====================================================\n",
        "    def _build_prompt(\n",
        "        self,\n",
        "        decision_packet: dict,\n",
        "        system_subsequence: np.ndarray,\n",
        "        extra_context: Optional[dict],\n",
        "        recent_events: List[dict],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Build a rich natural-language + structured prompt for the LLM.\n",
        "        Includes:\n",
        "          - system description\n",
        "          - sensor metadata\n",
        "          - known fault patterns\n",
        "          - current decision scores + flags\n",
        "          - per-sensor anomaly info (if available)\n",
        "          - small numeric snapshot of current window\n",
        "          - compressed recent history\n",
        "          - explicit JSON response schema\n",
        "        \"\"\"\n",
        "\n",
        "        # -------------------------------\n",
        "        # 1) Extract top-level scores\n",
        "        # -------------------------------\n",
        "        scores = decision_packet.get(\"scores\", {})\n",
        "        window_info = decision_packet.get(\"window_agent\", {})\n",
        "        alert_level = decision_packet.get(\"alert_level\", \"UNKNOWN\")\n",
        "        final_anomaly = decision_packet.get(\"final_anomaly\", False)\n",
        "        final_drift = decision_packet.get(\"final_drift\", False)\n",
        "        final_retrain = decision_packet.get(\"final_retrain\", False)\n",
        "\n",
        "        # -------------------------------\n",
        "        # 2) Extract raw master/window/model data (if available)\n",
        "        # -------------------------------\n",
        "        raw_block = decision_packet.get(\"raw\", {})\n",
        "        master_output = raw_block.get(\"master_output\", None)\n",
        "        window_output = raw_block.get(\"window_output\", None)\n",
        "        model_outputs = raw_block.get(\"model_outputs\", {})\n",
        "        metadata = raw_block.get(\"metadata\", {})\n",
        "\n",
        "        prototype_score = model_outputs.get(\"prototype_score\", None)\n",
        "        transformer_prob = model_outputs.get(\"transformer_prob\", None)\n",
        "\n",
        "        # -------------------------------\n",
        "        # 3) Per-sensor anomaly stats from master_output\n",
        "        # -------------------------------\n",
        "        per_sensor_summary = []\n",
        "        if master_output is not None:\n",
        "            sensor_results = master_output.get(\"sensor_results\", [])\n",
        "            for i, res in enumerate(sensor_results):\n",
        "                name = self.sensor_metadata.get(i, f\"Sensor_{i}\")\n",
        "                per_sensor_summary.append({\n",
        "                    \"sensor_index\": i,\n",
        "                    \"name\": name,\n",
        "                    \"is_anomaly\": bool(res.get(\"is_anomaly\", False)),\n",
        "                    \"drift_flag\": bool(res.get(\"drift_flag\", False)),\n",
        "                    \"needs_retrain\": bool(res.get(\"needs_retrain_flag\", False)),\n",
        "                    \"anomaly_score\": float(res.get(\"anomaly_score\", 0.0)),\n",
        "                    \"confidence\": float(res.get(\"confidence\", 0.0)),\n",
        "                })\n",
        "\n",
        "        # -------------------------------\n",
        "        # 4) Numeric snapshot of current window\n",
        "        # -------------------------------\n",
        "        # system_subsequence: [T, F]\n",
        "        try:\n",
        "            seq = np.asarray(system_subsequence)\n",
        "            T, F = seq.shape\n",
        "        except Exception:\n",
        "            seq = np.array(system_subsequence)\n",
        "            if seq.ndim == 1:\n",
        "                seq = seq.reshape(-1, 1)\n",
        "            T, F = seq.shape\n",
        "\n",
        "        preview_len = min(self.window_preview_len, T)\n",
        "        window_preview = seq[-preview_len:]  # shape [preview_len, F]\n",
        "\n",
        "        # Represent as {sensor_name: [values...]}\n",
        "        sensor_window_dict = {}\n",
        "        for j in range(F):\n",
        "            name = self.sensor_metadata.get(j, f\"Sensor_{j}\")\n",
        "            sensor_window_dict[name] = window_preview[:, j].round(4).tolist()\n",
        "\n",
        "        # -------------------------------\n",
        "        # 5) Compressed recent history\n",
        "        # -------------------------------\n",
        "        history_summaries = []\n",
        "        for ev in recent_events[: self.max_history_for_prompt]:\n",
        "            try:\n",
        "                # ev is a past decision_packet (because we stored 'packet=decision_packet')\n",
        "                scores_ev = ev.get(\"scores\", {})\n",
        "                history_summaries.append({\n",
        "                    \"timestamp\": ev.get(\"timestamp\", \"\"),\n",
        "                    \"alert_level\": ev.get(\"alert_level\", \"UNKNOWN\"),\n",
        "                    \"anomaly_score\": scores_ev.get(\"anomaly_score\", None),\n",
        "                    \"drift_score\": scores_ev.get(\"drift_score\", None),\n",
        "                })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # -------------------------------\n",
        "        # 6) Sensor metadata & fault patterns as text\n",
        "        # -------------------------------\n",
        "        sensor_meta_list = [f\"{idx}: {desc}\" for idx, desc in self.sensor_metadata.items()]\n",
        "\n",
        "        fault_knowledge_text = \"\\n\".join(\n",
        "            [f\"- {fk}\" for fk in self.fault_knowledge]\n",
        "        )\n",
        "\n",
        "        # -------------------------------\n",
        "        # 7) Build final instruction with JSON schema\n",
        "        # -------------------------------\n",
        "        schema_instruction = \"\"\"\n",
        "You MUST respond with ONLY a single valid JSON object, no extra text.\n",
        "The JSON MUST have exactly the following keys at the top level:\n",
        "\n",
        "- \"summary\": short 1-2 sentence description of what is happening.\n",
        "- \"explanation\": 2-6 sentences, clear reasoning in industrial / physical terms.\n",
        "- \"likely_fault\": short label of the most likely fault type, or \"Unknown\".\n",
        "- \"recommended_action\": one of:\n",
        "    - \"IGNORE\"\n",
        "    - \"MONITOR\"\n",
        "    - \"ACK_AND_INVESTIGATE\"\n",
        "    - \"SCHEDULE_MAINTENANCE\"\n",
        "    - \"IMMEDIATE_SHUTDOWN\"\n",
        "- \"severity\": one of: \"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\".\n",
        "\n",
        "Do NOT include any extra keys outside these five.\n",
        "Do NOT include any surrounding text, markdown, or commentary.\n",
        "Return ONLY the JSON object.\n",
        "\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an expert industrial fault diagnosis assistant for: {self.system_description}.\n",
        "\n",
        "System context:\n",
        "- This is a compressed-air production unit (APU) of a metro train.\n",
        "- Sensors include analog (pressure, current, temperature) and digital (valves, states).\n",
        "\n",
        "Sensor metadata (index -> description):\n",
        "{json.dumps(sensor_meta_list, indent=2)}\n",
        "\n",
        "Known fault patterns (domain knowledge):\n",
        "{fault_knowledge_text}\n",
        "\n",
        "CURRENT DECISION PACKET:\n",
        "\n",
        "- Alert level: {alert_level}\n",
        "\n",
        "PREDICTION SIGNALS (from ML failure model):\n",
        "- p_fault: {scores.get(\"p_fault\")}\n",
        "- p_warn: {scores.get(\"p_warn\")}\n",
        "- failure_risk: {scores.get(\"failure_risk\")}\n",
        "\n",
        "DETECTION SIGNALS (from sensor & window agents):\n",
        "- final_anomaly (sensor-based): {final_anomaly}\n",
        "- final_drift (sensor/window-based): {final_drift}\n",
        "- final_retrain: {final_retrain}\n",
        "\n",
        "All prediction and detection signals are separate:\n",
        "- Prediction = future failure likelihood (ML)\n",
        "- Detection = current abnormalities (sensor + drift + window)\n",
        "\n",
        "Window agent info:\n",
        "{json.dumps(window_info)}\n",
        "\n",
        "Prototype / transformer outputs (if any):\n",
        "- prototype_score: {prototype_score}\n",
        "- transformer_prob: {transformer_prob}\n",
        "\n",
        "Per-sensor anomaly summary:\n",
        "{json.dumps(per_sensor_summary, indent=2)}\n",
        "\n",
        "Current sensor window snapshot (last {preview_len} timesteps for each sensor):\n",
        "{json.dumps(sensor_window_dict, indent=2)}\n",
        "\n",
        "Recent history of decisions (compressed):\n",
        "{json.dumps(history_summaries, indent=2)}\n",
        "\n",
        "Extra context:\n",
        "{json.dumps(extra_context, default=str)}\n",
        "\n",
        "Your tasks:\n",
        "1. Decide whether this anomaly is likely REAL or a FALSE POSITIVE.\n",
        "2. Infer which fault pattern (if any) best matches the evidence.\n",
        "3. Explain the reasoning in terms of sensor behaviour (pressure, current, valves, temperature).\n",
        "4. Recommend the next action for the human operator, considering safety and cost.\n",
        "5. Assign a severity level: LOW, MEDIUM, HIGH, or CRITICAL.\n",
        "\n",
        "{schema_instruction}\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    # =====================================================\n",
        "    # LLM CALL + JSON HANDLING\n",
        "    # =====================================================\n",
        "    def _call_llm_with_json(self, prompt: str) -> dict:\n",
        "        \"\"\"\n",
        "        Calls the LLM via self.llm_client and returns a validated JSON dict.\n",
        "        Uses:\n",
        "          - prompt-based JSON contract\n",
        "          - retry with minimal fallback\n",
        "          - schema validation\n",
        "        \"\"\"\n",
        "\n",
        "        # If no client configured, return fallback\n",
        "        if self.llm_client is None:\n",
        "            return {\n",
        "                \"summary\": \"Anomaly detected (fallback, no LLM client configured).\",\n",
        "                \"explanation\": \"ExpertAgent has no LLM client; returning default recommendation.\",\n",
        "                \"likely_fault\": \"Unknown\",\n",
        "                \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "                \"severity\": \"MEDIUM\",\n",
        "            }\n",
        "\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = self.llm_client.responses.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    input=prompt,\n",
        "                    max_output_tokens=400,\n",
        "                )\n",
        "\n",
        "                # Using unified Responses API: easiest is output_text\n",
        "                raw_text = getattr(response, \"output_text\", None)\n",
        "                if raw_text is None:\n",
        "                    # fallback to explicit extraction\n",
        "                    raw_text = response.output[0].content[0].text\n",
        "\n",
        "                parsed = self._robust_json_parse(raw_text)\n",
        "                validated = self._validate_llm_json(parsed)\n",
        "                return validated\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ ExpertAgent LLM error (attempt {attempt+1}): {e}\")\n",
        "                last_error = e\n",
        "\n",
        "        # Final fallback if everything fails\n",
        "        return {\n",
        "            \"summary\": \"Anomaly detected (LLM fallback).\",\n",
        "            \"explanation\": f\"LLM failed or returned invalid JSON. Last error: {last_error}\",\n",
        "            \"likely_fault\": \"Unknown\",\n",
        "            \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "            \"severity\": \"MEDIUM\",\n",
        "        }\n",
        "\n",
        "    def _robust_json_parse(self, text: str) -> dict:\n",
        "        \"\"\"\n",
        "        Best-effort JSON parsing:\n",
        "          - First try direct json.loads\n",
        "          - If fails, try to extract the first {...} block\n",
        "        \"\"\"\n",
        "        text = text.strip()\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except Exception:\n",
        "            # Try to find a JSON object substring\n",
        "            start = text.find(\"{\")\n",
        "            end = text.rfind(\"}\")\n",
        "            if start != -1 and end != -1 and end > start:\n",
        "                snippet = text[start : end + 1]\n",
        "                return json.loads(snippet)\n",
        "            # If still failing, raise\n",
        "            raise\n",
        "\n",
        "    def _validate_llm_json(self, obj: Any) -> dict:\n",
        "        \"\"\"\n",
        "        Enforce a minimal schema:\n",
        "          - must be a dict\n",
        "          - must contain keys: summary, explanation, likely_fault, recommended_action, severity\n",
        "        If keys are missing, fill with defaults.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(obj, dict):\n",
        "            raise ValueError(\"LLM output is not a JSON object\")\n",
        "\n",
        "        required_keys = [\"summary\", \"explanation\", \"likely_fault\", \"recommended_action\", \"severity\"]\n",
        "        defaults = {\n",
        "            \"summary\": \"No summary provided by LLM.\",\n",
        "            \"explanation\": \"No explanation provided by LLM.\",\n",
        "            \"likely_fault\": \"Unknown\",\n",
        "            \"recommended_action\": \"ACK_AND_INVESTIGATE\",\n",
        "            \"severity\": \"MEDIUM\",\n",
        "        }\n",
        "\n",
        "        cleaned = {}\n",
        "        for k in required_keys:\n",
        "            v = obj.get(k, defaults[k])\n",
        "            # ensure string\n",
        "            cleaned[k] = str(v)\n",
        "\n",
        "        # Optionally normalise recommended_action/severity to known values\n",
        "        cleaned[\"recommended_action\"] = cleaned[\"recommended_action\"].upper().strip()\n",
        "        cleaned[\"severity\"] = cleaned[\"severity\"].upper().strip()\n",
        "\n",
        "        # Clamp to allowed sets if desired\n",
        "        allowed_actions = {\n",
        "            \"IGNORE\",\n",
        "            \"MONITOR\",\n",
        "            \"ACK_AND_INVESTIGATE\",\n",
        "            \"SCHEDULE_MAINTENANCE\",\n",
        "            \"IMMEDIATE_SHUTDOWN\",\n",
        "        }\n",
        "        if cleaned[\"recommended_action\"] not in allowed_actions:\n",
        "            cleaned[\"recommended_action\"] = \"ACK_AND_INVESTIGATE\"\n",
        "\n",
        "        allowed_severity = {\"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\"}\n",
        "        if cleaned[\"severity\"] not in allowed_severity:\n",
        "            cleaned[\"severity\"] = \"MEDIUM\"\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SIMPLE HUMAN LOOP + ALERT STUB\n",
        "# =====================================================\n",
        "\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "\n",
        "def send_alert_to_human(expert_packet: dict):\n",
        "    \"\"\"\n",
        "    Sends an email alert to a human operator using SMTP.\n",
        "    Includes summary, recommended action, severity, and JSON dump.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Extract ExpertAgent output\n",
        "    # ----------------------\n",
        "    result = expert_packet.get(\"llm_result\", {})\n",
        "    summary = result.get(\"summary\", \"No summary\")\n",
        "    action = result.get(\"recommended_action\", \"N/A\")\n",
        "    severity = result.get(\"severity\", \"N/A\")\n",
        "\n",
        "    decision_packet_json = json.dumps(\n",
        "        expert_packet.get(\"decision_packet\", {}),\n",
        "        indent=2\n",
        "    )\n",
        "    llm_json = json.dumps(result, indent=2)\n",
        "\n",
        "    # ----------------------\n",
        "    # Email content\n",
        "    # ----------------------\n",
        "    subject = f\"⚠️ APU Alert – Severity: {severity}\"\n",
        "\n",
        "    body = f\"\"\"\n",
        "Human Operator,\n",
        "\n",
        "An alert has been generated by the APU Monitoring System.\n",
        "\n",
        "-------------------\n",
        "SUMMARY\n",
        "-------------------\n",
        "{summary}\n",
        "\n",
        "-------------------\n",
        "RECOMMENDED ACTION\n",
        "-------------------\n",
        "{action}\n",
        "\n",
        "-------------------\n",
        "SEVERITY\n",
        "-------------------\n",
        "{severity}\n",
        "\n",
        "-------------------\n",
        "FULL LLM RESULT\n",
        "-------------------\n",
        "{llm_json}\n",
        "\n",
        "-------------------\n",
        "RAW DECISION PACKET\n",
        "-------------------\n",
        "{decision_packet_json}\n",
        "\n",
        "Timestamp: {expert_packet.get('timestamp')}\n",
        "\"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Create email object\n",
        "    # ----------------------\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = EMAIL_SENDER\n",
        "    msg[\"To\"] = EMAIL_RECIPIENT\n",
        "    msg[\"Subject\"] = subject\n",
        "\n",
        "    msg.attach(MIMEText(body, \"plain\"))\n",
        "\n",
        "    # ----------------------\n",
        "    # Send email via SMTP\n",
        "    # ----------------------\n",
        "    try:\n",
        "        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
        "        server.starttls()\n",
        "        server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
        "        server.sendmail(EMAIL_SENDER, EMAIL_RECIPIENT, msg.as_string())\n",
        "        server.quit()\n",
        "\n",
        "        print(\"\\n=== EMAIL ALERT SENT SUCCESSFULLY ===\")\n",
        "        print(f\"To: {EMAIL_RECIPIENT}\")\n",
        "        print(f\"Summary: {summary}\")\n",
        "        print(\"======================================\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to send alert email:\", e)\n",
        "\n",
        "\n",
        "\n",
        "def get_human_feedback_stub(expert_packet: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Stub: in a real deployment, this would come from UI / operator.\n",
        "    Here we just echo back a synthetic 'ACK'.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"final_decision\": \"ACK_AND_LOG\",\n",
        "        \"notes\": \"Stub human feedback – replace with real operator input.\",\n",
        "    }\n",
        "##################################################\n",
        "#GROUND TRUTH ANOMALY VS WHICH AGENT AGENT IS RIGHT\n",
        "############################################################\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_agent_vs_groundtruth(\n",
        "    results,\n",
        "    detection_labels=None,\n",
        "    h1_labels=None,\n",
        "    h3_labels=None,\n",
        "    h12_labels=None,\n",
        "    max_samples=200\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot comparison between agent decisions, ground-truth labels,\n",
        "    decision outputs, and AdaptiveWindowAgent predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    n = min(max_samples, len(results))\n",
        "    t = np.arange(n)\n",
        "\n",
        "    # Extract decision anomaly/drift\n",
        "    agent_anomaly = [1 if r[\"decision\"][\"final_anomaly\"] else 0 for r in results[:n]]\n",
        "    agent_drift   = [1 if r[\"decision\"][\"final_drift\"] else 0 for r in results[:n]]\n",
        "\n",
        "    # New: extract ML prediction\n",
        "    failure_prob = [\n",
        "        results[i][\"decision\"][\"scores\"].get(\"prediction_failure_prob\", 0.0)\n",
        "        for i in range(n)\n",
        "    ]\n",
        "    final_failure = [\n",
        "        1 if results[i][\"decision\"].get(\"final_failure\") else 0\n",
        "        for i in range(n)\n",
        "    ]\n",
        "\n",
        "    # Ground-truth labels\n",
        "    det = detection_labels[:n] if detection_labels is not None else None\n",
        "    h1  = h1_labels[:n] if h1_labels is not None else None\n",
        "    h3  = h3_labels[:n] if h3_labels is not None else None\n",
        "    h12 = h12_labels[:n] if h12_labels is not None else None\n",
        "\n",
        "    # Window agent outputs\n",
        "    window_sizes  = [r[\"window\"][\"predicted_window\"] for r in results[:n]]\n",
        "    window_events = [r[\"window\"].get(\"event_type\") for r in results[:n]]\n",
        "\n",
        "    # ---- Create subplots ----\n",
        "    fig, axes = plt.subplots(6, 1, figsize=(15, 17), sharex=True)\n",
        "\n",
        "    # Agent outputs\n",
        "    axes[0].plot(t, agent_anomaly, label=\"Agent Anomaly\", color=\"red\")\n",
        "    axes[0].plot(t, agent_drift,   label=\"Agent Drift\", color=\"orange\")\n",
        "    axes[0].set_ylabel(\"Agent\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Detection labels\n",
        "    if det is not None:\n",
        "        axes[1].plot(t, det, label=\"Detection Labels\", color=\"blue\")\n",
        "        axes[1].set_ylabel(\"Detect\")\n",
        "        axes[1].legend()\n",
        "\n",
        "    # Prediction labels (1h, 3h, 12h)\n",
        "    if h1 is not None:\n",
        "        axes[2].plot(t, h1, label=\"H1\", color=\"green\")\n",
        "    if h3 is not None:\n",
        "        axes[2].plot(t, h3, label=\"H3\", color=\"purple\")\n",
        "    if h12 is not None:\n",
        "        axes[2].plot(t, h12, label=\"H12\", color=\"brown\")\n",
        "    axes[2].set_ylabel(\"Prediction\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    # decision alert level\n",
        "    alert_map = {\"NORMAL\": 0, \"MEDIUM\": 1, \"HIGH\": 2, \"CRITICAL\": 3}\n",
        "    alerts = [alert_map.get(r[\"decision\"][\"alert_level\"], 0) for r in results[:n]]\n",
        "    axes[3].plot(t, alerts, label=\"Alert Level\", color=\"black\")\n",
        "    axes[3].set_yticks([0,1,2,3])\n",
        "    axes[3].set_yticklabels([\"NORMAL\",\"MED\",\"HIGH\",\"CRIT\"])\n",
        "    axes[3].set_ylabel(\"decision\")\n",
        "    axes[3].legend()\n",
        "\n",
        "    # AdaptiveWindowAgent subplot\n",
        "    axes[4].plot(t, window_sizes, label=\"Predicted Window\", color=\"blue\")\n",
        "\n",
        "    # Mark drift/anomaly events\n",
        "    for i, evt in enumerate(window_events):\n",
        "        if evt == \"DRIFT\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"orange\", marker=\"x\", label=\"Window Drift\" if i==0 else \"\")\n",
        "        elif evt == \"ANOMALY\":\n",
        "            axes[4].scatter(i, window_sizes[i], color=\"red\", marker=\"o\", label=\"Window Anomaly\" if i==0 else \"\")\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "    # ⭐ Row 6: ML Failure Prediction Probability\n",
        "    # ----------------------------------------------------\n",
        "    axes[5].plot(t, failure_prob, label=\"Failure Prob (ML)\", color=\"green\")\n",
        "    axes[5].plot(t, final_failure, label=\"Final Failure Decision\", color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    axes[5].set_ylabel(\"Fail Prob\")\n",
        "    axes[5].set_ylim([-0.1, 1.1])\n",
        "    axes[5].legend()\n",
        "\n",
        "    # Overlay ground-truth fault events (vertical lines)\n",
        "    if det is not None:\n",
        "        for i, val in enumerate(det):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Fault (Detection)\" if i==0 else \"\")\n",
        "    if h1 is not None:\n",
        "        for i, val in enumerate(h1):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"green\", linestyle=\"--\", alpha=0.2, label=\"Fault (H1)\" if i==0 else \"\")\n",
        "    if h3 is not None:\n",
        "        for i, val in enumerate(h3):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"purple\", linestyle=\"--\", alpha=0.2, label=\"Fault (H3)\" if i==0 else \"\")\n",
        "    if h12 is not None:\n",
        "        for i, val in enumerate(h12):\n",
        "            if val == 1:\n",
        "                axes[4].axvline(i, color=\"brown\", linestyle=\"--\", alpha=0.2, label=\"Fault (H12)\" if i==0 else \"\")\n",
        "\n",
        "    axes[4].set_ylabel(\"Window Size\")\n",
        "    axes[4].legend()\n",
        "\n",
        "    axes[-1].set_xlabel(\"Sample index\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#=========================================================================================================================================\n",
        "#================= TESTING MAIN CODE========================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#=========================================================================================================================================\n",
        "#############################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 1. LOAD DATA + MASK + LABELS\n",
        "    # -------------------------------------------------\n",
        "data_path       = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/multivariate_long_sequences-TRAIN-10Sec-DIRECT-VAR.npy\"\n",
        "label_path      = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/window_labels_3class.npy\"\n",
        "train_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/train_mask.npy\"\n",
        "test_mask_path  = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/test_mask.npy\"\n",
        "holdout_mask_path = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/holdout_mask.npy\"\n",
        "\n",
        "X = np.load(data_path)        # (N, W, S)\n",
        "y = np.load(label_path)       # (N,) ∈ {0,1,2}\n",
        "\n",
        "train_mask   = np.load(train_mask_path).astype(bool)\n",
        "test_mask    = np.load(test_mask_path).astype(bool)\n",
        "holdout_mask = np.load(holdout_mask_path).astype(bool)\n",
        "\n",
        "# Sanity check (VERY important) -- This assertion enforces strict mutual exclusivity between evaluation and holdout subsets, preventing data leakage and ensuring unbiased performance estimation.\n",
        "assert not np.any(train_mask & test_mask)\n",
        "assert not np.any(train_mask & holdout_mask)\n",
        "assert not np.any(test_mask & holdout_mask)\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "X_hold,  y_hold  = X[holdout_mask], y[holdout_mask]\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)\n",
        "print(\"Hold :\", X_hold.shape)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# -------------TRansformer for 3-class classification - NO CONTRASTIVE LEARNING\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "MODEL_DIR  = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class\"\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, \"transformer_classifier.keras\")\n",
        "\n",
        "\n",
        "def transformer_encoder(x, head_size, num_heads, ff_dim, dropout=0.2):\n",
        "    # --- Self-attention block ---\n",
        "    attn_output = layers.MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout\n",
        "    )(x, x)\n",
        "\n",
        "    attn_output = layers.Dropout(dropout)(attn_output)\n",
        "    x = layers.Add()([x, attn_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # --- Feed-forward block ---\n",
        "    ff_output = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff_output = layers.Dropout(dropout)(ff_output)\n",
        "    ff_output = layers.Dense(x.shape[-1])(ff_output)\n",
        "\n",
        "    x = layers.Add()([x, ff_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_transformer(window_len, n_features, n_classes=3):\n",
        "    inputs = keras.Input(shape=(window_len, n_features))\n",
        "\n",
        "    # Project features to model dimension\n",
        "    x = layers.Dense(64)(inputs)\n",
        "\n",
        "    # Transformer blocks\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.2)\n",
        "\n",
        "    # Pooling + classifier\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "n_classes = 3\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(\"✅ Found pretrained Transformer. Loading...\")\n",
        "\n",
        "    classifier = load_model(MODEL_PATH)\n",
        "    classifier.trainable = False\n",
        "\n",
        "else:\n",
        "    print(\"🚀 No pretrained model found. Training Transformer...\")\n",
        "\n",
        "    window_len  = X_train.shape[1]\n",
        "    n_features = X_train.shape[2]\n",
        "\n",
        "    classifier = build_transformer(window_len, n_features, n_classes)\n",
        "\n",
        "    classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(k=2, name=\"top2_acc\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Class weights (IMBALANCE HANDLING)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    class_weight = {\n",
        "        i: w for i, w in zip(np.unique(y_train), class_weights)\n",
        "    }\n",
        "\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=50,\n",
        "        batch_size=256,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "    classifier.save(MODEL_PATH, include_optimizer=False)\n",
        "\n",
        "    print(\"✅ Transformer trained and saved.\")\n",
        "\n",
        "\n",
        "\n",
        "####-----------------------------Load and test-----------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        modulating_factor = K.pow(1.0 - p_t, gamma)\n",
        "\n",
        "        return K.mean(alpha_factor * modulating_factor * bce)\n",
        "    return loss\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/PHD/2025/models/transformer_3class/transformer_classifier.keras\"\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "classifier = load_model(\n",
        "    model_path,\n",
        "    custom_objects={\"focal_loss\": focal_loss}  # only if you actually used it\n",
        ")\n",
        "\n",
        "classifier.trainable = False  # safety\n",
        "\n",
        "\n",
        "print(\"✅ Transformer loaded for TEST\")\n",
        "probs_test = classifier.predict(X_test)\n",
        "y_pred_test = np.argmax(probs_test, axis=1)\n",
        "\n",
        "print(\"\\n=== TEST SET PERFORMANCE ===\")\n",
        "print(classification_report(y_test, y_pred_test, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "###-------------------------HOLDOUT-------------------\n",
        "############################################################\n",
        "probs_hold = classifier.predict(X_hold)\n",
        "y_pred_hold = np.argmax(probs_hold, axis=1)\n",
        "\n",
        "print(\"\\n=== HOLDOUT SET PERFORMANCE (FINAL) ===\")\n",
        "print(classification_report(y_hold, y_pred_hold, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_hold, y_pred_hold))\n",
        "\n",
        "\n",
        "\n",
        "def transformer_signals(classifier, X_window):\n",
        "    probs = classifier.predict(X_window, verbose=0)[0]  # shape (3,)\n",
        "\n",
        "    p0, p1, p2 = probs\n",
        "\n",
        "    return {\n",
        "        \"p_normal\": float(p0),\n",
        "        \"early_fault_score\": float(p1),\n",
        "        \"severe_fault_score\": float(p2),\n",
        "        \"anomaly_score\": float(1.0 - p0)\n",
        "    }\n",
        "\n",
        "signals = transformer_signals(classifier, X_hold) #Get actual prob distribution now\n",
        "print(signals)\n",
        "\n",
        "\n",
        "# ===========================================================================================================================\n",
        "#  MAIN ORCHESTRATION: DECISION + EXPERT + EVENT STORE\n",
        "# ============================================================================================================================\n",
        "\n",
        "# Reuse your existing:\n",
        "# - window_agent  (AdaptiveWindowAgent)\n",
        "# - sensor_agents, master (RobustMasterAgent)\n",
        "# - scores        (prototype_scores on emb_holdout)\n",
        "# - X_holdout, y_holdout\n",
        "\n",
        "print(\"\\n🚀 Running DecisionAgent + ExpertAgent pipeline on HOLDOUT...\\n\")\n",
        "\n",
        "# ===========================================================================================================================\n",
        "# ONE OFF TUNING - Following code is now commented out because one off tuning completed and base policies saved\n",
        "# ============================================================================================================================\n",
        "\n",
        "# # =====================================================\n",
        "# # FAST POLICY TUNING + FINAL HOLDOUT (<= ~2 hours)\n",
        "# # - Stratified subset (guarantees WARNING samples)\n",
        "# # - Successive Halving\n",
        "# # - PRECOMP caching (perception runs once)\n",
        "# # - Checkpoint/resume for tuning\n",
        "# # - Saves best_policy JSON for holdout\n",
        "# # =====================================================\n",
        "\n",
        "# # ----------------------------\n",
        "# # 0) Imports\n",
        "# # ----------------------------\n",
        "# import os, json, time\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from sklearn.metrics import recall_score\n",
        "# from datetime import datetime\n",
        "# models_dir     = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "# # ----------------------------\n",
        "# # 1) REQUIRED: you must already have these in memory\n",
        "# # ----------------------------\n",
        "# # X_test, y_test, X_hold  : numpy arrays\n",
        "# # classifier              : trained transformer classifier (predicts 3-class probs)\n",
        "# # create_robust_system     : your function\n",
        "# # AdaptiveWindowAgent      : your class\n",
        "# # DecisionAgent            : your class\n",
        "# # models_dir               : path to AE sensor models folder\n",
        "\n",
        "# # Sanity checks\n",
        "# assert \"X_test\" in globals(), \"X_test not found\"\n",
        "# assert \"y_test\" in globals(), \"y_test not found\"\n",
        "# assert \"X_hold\" in globals(), \"X_hold not found\"\n",
        "# assert \"classifier\" in globals(), \"classifier not found\"\n",
        "# assert \"create_robust_system\" in globals(), \"create_robust_system not found\"\n",
        "# assert \"AdaptiveWindowAgent\" in globals(), \"AdaptiveWindowAgent not found\"\n",
        "# assert \"DecisionAgent\" in globals(), \"DecisionAgent not found\"\n",
        "# assert \"models_dir\" in globals(), \"models_dir not found\"\n",
        "\n",
        "# # ----------------------------\n",
        "# # 2) Paths\n",
        "# # ----------------------------\n",
        "# BASE_DIR = \"/content/drive/MyDrive/PHD/2025\"\n",
        "# CACHE_DIR = f\"{BASE_DIR}/cache\"\n",
        "# POLICY_DIR = f\"{BASE_DIR}/policies\"\n",
        "\n",
        "# PRECOMP_PATH = f\"{CACHE_DIR}/precomp_test.json\"                  # perception cache for X_test\n",
        "# CKPT_PATH    = f\"{POLICY_DIR}/tuning_checkpoint_fast.json\"       # tuning resume checkpoint\n",
        "# BEST_POLICY_PATH = f\"{POLICY_DIR}/best_decision_policy.json\"     # final tuned policy\n",
        "\n",
        "# os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "# os.makedirs(POLICY_DIR, exist_ok=True)\n",
        "\n",
        "# # ----------------------------\n",
        "# # 3) Policy grid (small & meaningful)\n",
        "# # ----------------------------\n",
        "# POLICY_GRID = [\n",
        "#     {\n",
        "#         \"detection_threshold\": dt,\n",
        "#         \"drift_threshold\": dr,\n",
        "#         \"failure_threshold\": ft,\n",
        "#         \"w_sensor\": ws,\n",
        "#         \"w_window\": ww,\n",
        "#         \"w_fault\": wf,\n",
        "#         \"w_warn\": ww2,\n",
        "#     }\n",
        "#     for dt in [0.45, 0.50, 0.55]\n",
        "#     for dr in [0.30, 0.35]\n",
        "#     for ft in [0.45, 0.50]\n",
        "#     for ws, ww in [(0.6, 0.4), (0.5, 0.5)]\n",
        "#     for wf, ww2 in [(0.7, 0.2), (0.6, 0.25)]\n",
        "# ]\n",
        "\n",
        "# # ----------------------------\n",
        "# # 4) Stratified subset (guarantee label-1 presence)\n",
        "# # ----------------------------\n",
        "# def build_stratified_subset(y_warn: np.ndarray, max_samples: int = 15000, warn_ratio: float = 0.40, seed: int = 42):\n",
        "#     rng = np.random.default_rng(seed)\n",
        "#     warn_idx = np.where(y_warn == 1)[0]\n",
        "#     norm_idx = np.where(y_warn == 0)[0]\n",
        "\n",
        "#     if len(warn_idx) == 0:\n",
        "#         raise ValueError(\"No WARNING (label==1) samples found in y_warn. Cannot stratify.\")\n",
        "\n",
        "#     n_warn = min(len(warn_idx), int(max_samples * warn_ratio))\n",
        "#     n_norm = min(len(norm_idx), max_samples - n_warn)\n",
        "\n",
        "#     sel_warn = rng.choice(warn_idx, size=n_warn, replace=False)\n",
        "#     sel_norm = rng.choice(norm_idx, size=n_norm, replace=False)\n",
        "\n",
        "#     sel = np.concatenate([sel_warn, sel_norm])\n",
        "#     rng.shuffle(sel)\n",
        "#     return sel\n",
        "\n",
        "# # ----------------------------\n",
        "# # 5) Build / load PRECOMP (perception outputs)\n",
        "# # ----------------------------\n",
        "# # NOTE:\n",
        "# # - This uses master + window_agent and classifier once.\n",
        "# # - It saves JSON so future tuning is fast.\n",
        "\n",
        "# def safe_jsonable(obj):\n",
        "#     \"\"\"Convert numpy / datetime objects into json-safe values.\"\"\"\n",
        "#     try:\n",
        "#         if isinstance(obj, (np.integer, np.int64)):\n",
        "#             return int(obj)\n",
        "#         if isinstance(obj, (np.floating, np.float64)):\n",
        "#             return float(obj)\n",
        "#         if isinstance(obj, np.ndarray):\n",
        "#             return obj.tolist()\n",
        "#     except Exception:\n",
        "#         pass\n",
        "#     return str(obj)\n",
        "\n",
        "# if os.path.exists(PRECOMP_PATH):\n",
        "#     print(\"✅ Loading cached PRECOMP...\")\n",
        "#     with open(PRECOMP_PATH, \"r\") as f:\n",
        "#         PRECOMP = json.load(f)\n",
        "# else:\n",
        "#     print(\"⏳ Precomputing perception outputs (runs ONCE per X_test)...\")\n",
        "#     PRECOMP = []\n",
        "\n",
        "#     # Instantiate perception agents ONCE\n",
        "#     # IMPORTANT: create_robust_system returns (sensor_agents, master)\n",
        "#     sensor_agents, master = create_robust_system(\n",
        "#         num_sensors=X_test.shape[2],\n",
        "#         models_dir=models_dir,\n",
        "#         win_length=X_test.shape[1],\n",
        "#         warmup_steps=100,\n",
        "#     )\n",
        "\n",
        "#     window_agent = AdaptiveWindowAgent(\n",
        "#         model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        "#     )\n",
        "\n",
        "#     SUB_IDX = build_stratified_subset(\n",
        "#     y_warn=(y_test == 1).astype(int),\n",
        "#     max_samples=12000,     # <= finishes in ~3–4 hours worst case\n",
        "#     warn_ratio=0.4\n",
        "#     )\n",
        "\n",
        "#     X_tune = X_test[SUB_IDX]\n",
        "\n",
        "#     for seq in tqdm(X_tune, total=len(X_tune)):\n",
        "#         master_out = master.process_system_input(seq)\n",
        "#         window_out = window_agent.predict_window_size(seq.flatten(), seq)\n",
        "#         probs = classifier.predict(seq[np.newaxis, ...], verbose=0)[0]\n",
        "\n",
        "#         PRECOMP.append({\n",
        "#             \"master\": master_out,\n",
        "#             \"window\": window_out,\n",
        "#             \"model_outputs\": {\n",
        "#                 \"failure_prob\": float(probs[2]),\n",
        "#                 \"transformer_prob\": float(probs[1]),\n",
        "#             }\n",
        "#         })\n",
        "\n",
        "#     with open(PRECOMP_PATH, \"w\") as f:\n",
        "#         json.dump(PRECOMP, f, default=safe_jsonable)\n",
        "\n",
        "#     print(f\"✅ PRECOMP saved to {PRECOMP_PATH}\")\n",
        "\n",
        "# # ----------------------------\n",
        "# # 6) Ensure master + window_agent exist for HOLDOUT run\n",
        "# #    (if PRECOMP was loaded, these may not exist in RAM)\n",
        "# # ----------------------------\n",
        "# if \"master\" not in globals() or master is None:\n",
        "#     print(\"⚠️ master not in memory; creating for holdout...\")\n",
        "#     sensor_agents, master = create_robust_system(\n",
        "#         num_sensors=X_test.shape[2],\n",
        "#         models_dir=models_dir,\n",
        "#         win_length=X_test.shape[1],\n",
        "#         warmup_steps=100,\n",
        "#     )\n",
        "\n",
        "# if \"window_agent\" not in globals() or window_agent is None:\n",
        "#     print(\"⚠️ window_agent not in memory; creating for holdout...\")\n",
        "#     window_agent = AdaptiveWindowAgent(\n",
        "#         model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        "#     )\n",
        "\n",
        "# # ----------------------------\n",
        "# # 7) Build tuning subset\n",
        "# # ----------------------------\n",
        "# y_warn_test = (y_test == 1).astype(int)   # WARNING only\n",
        "\n",
        "# SUB_IDX = build_stratified_subset(y_warn_test, max_samples=12000, warn_ratio=0.40)\n",
        "# PRECOMP_SUB = PRECOMP\n",
        "# y_warn_sub  = (y_test[SUB_IDX] == 1).astype(int)\n",
        "\n",
        "# print(f\"✅ Tuning subset size: {len(PRECOMP_SUB)} | WARNING ratio: {y_warn_sub.mean():.3f}\")\n",
        "\n",
        "# # ----------------------------\n",
        "# # 8) Successive Halving schedule\n",
        "# # ----------------------------\n",
        "# # Budget = how many PRECOMP_SUB items to use per policy in that stage\n",
        "# SH_SCHEDULE = [\n",
        "#     {\"budget\": 2000,  \"keep_frac\": 0.33},\n",
        "#     {\"budget\": 6000,  \"keep_frac\": 0.33},\n",
        "#     {\"budget\": 15000, \"keep_frac\": 1.00},\n",
        "# ]\n",
        "\n",
        "# # ----------------------------\n",
        "# # 9) Fast evaluation\n",
        "# # ----------------------------\n",
        "# def evaluate_policy_budget(policy_cfg, PRECOMP_list, y_warn_vec, budget):\n",
        "#     \"\"\"Primary objective: WARNING recall with limited budget.\"\"\"\n",
        "#     agent = DecisionAgent(**policy_cfg)\n",
        "#     preds = []\n",
        "\n",
        "#     # budget guard\n",
        "#     budget = min(budget, len(PRECOMP_list))\n",
        "\n",
        "#     for i in range(budget):\n",
        "#         d = PRECOMP_list[i]\n",
        "#         packet = agent.decide(\n",
        "#             master_output=d[\"master\"],\n",
        "#             window_output=d[\"window\"],\n",
        "#             model_outputs=d[\"model_outputs\"],\n",
        "#         )\n",
        "#         preds.append(1 if (packet[\"final_anomaly\"] or packet[\"final_failure\"]) else 0)\n",
        "\n",
        "#     return recall_score(y_warn_vec[:budget], preds)\n",
        "\n",
        "# # ----------------------------\n",
        "# # 10) Successive Halving with checkpointing\n",
        "# # ----------------------------\n",
        "# TIME_LIMIT_MIN = 120   # ~2 hours hard stop\n",
        "# start_time = time.time()\n",
        "\n",
        "# def minutes_elapsed():\n",
        "#     return (time.time() - start_time) / 60.0\n",
        "\n",
        "# # Load checkpoint if exists\n",
        "# if os.path.exists(CKPT_PATH):\n",
        "#     print(\"♻️ Resuming tuning from checkpoint...\")\n",
        "#     ckpt = json.load(open(CKPT_PATH))\n",
        "#     best_policy = ckpt.get(\"best_policy\", None)\n",
        "#     best_score  = ckpt.get(\"best_score\", -1.0)\n",
        "#     policies    = ckpt.get(\"remaining_policies\", POLICY_GRID.copy())\n",
        "#     start_stage = int(ckpt.get(\"stage\", 0))\n",
        "# else:\n",
        "#     best_policy = None\n",
        "#     best_score = -1.0\n",
        "#     policies = POLICY_GRID.copy()\n",
        "#     start_stage = 0\n",
        "\n",
        "# print(f\"🔍 Starting tuning: {len(policies)} policies | starting stage: {start_stage+1}/{len(SH_SCHEDULE)}\")\n",
        "\n",
        "# for stage in range(start_stage, len(SH_SCHEDULE)):\n",
        "#     cfg = SH_SCHEDULE[stage]\n",
        "#     budget = cfg[\"budget\"]\n",
        "#     keep_frac = cfg[\"keep_frac\"]\n",
        "\n",
        "#     print(f\"\\n🚀 SH Stage {stage+1}/{len(SH_SCHEDULE)} | Budget={budget} | Policies={len(policies)}\")\n",
        "\n",
        "#     scored = []\n",
        "\n",
        "#     for pi, policy in enumerate(policies):\n",
        "#         if minutes_elapsed() > TIME_LIMIT_MIN:\n",
        "#             print(\"\\n⏹ Time limit reached — checkpoint saved, resume later.\")\n",
        "#             break\n",
        "\n",
        "#         score = evaluate_policy_budget(policy, PRECOMP_SUB, y_warn_sub, budget)\n",
        "#         scored.append((policy, float(score)))\n",
        "\n",
        "#         if score > best_score:\n",
        "#             best_score = float(score)\n",
        "#             best_policy = policy\n",
        "\n",
        "#         print(f\"[{pi+1}/{len(policies)}] Recall={score:.4f} | Best={best_score:.4f} | Elapsed={minutes_elapsed():.1f} min\")\n",
        "\n",
        "#     # If we stopped early due to time, save and exit\n",
        "#     if minutes_elapsed() > TIME_LIMIT_MIN:\n",
        "#         with open(CKPT_PATH, \"w\") as f:\n",
        "#             json.dump({\n",
        "#                 \"best_policy\": best_policy,\n",
        "#                 \"best_score\": best_score,\n",
        "#                 \"remaining_policies\": policies,  # unchanged; not fully evaluated this stage\n",
        "#                 \"stage\": stage,\n",
        "#                 \"timestamp\": time.time(),\n",
        "#                 \"note\": \"stopped_due_to_time_limit\"\n",
        "#             }, f, indent=2, default=safe_jsonable)\n",
        "#         break\n",
        "\n",
        "#     # Keep top fraction\n",
        "#     scored.sort(key=lambda x: x[1], reverse=True)\n",
        "#     keep_n = max(1, int(len(scored) * keep_frac))\n",
        "#     policies = [p for p, _ in scored[:keep_n]]\n",
        "\n",
        "#     # Save checkpoint after each stage\n",
        "#     with open(CKPT_PATH, \"w\") as f:\n",
        "#         json.dump({\n",
        "#             \"best_policy\": best_policy,\n",
        "#             \"best_score\": best_score,\n",
        "#             \"remaining_policies\": policies,\n",
        "#             \"stage\": stage + 1,   # next stage\n",
        "#             \"timestamp\": time.time(),\n",
        "#             \"progress\": f\"stage {stage+1}/{len(SH_SCHEDULE)} done\"\n",
        "#         }, f, indent=2, default=safe_jsonable)\n",
        "\n",
        "# print(\"\\n✅ TUNING COMPLETE (or stopped early)\")\n",
        "# print(\"🏆 Best WARNING Recall:\", best_score)\n",
        "# print(\"🏆 Best Policy:\", best_policy)\n",
        "\n",
        "# # ----------------------------\n",
        "# # 11) Save best policy (for holdout)\n",
        "# # ----------------------------\n",
        "# if best_policy is not None:\n",
        "#     with open(BEST_POLICY_PATH, \"w\") as f:\n",
        "#         json.dump({\n",
        "#             \"policy\": best_policy,\n",
        "#             \"metric\": \"WARNING_RECALL\",\n",
        "#             \"best_score\": float(best_score),\n",
        "#             \"method\": \"successive_halving\",\n",
        "#             \"timestamp\": datetime.now().isoformat(),\n",
        "#         }, f, indent=2, default=safe_jsonable)\n",
        "\n",
        "#     print(f\"✅ Best policy saved to: {BEST_POLICY_PATH}\")\n",
        "# else:\n",
        "#     print(\"⚠️ No best_policy produced (something went wrong).\")\n",
        "\n",
        "# # ----------------------------\n",
        "# # 12) FINAL HOLDOUT RUN (no tuning)\n",
        "# # ----------------------------\n",
        "# print(\"\\n🚀 Running FINAL HOLDOUT with best_policy...\\n\")\n",
        "\n",
        "# # Load best policy from disk if you want absolute reproducibility\n",
        "# if os.path.exists(BEST_POLICY_PATH):\n",
        "#     blob = json.load(open(BEST_POLICY_PATH))\n",
        "#     BASE_POLICY = blob[\"policy\"]\n",
        "# else:\n",
        "#     BASE_POLICY = best_policy\n",
        "\n",
        "# decision_agent = DecisionAgent(**BASE_POLICY)\n",
        "\n",
        "# decision_packets = []\n",
        "# final_failure_flags = []\n",
        "# final_anomaly_flags = []\n",
        "# final_alert_levels = []\n",
        "\n",
        "# for seq in tqdm(X_hold, total=len(X_hold)):\n",
        "#     master_out = master.process_system_input(seq)\n",
        "#     window_out = window_agent.predict_window_size(seq.flatten(), seq)\n",
        "#     probs = classifier.predict(seq[np.newaxis, ...], verbose=0)[0]\n",
        "\n",
        "#     packet = decision_agent.decide(\n",
        "#         master_output=master_out,\n",
        "#         window_output=window_out,\n",
        "#         model_outputs={\n",
        "#             \"failure_prob\": float(probs[2]),\n",
        "#             \"transformer_prob\": float(probs[1]),\n",
        "#         },\n",
        "#     )\n",
        "\n",
        "#     decision_packets.append(packet)\n",
        "#     final_failure_flags.append(int(packet[\"final_failure\"]))\n",
        "#     final_anomaly_flags.append(int(packet[\"final_anomaly\"]))\n",
        "#     final_alert_levels.append(packet[\"alert_level\"])\n",
        "\n",
        "# final_failure_flags = np.array(final_failure_flags)\n",
        "# final_anomaly_flags = np.array(final_anomaly_flags)\n",
        "\n",
        "# print(\"\\n✅ HOLDOUT COMPLETE\")\n",
        "# print(\"Failure flags:\", np.unique(final_failure_flags, return_counts=True))\n",
        "# print(\"Anomaly flags:\", np.unique(final_anomaly_flags, return_counts=True))\n",
        "# print(\"Alert levels:\", {k: final_alert_levels.count(k) for k in set(final_alert_levels)})\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# FINAL HOLDOUT RUN (NO TUNING, NO SEARCH)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "import os, json, numpy as np\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "\n",
        "USE_EXPERT_AGENT = False\n",
        "MAX_EXPERT_CALLS = 50\n",
        "expert_calls = 0\n",
        "\n",
        "CKPT_PATH  = \"/content/drive/MyDrive/PHD/2025/holdout_eval_checkpoint.json\"\n",
        "SAVE_EVERY = 2000\n",
        "\n",
        "# -----------------------------\n",
        "# Build perception agents ONCE (use HOLDOUT shapes!)\n",
        "# -----------------------------\n",
        "models_dir = \"/content/drive/MyDrive/PHD/2025/TEMP_OUTPUT_METROPM/sensor/model\"\n",
        "\n",
        "num_sensors = X_hold.shape[2]\n",
        "win_length  = X_hold.shape[1]\n",
        "\n",
        "sensor_agents, master = create_robust_system(\n",
        "    num_sensors=num_sensors,\n",
        "    models_dir=models_dir,\n",
        "    win_length=win_length,\n",
        "    warmup_steps=100,\n",
        ")\n",
        "\n",
        "window_agent = AdaptiveWindowAgent(\n",
        "    model_path=\"/content/drive/MyDrive/PHD/2025/DGRNet-MLP-Versions/METROPM_MLP_model_10Sec.keras\"\n",
        ")\n",
        "\n",
        "event_store = EventStore(db_path=\"event_store.db\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load best policy\n",
        "# -----------------------------\n",
        "# policy_path = \"/content/drive/MyDrive/PHD/2025/policies/best_decision_policy.json\"\n",
        "# with open(policy_path, \"r\") as f:\n",
        "#     policy_blob = json.load(f)\n",
        "\n",
        "# BASE_POLICY = policy_blob[\"policy\"]\n",
        "\n",
        "##for now use a fixed policy\n",
        "BASE_POLICY = {\n",
        "    # thresholds\n",
        "    \"detection_threshold\": 0.50,\n",
        "    \"drift_threshold\": 0.35,\n",
        "    \"failure_threshold\": 0.50,\n",
        "\n",
        "    # fusion weights (unchanged, conservative)\n",
        "    \"w_sensor\": 0.5,\n",
        "    \"w_window\": 0.5,\n",
        "    \"w_warn\": 0.2,\n",
        "    \"w_fault\": 0.7,\n",
        "\n",
        "    # warning logic (IMPORTANT)\n",
        "    \"warn_threshold\": 0.40,   # lower than before (early warning)\n",
        "    \"warn_boost\": 0.15,\n",
        "    \"warn_penalty\": 0.0,      # ⬅️ disable penalty for now\n",
        "\n",
        "    # keep gates but don’t tune\n",
        "    \"warn_gate_high_contra\": 0.75,\n",
        "}\n",
        "\n",
        "\n",
        "def validate_policy(policy):\n",
        "    required = {\n",
        "        \"detection_threshold\",\"drift_threshold\",\"failure_threshold\",\n",
        "        \"w_sensor\",\"w_window\",\"w_fault\",\"w_warn\",\n",
        "    }\n",
        "    missing = required - set(policy.keys())\n",
        "    if missing:\n",
        "        raise ValueError(f\"Policy missing keys: {missing}\")\n",
        "\n",
        "validate_policy(BASE_POLICY)\n",
        "\n",
        "print(\"✅ Loaded DecisionAgent policy:\")\n",
        "print(json.dumps(BASE_POLICY, indent=2))\n",
        "\n",
        "decision_agent = DecisionAgent(\n",
        "    detection_threshold=BASE_POLICY.get(\"detection_threshold\", 0.50),\n",
        "    drift_threshold=BASE_POLICY.get(\"drift_threshold\", 0.35),\n",
        "    failure_threshold=BASE_POLICY.get(\"failure_threshold\", 0.50),\n",
        "    # optional carry-over weights if present:\n",
        "    w_sensor=BASE_POLICY.get(\"w_sensor\", 0.60),\n",
        "    w_window=BASE_POLICY.get(\"w_window\", 0.40),\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Expert agent (ONLY if enabled)\n",
        "# -----------------------------\n",
        "expert_agent = None\n",
        "openai_client = None\n",
        "if USE_EXPERT_AGENT:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    from openai import OpenAI\n",
        "    openai_client = OpenAI()\n",
        "\n",
        "    expert_agent = ExpertAgent(\n",
        "        event_store=event_store,\n",
        "        system_description=\"MetroPT multivariate APU system\",\n",
        "        llm_client=openai_client,\n",
        "        history_limit=100,\n",
        "        max_history_for_prompt=10,\n",
        "        window_preview_len=30,\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation storage\n",
        "# -----------------------------\n",
        "start_idx = 0\n",
        "\n",
        "y_warn_true = []\n",
        "y_fail_true = []\n",
        "\n",
        "transformer_warn_flags = []\n",
        "transformer_fail_flags = []\n",
        "\n",
        "decision_warn_flags = []\n",
        "decision_fail_flags = []\n",
        "\n",
        "final_alert_levels = []\n",
        "expert_audit_log = []\n",
        "\n",
        "# optional (if you still want them)\n",
        "final_failure_flags = []\n",
        "final_anomaly_flags = []\n",
        "final_warning_flags = []\n",
        "\n",
        "# -----------------------------\n",
        "# Resume from checkpoint (robust)\n",
        "# -----------------------------\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    print(\"♻️ Resuming HOLDOUT evaluation from checkpoint...\")\n",
        "    with open(CKPT_PATH, \"r\") as f:\n",
        "        ckpt = json.load(f)\n",
        "\n",
        "    start_idx              = ckpt[\"last_index\"] + 1\n",
        "    y_warn_true            = ckpt[\"y_warn_true\"]\n",
        "    y_fail_true            = ckpt[\"y_fail_true\"]\n",
        "    transformer_warn_flags = ckpt[\"transformer_warn_flags\"]\n",
        "    transformer_fail_flags = ckpt[\"transformer_fail_flags\"]\n",
        "    decision_warn_flags    = ckpt[\"decision_warn_flags\"]\n",
        "    decision_fail_flags    = ckpt[\"decision_fail_flags\"]\n",
        "    final_alert_levels     = ckpt[\"final_alert_levels\"]\n",
        "    expert_audit_log       = ckpt.get(\"expert_audit_log\", [])\n",
        "    final_warning_flags = ckpt.get(\"final_warning_flags\", [])\n",
        "    final_failure_flags = ckpt.get(\"final_failure_flags\", [])\n",
        "    final_anomaly_flags = ckpt.get(\"final_anomaly_flags\", [])\n",
        "\n",
        "    print(f\"➡️ Resuming from {start_idx}/{len(X_hold)}\")\n",
        "else:\n",
        "    print(\"🆕 Starting fresh HOLDOUT evaluation\")\n",
        "\n",
        "print(\"\\n🚀 Running FINAL pipeline on HOLDOUT...\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN HOLDOUT LOOP (resume-aware)\n",
        "# -----------------------------\n",
        "MAX_SAMPLES = 5000 ##ENough of holdout to test\n",
        "end_idx = min(len(X_hold), MAX_SAMPLES)\n",
        "for i in range(start_idx, end_idx):\n",
        "    seq = X_hold[i]\n",
        "\n",
        "    try:\n",
        "        master_out = master.process_system_input(seq)\n",
        "\n",
        "        window_out = window_agent.predict_window_size(seq.flatten(), seq)\n",
        "\n",
        "        probs = classifier.predict(seq[np.newaxis, ...], verbose=0)[0]\n",
        "\n",
        "        model_outputs = {\n",
        "            \"failure_prob\": float(probs[2]),\n",
        "            \"transformer_prob\": float(probs[1]),\n",
        "        }\n",
        "\n",
        "        decision_packet = decision_agent.decide(\n",
        "            master_output=master_out,\n",
        "            window_output=window_out,\n",
        "            model_outputs=model_outputs,\n",
        "            metadata={\"index\": int(i)},\n",
        "        )\n",
        "\n",
        "        # Expert Agent block (optional)\n",
        "        expert_packet = None\n",
        "        human_feedback = None\n",
        "\n",
        "        if (\n",
        "            USE_EXPERT_AGENT\n",
        "            and decision_packet[\"alert_level\"] in [\"HIGH\", \"CRITICAL\"]\n",
        "            and expert_calls < MAX_EXPERT_CALLS\n",
        "        ):\n",
        "            print(f\"\\n🧠 ExpertAgent invoked at sample {i} | {decision_packet['alert_level']}\")\n",
        "            expert_packet = expert_agent.analyse_anomaly(\n",
        "                decision_packet=decision_packet,\n",
        "                system_subsequence=seq,\n",
        "                extra_context={\"index\": int(i)},\n",
        "            )\n",
        "            expert_calls += 1\n",
        "\n",
        "            llm_out = expert_packet.get(\"llm_result\", {})\n",
        "            print(\"   Summary:\", llm_out.get(\"summary\"))\n",
        "            print(\"   Likely fault:\", llm_out.get(\"likely_fault\"))\n",
        "            print(\"   Recommended action:\", llm_out.get(\"recommended_action\"))\n",
        "            print(\"   Severity:\", llm_out.get(\"severity\"))\n",
        "\n",
        "            expert_audit_log.append({\n",
        "                \"index\": i,\n",
        "                \"alert_level\": decision_packet[\"alert_level\"],\n",
        "                \"final_failure\": decision_packet[\"final_failure\"],\n",
        "                \"final_anomaly\": decision_packet[\"final_anomaly\"],\n",
        "                \"llm_result\": llm_out,\n",
        "            })\n",
        "\n",
        "            if \"get_human_feedback_stub\" in globals():\n",
        "                human_feedback = get_human_feedback_stub(expert_packet)\n",
        "\n",
        "            event_store.save_event(\"ALERT\", decision_packet, expert_packet, human_feedback)\n",
        "        else:\n",
        "            event_store.save_event(\"NORMAL\", decision_packet, None, None)\n",
        "\n",
        "        # Ground truth\n",
        "        y_warn_true.append(1 if y_hold[i] == 1 else 0)\n",
        "        y_fail_true.append(1 if y_hold[i] == 2 else 0)\n",
        "\n",
        "        # Transformer baselines\n",
        "\n",
        "        pred_cls = np.argmax(probs)\n",
        "        transformer_warn_flags.append(1 if pred_cls == 1 else 0)\n",
        "        transformer_fail_flags.append(1 if pred_cls == 2 else 0)\n",
        "\n",
        "\n",
        "        # Decision outputs\n",
        "        decision_warn_flags.append(1 if decision_packet.get(\"final_warning\", False) else 0)\n",
        "        decision_fail_flags.append(1 if decision_packet.get(\"final_failure\", False) else 0)\n",
        "\n",
        "\n",
        "        final_failure_flags.append(int(decision_packet[\"final_failure\"]))\n",
        "        final_anomaly_flags.append(int(decision_packet[\"final_anomaly\"]))\n",
        "        final_warning_flags.append(int(decision_packet[\"final_warning\"]))\n",
        "        final_alert_levels.append(decision_packet[\"alert_level\"])\n",
        "\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Processed {i+1}/{len(X_hold)} samples...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Sample {i} failed: {e}\")\n",
        "\n",
        "        y_warn_true.append(0)\n",
        "        y_fail_true.append(0)\n",
        "        transformer_warn_flags.append(0)\n",
        "        transformer_fail_flags.append(0)\n",
        "        decision_warn_flags.append(0)\n",
        "        decision_fail_flags.append(0)\n",
        "        final_alert_levels.append(\"NORMAL\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # CHECKPOINT SAVE (SAFE + SMALL, ATOMIC)\n",
        "    # -----------------------------\n",
        "    if (i + 1) % SAVE_EVERY == 0:\n",
        "        ckpt = {\n",
        "            \"last_index\": i,\n",
        "            \"y_warn_true\": y_warn_true,\n",
        "            \"y_fail_true\": y_fail_true,\n",
        "            \"transformer_warn_flags\": transformer_warn_flags,\n",
        "            \"transformer_fail_flags\": transformer_fail_flags,\n",
        "            \"decision_warn_flags\": decision_warn_flags,\n",
        "            \"decision_fail_flags\": decision_fail_flags,\n",
        "            \"final_alert_levels\": final_alert_levels,\n",
        "            \"expert_audit_log\": expert_audit_log,\n",
        "\n",
        "            # ✅ new\n",
        "            \"final_warning_flags\": final_warning_flags,\n",
        "            \"final_failure_flags\": final_failure_flags,\n",
        "            \"final_anomaly_flags\": final_anomaly_flags\n",
        "        }\n",
        "\n",
        "        tmp_path = CKPT_PATH + \".tmp\"\n",
        "        with open(tmp_path, \"w\") as f:\n",
        "            json.dump(ckpt, f)\n",
        "        os.replace(tmp_path, CKPT_PATH)\n",
        "\n",
        "        print(f\"💾 Checkpoint saved safely at {i+1}/{len(X_hold)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# METRIC COMPUTATION\n",
        "# -----------------------------\n",
        "y_warn_true = np.array(y_warn_true)\n",
        "y_fail_true = np.array(y_fail_true)\n",
        "\n",
        "transformer_warn_flags = np.array(transformer_warn_flags)\n",
        "decision_warn_flags = np.array(decision_warn_flags)\n",
        "\n",
        "transformer_fail_flags = np.array(transformer_fail_flags)\n",
        "decision_fail_flags = np.array(decision_fail_flags)\n",
        "\n",
        "print(\"\\n================= HOLDOUT EVALUATION =================\")\n",
        "\n",
        "print(\"\\n🟦 TRANSFORMER-ONLY (WARNING)\")\n",
        "print(\"Recall   :\", recall_score(y_warn_true, transformer_warn_flags))\n",
        "print(\"Precision:\", precision_score(y_warn_true, transformer_warn_flags))\n",
        "print(\"F1       :\", f1_score(y_warn_true, transformer_warn_flags))\n",
        "\n",
        "print(\"\\n🟩 DECISION AGENT (WARNING)\")\n",
        "print(\"Recall   :\", recall_score(y_warn_true, decision_warn_flags))\n",
        "print(\"Precision:\", precision_score(y_warn_true, decision_warn_flags))\n",
        "print(\"F1       :\", f1_score(y_warn_true, decision_warn_flags))\n",
        "\n",
        "print(\"\\n🟥 TRANSFORMER-ONLY (FAILURE)\")\n",
        "print(\"Recall   :\", recall_score(y_fail_true, transformer_fail_flags))\n",
        "print(\"Precision:\", precision_score(y_fail_true, transformer_fail_flags))\n",
        "print(\"F1       :\", f1_score(y_fail_true, transformer_fail_flags))\n",
        "\n",
        "print(\"\\n🟧 DECISION AGENT (FAILURE)\")\n",
        "print(\"Recall   :\", recall_score(y_fail_true, decision_fail_flags))\n",
        "print(\"Precision:\", precision_score(y_fail_true, decision_fail_flags))\n",
        "print(\"F1       :\", f1_score(y_fail_true, decision_fail_flags))\n",
        "\n",
        "print(\"\\n======================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_5iji919H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OtWHK--uG6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/supriyag123/PHD_Pub/blob/main/AGENTIC-MODULE6-REVISED-DecisionExpertLLM.ipynb",
      "authorship_tag": "ABX9TyPR6AJqsc4WR2+1yInTeeUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}